{"meta":{"title":"YangWC's Blog","subtitle":null,"description":"Personal blog website.","author":"WC Yang","url":"http://yoursite.com","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-04-27T07:26:21.624Z","updated":"2019-04-27T07:26:21.624Z","comments":true,"path":"404.html","permalink":"http://yoursite.com/404.html","excerpt":"","text":"**404 Not Found** **�ܱ�Ǹ�������ʵ�ҳ�治����** �����������ַ�����õ�ַ�ѱ�ɾ��"},{"title":"关于","date":"2019-04-27T10:20:23.860Z","updated":"2019-04-27T10:20:23.860Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"[](https://github.com/ZeusYang) 中山大学本科四年级 计算机科学与技术专业 准研究生，研究方向计算机图形学 现居广州大学城 关于本站欢迎来到 YangWC 的博客！本站会记录自己的一些学习内容，如若有错，欢迎指正，感谢！ 关于主题本站的主题风格是：Material X有任何问题请留言。"},{"title":"所有分类","date":"2019-04-27T08:54:04.778Z","updated":"2019-04-27T08:54:04.778Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-04-27T08:02:52.306Z","updated":"2019-04-27T08:02:52.306Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"大佬的博客","date":"2019-04-27T10:52:08.692Z","updated":"2019-04-27T10:52:08.692Z","comments":true,"path":"friends/index.html","permalink":"http://yoursite.com/friends/index.html","excerpt":"","text":"名称： YangWC’s Blog头像： https://cdn.jsdelivr.net/gh/ZeusYang/CDN-for-yangwc.com@1.1.4//globalImage/avator.jpg网址： https://yangwc.com"}],"posts":[{"title":"体素化Voxelization：基于GPU的三维体素化","slug":"Voxelization","date":"2019-06-11T09:28:14.040Z","updated":"2019-06-11T09:33:00.231Z","comments":true,"path":"2019/06/11/Voxelization/","link":"","permalink":"http://yoursite.com/2019/06/11/Voxelization/","excerpt":"本篇文章主要是关于三维网格模型的基于GPU并行的体素化算法，这个算法我是偶然从NVIDIA官网上看到的。基于GPU的体素化算法巧妙地借助了渲染流程的光栅化处理，将整个体素化的过程并行化，速度极快，缺点就是占用的内存较高。相关的完整代码请看这个链接中的Renderer目录下的Voxelization.h文件和Voxelization.cpp文件。","text":"本篇文章主要是关于三维网格模型的基于GPU并行的体素化算法，这个算法我是偶然从NVIDIA官网上看到的。基于GPU的体素化算法巧妙地借助了渲染流程的光栅化处理，将整个体素化的过程并行化，速度极快，缺点就是占用的内存较高。相关的完整代码请看这个链接中的Renderer目录下的Voxelization.h文件和Voxelization.cpp文件。 体素化 修补裂缝 修补孔洞 参考资料 &emsp;&emsp;在基于位置动力学的物理模拟中，所有要模拟的物体都由一组粒子来表示，每个粒子都是一个给定半径大小的球体，对于固体这类的物体，粒子通常是紧密相连的。为此，为了实现基于位置动力学的物理模拟，我们需要采用一种算法将网格物体的三角网格模型用一个个粒子表示，这个并不是简单地取网格模型的所有顶点就行，因为我们需要紧密连接的粒子，面片网格模型的顶点通常是稀疏的。这个过程其实就是体素化，三维体素是二维像素的三维扩展，体素的基本单元不再是二维的正方形，而是三维的立方体，立方体的边长决定了体素化的分辨率，通常边长越长，则分辨率越低。将网格体素化后我们得到了一组体素的中心顶点位置，可将其用于后续的基于位置动力学的物理模拟当中。 &emsp;&emsp;目前常用的体素化方法大都是基于CPU的，这类方法通常是将射线与物体求交，根据是奇数个交点还是偶数个交点来判断当前的体素是否在物体的内部。在没有采用特殊的数据结构时，每次求交都要遍历一次网格模型的所有三角形，效率非常低。在采用了八叉树加速之后，速度有所提升，但随着模型的三角形面片数增加，串行的体素化算法耗费的时间越来越长。我没有采用CPU串行的体素化方法，而是采用了基于GPU并行的体素化算法，这个算法我是偶然从NVIDIA官网上看到的。基于GPU的体素化算法巧妙地借助了渲染流程的光栅化处理，将整个体素化的过程并行化，速度极快，缺点就是占用的内存较高。 图1 三维体素模型 一、体素化&emsp;&emsp;基于GPU的三维体素化大致思想就是：首先计算出需要体素化模型的AABB包围盒，然后将模型投影到AABB包围盒的某个平面上，经过渲染管线的光栅化插值操作，我们可以在片元着色器得到每个像素点对应的世界空间的顶点坐标，根据这个顶点坐标标记三维空间数组（这个三维空间数组就是根据体素划分的空间序列）的相应位置，最后在CPU端读出这个三维空间数组，若当前的数组位置有标记，则将该数组位置对应的立方体作为一个体素。可以看到，整个流程思路非常清晰，但是还需要借助一些手段修正算法存在的缺陷，这个在后面会提到。 &emsp;&emsp;首先就是计算网格模型的AABB包围盒，在导入模型时获取$x$、$y$、$z$轴分量的最大值和最小值，从而得到包围盒的最大顶点和最小顶点。这个比较简单，不再赘述： 12345678910111213// bounding box.if (mesh-&gt;mVertices[x].x &lt; m_min.x) m_min.x = mesh-&gt;mVertices[x].x;if (mesh-&gt;mVertices[x].y &lt; m_min.y) m_min.y = mesh-&gt;mVertices[x].y;if (mesh-&gt;mVertices[x].z &lt; m_min.z) m_min.z = mesh-&gt;mVertices[x].z;if (mesh-&gt;mVertices[x].x &gt; m_max.x) m_max.x = mesh-&gt;mVertices[x].x;if (mesh-&gt;mVertices[x].y &gt; m_max.y) m_max.y = mesh-&gt;mVertices[x].y;if (mesh-&gt;mVertices[x].z &gt; m_max.z) m_max.z = mesh-&gt;mVertices[x].z; 图2 模型包围盒 &emsp;&emsp;获取了模型的包围盒之后，我们就需要根据这个包围盒设置我们的观察角度和投影平面，这关系到后面的体素化结果。同时为了保证正确地体素化模型，我们采用的投影方式是正交投影。首先我们要选择一个观察方向和投影平面，AABB包围盒有六个面，其中前和后、上和下、左和右的投影结果是一样的，因此实际的选择只有三个平面，分别是前、上、右（或者后、下、左）。显然一个物体投影到这个三个平面上的结果都不一样，目前我们暂时先选择投影到前面这个平面上，摄像机的视线朝向z轴的负方向。注意正确地设置摄像机的位置，否则什么看不到。既然我们选择投影到前面这个平面上，我们就设置摄像机的位置在包围盒前面这个平面的中心再往前一点。同时了为了确保模型全部投影到屏幕上，我们设置的正交投影平面比选定的包围盒平面稍微大一点点。具体代码如下所示： 图3 三个面上的投影结果 12345678910111213141516171819// bounding box and resolution.glm::vec3 min, max;glm::ivec3 resolution;target-&gt;getAABB(min, max);glm::vec3 range(max.x - min.x, max.y - min.y, max.z - min.z);resolution.x = static_cast&lt;int&gt;(range.x / step);resolution.y = static_cast&lt;int&gt;(range.y / step);resolution.z = static_cast&lt;int&gt;(range.z / step);int length = static_cast&lt;int&gt;(resolution.x * resolution.y * resolution.z);// cameraglm::vec3 cameraPos;cameraPos.x = (min.x + max.x) * 0.5f;cameraPos.y = (min.y + max.y) * 0.5f;cameraPos.z = max.z + 0.2f;FPSCamera::ptr camera(new FPSCamera(cameraPos));camera-&gt;lookAt(glm::vec3(0.0f, 0.0f, -1.0f));camera-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.z * 1.2f + 0.2f); 图4 包围盒投影平面 &emsp;&emsp;设置好投影矩阵和视图矩阵之后，我们需要申请一个着色器可写的缓冲，这个缓冲的大小等于AABB包围盒的分辨率，在片元着色器阶段我们需要根据当前片元的世界空间位置对这个缓冲做标记，表示该缓冲位置上有一个体素。我们采用OpenGL的GL_SHADER_STORAGE_BUFFER，这是一个着色器可读写的缓冲类型。申请缓冲之后，将缓冲全部初始化为0。然后将需要体素化的网格模型送入渲染管线进行渲染。在片元着色器中，将每个片元的世界空间位置对应的缓冲位置加1。最后在CPU端读出缓冲内容，缓冲值大于0时，则表示该位置有一个体素。CPU端的整个流程代码如下所示。这里需要特别注意的是，我们应该关闭深度测试和背面剔除，保证模型的全面三角形都进入片元着色器，确保所有的三角形不被剔除，从而使得全部的三角形都被处理，最后得到正确的体素化结果。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788bool Voxelization::voxelize(Drawable* target, const float &amp; step, std::vector&lt;glm::vec3&gt;&amp; ret)&#123; // shader ShaderMgr::ptr shaderMgr = ShaderMgr::getSingleton(); unsigned int voxelizeCount = shaderMgr-&gt;loadShader(\"voxelizeCount\", \"./glsl/voxelizeCount.vert\", \"./glsl/voxelizeCount.frag\"); Shader::ptr shader = shaderMgr-&gt;getShader(voxelizeCount); // bounding box and resolution. glm::vec3 min, max; glm::ivec3 resolution; target-&gt;getAABB(min, max); glm::vec3 range(max.x - min.x, max.y - min.y, max.z - min.z); resolution.x = static_cast&lt;int&gt;(range.x / step) + 1; resolution.y = static_cast&lt;int&gt;(range.y / step) + 1; resolution.z = static_cast&lt;int&gt;(range.z / step) + 1; int length = static_cast&lt;int&gt;(resolution.x * resolution.y * resolution.z); // camera glm::vec3 cameraPos; cameraPos.x = (min.x + max.x) * 0.5f; cameraPos.y = (min.y + max.y) * 0.5f; cameraPos.z = max.z + 0.2f; FPSCamera::ptr camera(new FPSCamera(cameraPos)); camera-&gt;lookAt(glm::vec3(0.0f, 0.0f, -1.0f)); camera-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.z * 1.2f + 0.2f); // polygon mode. glPolygonMode(GL_FRONT_AND_BACK, GL_FILL); glDisable(GL_CULL_FACE); glDisable(GL_DEPTH_TEST); glClearColor(1.0f, 0.0f, 0.0f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); // generate ssbo. glGenBuffers(1, &amp;m_cntBuffer); glBindBuffer(GL_SHADER_STORAGE_BUFFER, m_cntBuffer); glBufferData(GL_SHADER_STORAGE_BUFFER, length * sizeof(int), nullptr, GL_STATIC_DRAW); glBindBufferBase(GL_SHADER_STORAGE_BUFFER, 0, m_cntBuffer); // bind shader and ssbo. shader-&gt;bind(); shader-&gt;setVec3(\"boxMin\", min); shader-&gt;setFloat(\"step\", step); shader-&gt;setVec3(\"resolution\", resolution); int *writePtr = reinterpret_cast&lt;int*&gt;(glMapBuffer(GL_SHADER_STORAGE_BUFFER, GL_WRITE_ONLY)); for (int x = 0; x &lt; length; ++x) &#123; writePtr[x] = 0; &#125; if (!glUnmapBuffer(GL_SHADER_STORAGE_BUFFER)) std::cout &lt;&lt; \"unMap error\\n\" &lt;&lt; std::endl; // draw and count. target-&gt;render(camera, nullptr, nullptr, shader); glMemoryBarrier(GL_SHADER_STORAGE_BARRIER_BIT); // get count buffer. glBindBuffer(GL_SHADER_STORAGE_BUFFER, m_cntBuffer); int *readPtr = reinterpret_cast&lt;int*&gt;(glMapBuffer(GL_SHADER_STORAGE_BUFFER, GL_READ_ONLY)); if (readPtr != nullptr) &#123; for (int x = 0; x &lt; length; ++x) &#123; if (*(readPtr + x) != 0) &#123; int iy = x / (resolution.x * resolution.z); int iz = (x - iy * resolution.x * resolution.z) / (resolution.x); int ix = x - iy * resolution.x * resolution.z - iz * resolution.x; ret.push_back(min + glm::vec3(ix * step, iy * step, iz * step)); &#125; &#125; &#125; else &#123; std::cout &lt;&lt; \"nullptr error!\\n\"; &#125; glUnmapBuffer(m_cntBuffer); glBindBuffer(GL_SHADER_STORAGE_BUFFER, 0); glDeleteBuffers(1, &amp;m_cntBuffer); return false;&#125; &emsp;&emsp;接下来就需要在着色器中做一些操作。首先是顶点着色器，在顶点着色器中并没有什么复杂的操作，我们需要将当前的顶点位置传到片元着色器，借助渲染管线的光栅化功能，从而在片元着色器中得到每个片元对应的世界空间位置。下面顶点着色器的代码，其余部分乘上视图矩阵和投影矩阵就不说了。 123456789101112#version 430 corelayout (location = 0) in vec3 position;out vec3 FragPos;uniform mat4 viewMatrix;uniform mat4 projectMatrix;void main()&#123; FragPos = position; gl_Position = projectMatrix * viewMatrix * vec4(position,1.0f);&#125; &emsp;&emsp;中间经过光栅化处理，我们在片元着色器得到每个片元的世界空间坐标。根据这个世界空间的坐标去索引计数缓冲，注意这里采用了GLSL的原子操作函数atmoicAdd，避免GPU线程之间的写冲突。缓冲下标索引的计算基本就是根据体素的大小和包围盒来确定。 12345678910111213141516171819202122#version 430 corein vec3 FragPos;layout (std430, binding = 0) buffer CountBuffer&#123; int cnts[];&#125;;uniform float step;uniform vec3 boxMin;uniform vec3 resolution;out vec4 color;void main()&#123; int x = int((FragPos.x - boxMin.x)/step); int y = int((FragPos.y - boxMin.y)/step); int z = int((FragPos.z - boxMin.z)/step); int index = int(y * (resolution.z * resolution.x) + z * resolution.x + x); atomicAdd(cnts[index], 1); color = vec4(0.0,1.0,0.0,1.0);&#125; &emsp;&emsp;然后下面就是我实现的体素化效果，每个体素用一个立方体绘制，当然也可以用球体绘制。看起来颇有游戏《我的世界》的风格。 二、修补裂缝&emsp;&emsp;上面的实现效果看起来貌似非常不错，但是却存在一个非常严重的问题。前面我们在选择投影平面的时候固定投影在了z轴方向的包围盒平面，这是问题产生的根源。因为模型的每个三角形面片在每个包围盒投影面上的投影结果都不同，若当前的三角形与选取的投影面垂直，那么三角形投影到平面上的将是一条直线，这丢失了很多信息，从而导致裂缝的产生。 图5 不同投影平面的体素化结果 &emsp;&emsp;图5中，左图选取的投影面是摄像机在右边，朝向坐标，这时光栅化得到的结果很好，因而体素化的结果也很好。但是右边的这张图选取的投影面是摄像机在上面，朝向下边，这时光栅化得到的几何面片较少，很多相邻的位置都被投影到了一个片元像素，一些地方没有被体素化，从而导致了裂缝的产生！下面是我实现的程序产生的裂缝，选取的投影方向是z轴方向，下图中的红框部分的几何面片几乎平行于xz平面，从而导致投影光栅化产生的是一个被“压缩“的结果。由于裂缝非常明显且几乎必然会产生（因为通常模型都很复杂，三角形面片朝向很随机），因此有必要采取一些措施来修补这些裂缝。 图6 根据前面步骤产生的裂缝 &emsp;&emsp;如前面的图3所示，每个三角形面片在不同包围盒投影面上的投影结果不同，根据三角形的朝向不同，投影到平面上的三角形大小也各不相同。裂缝产生的原因就是因为投影到平面上的三角形面积被”压缩“了，因此我们需要选取一个投影方向，在该投影方向上三角形的投影面积最大，这样就能够确保所有的三角形面片被充分地体素化，从而使得裂缝小时。 图7 分别投影到包围盒的右、上、前平面上 &emsp;&emsp;因此，我们首先创建三个投影摄像机，将物体分别投影到沿着$x$、$y$、$z$轴的平面上，如图7所示，用以后面着色器中根据三角形的投影面积做选择。代码如下： 1234567891011121314151617181920212223242526272829303132// Camerasfloat offset = 0.2f;glm::vec3 cameraPosZ, cameraPosX, cameraPosY;// looking along z axis.cameraPosZ.x = (min.x + max.x) * 0.5f;cameraPosZ.y = (min.y + max.y) * 0.5f;cameraPosZ.z = max.z + offset;FPSCamera::ptr cameraZ(new FPSCamera(cameraPosZ));cameraZ-&gt;lookAt(glm::vec3(0.0f, 0.0f, -1.0f), Camera3D::LocalUp);cameraZ-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.z * 1.2f + offset);// looking along x axis.cameraPosX.x = max.x + offset;cameraPosX.y = (min.y + max.y) * 0.5f;cameraPosX.z = (min.z + max.z) * 0.5f;FPSCamera::ptr cameraX(new FPSCamera(cameraPosX));cameraX-&gt;lookAt(glm::vec3(-1.0f, 0.0f, 0.0f), Camera3D::LocalUp);cameraX-&gt;setOrthographicProject(-range.z * 0.51, +range.z * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.x * 1.2f + offset);// looking along y axis.cameraPosY.x = (min.x + max.x) * 0.5f;cameraPosY.y = max.y + offset;cameraPosY.z = (min.z + max.z) * 0.5f;FPSCamera::ptr cameraY(new FPSCamera(cameraPosY));cameraY-&gt;lookAt(glm::vec3(0.0f, -1.0f, 0.0f), glm::vec3(0, 1.0, 0.001));cameraY-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.z * 0.51, +range.z * 0.51, 0.1, range.y * 1.2f + offset);......shader-&gt;setMat4(\"viewProject[0]\", cameraX-&gt;getProjectMatrix() * cameraX-&gt;getViewMatrix());shader-&gt;setMat4(\"viewProject[1]\", cameraY-&gt;getProjectMatrix() * cameraY-&gt;getViewMatrix());shader-&gt;setMat4(\"viewProject[2]\", cameraZ-&gt;getProjectMatrix() * cameraZ-&gt;getViewMatrix()); &emsp;&emsp;接下来我们将用到几何着色器，几何着色器阶段在顶点着色器之后、光栅化之前，它根据给定的输入图元和输出图元进行相关的几何图元操作，正好我们可以接用它来根据三角形的投影面积选择采用哪一个投影相机。这里有一个技巧，直观上我们说是根据三角形的投影面积来渲染采用哪个投影相机，实际上没有必要真正地去计算三角形的投影面积，我们可以直接根据当前三角形的世界空间法线朝向来决定投影方向。举个例子，当法线向量的x分量比其余两个分量大时，则当前的三角形肯定投影到x轴方向的投影平面上的面积更大。更深入的理解：设法线向量为$n=(nx,ny,nz)$，我们将法线向量$n$与$(1,0,0)$、$(0,1,0)$、$(0,0,1)$分别做点乘，结果为$nx$、$ny$、$nz$，而法线向量分别与该三个基向量点乘的意义为法线向量在$x$、$y$、$z$轴上的投影值，该值越大则三角形投影到该平面上的面积也越大。所以，我们直接根据最大的法线分量来选择采用哪个投影相机。如下所示： 12345678910111213141516171819202122uint selectViewProject()&#123; vec3 p1 = gl_in[1].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 p2 = gl_in[2].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 faceNormal = cross(p1, p2); float nDX = abs(faceNormal.x); float nDY = abs(faceNormal.y); float nDZ = abs(faceNormal.z); if( nDX &gt; nDY &amp;&amp; nDX &gt; nDZ ) &#123; return 0; &#125; else if( nDY &gt; nDX &amp;&amp; nDY &gt; nDZ ) &#123; return 1; &#125; else &#123; return 2; &#125;&#125; &emsp;&emsp;然后我们将上述的代码应用到我们的几何着色器中，因为视图投影过程挪到了几何着色器阶段，所以顶点着色器直接输入顶点的位置，不做任何变换。几何着色器设置输入图元为三角形，输出图元为最大顶点数为3的三角形带，设置一个viewProject的uniform数组。通过几何着色器，我们对模型的每个三角形面片都做了一个投影选择的处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// vertex shader#version 430 corelayout (location = 0) in vec3 position;void main()&#123; gl_Position = vec4(position, 1.0f);&#125;// geometry shader#version 430 corelayout (triangles) in;layout (triangle_strip, max_vertices = 3) out;out vec3 FragPos;uniform mat4 viewProject[3];uint selectViewProject()&#123; vec3 p1 = gl_in[1].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 p2 = gl_in[2].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 faceNormal = cross(p1, p2); float nDX = abs(faceNormal.x); float nDY = abs(faceNormal.y); float nDZ = abs(faceNormal.z); if( nDX &gt; nDY &amp;&amp; nDX &gt; nDZ ) &#123; return 0; &#125; else if( nDY &gt; nDX &amp;&amp; nDY &gt; nDZ ) &#123; return 1; &#125; else &#123; return 2; &#125;&#125; void main() &#123; uint projectIndex = selectViewProject(); FragPos = gl_in[0].gl_Position.xyz; gl_Position = viewProject[projectIndex] * gl_in[0].gl_Position; EmitVertex(); FragPos = gl_in[1].gl_Position.xyz; gl_Position = viewProject[projectIndex] * gl_in[1].gl_Position; EmitVertex(); FragPos = gl_in[2].gl_Position.xyz; gl_Position = viewProject[projectIndex] * gl_in[2].gl_Position; EmitVertex(); EndPrimitive();&#125; &emsp;&emsp;最终，我们成功的修补了体素的裂缝，如下图所示，先前的裂缝已经填上了体素。 图8 成功修补裂缝 三、修补孔洞&emsp;&emsp;然而，通过前面2部分的处理，另外一个问题出现了。由于模型的每个三角形都是各自根据在每个平面上的投影面积来选择投影相机，这意味着两个相邻的三角形片面可能选取了不同投影相机，使得三角形面片之间因为体素化投影平面的不同而产生过渡问题，从而出现孔洞，即有些部分没有被体素化到。如下图9所示。 图9 体素孔洞 &emsp;&emsp;孔洞的产生根源于光栅化处理，一个像素是否作为当前图元的光栅片元，是通过判断当前图元是否覆盖了该像素中心来完成的。对于那些没有覆盖像素中心的片元，不作为该图元的光栅片元送入片元着色器做进一步的处理，因而模型的一些部分可能会被丢失，从而造成孔洞。为了解决这个问题，我们将在几何着色器中实现一种被称为保守光栅化（Conservative Rasterization）的算法，依旧在几何着色器中实现。 &emsp;&emsp;通常的硬件光栅化，都是默认只取那些中心被图元覆盖的像素单元。而保守光栅化则将所有被图元覆盖（无论是否覆盖到像素单元的中心点）的像素单元都作为光栅化的片元，从而确保图元覆盖的所有区域都被光栅化，故名思意，这就是“保守”一词的由来。如下图10所示，通常情况下硬件默认的光栅片元是绿色部分，边缘红色部分的片元没有被光栅化，导致我们的体素化结果出现孔洞。为了修补体素化的孔洞，我们必须使得被图元哪怕一点点覆盖到的像素（就是下图中的红色部分）都作为当前图元的光栅化结果，这个过程就是保守光栅化算法。 图10 保守光栅化 &emsp;&emsp;那么怎么实现保守光栅化算法，使得上面的红色部分也被光栅化到呢？一个简单直观的思路就是手动扩充三角形图元面片。如上图10所示，里面的三角形是最初的我们要光栅化的三角形，为了使得边缘红色的像素也包含进来，我们扩张最初的三角形得到外面的那个三角形，这个三角形比原来的三角形稍微大一点，此时若将该扩大的三角形送入硬件默认的光栅化单元进行处理，则红色像素也被当作光栅片元，从而达到了我们的目的。注意，这里三角形的扩大程度非常关键，上面的扩大的三角形将我们不需要的像素单元也包含了进来，即黄色像素部分，我们将通过计算三角形的包围盒来剔除那些黄色像素单元，剔除像素部分我们将在片元着色器中实现。下图11是我实现的保守光栅化（图右）效果，图左是默认光栅化的效果。 图11 默认的光栅化和保守光栅化对比 &emsp;&emsp;扩大三角形和剔除像素整个过程都是在裁剪空间中进行的，也就是经过摄像机空间变换和投影变换之后。故而三角形的包围盒只需二维即可，然后需要适当地扩大一点，以免剔除红色的像素片元。一个裁剪空间的三角形包围盒计算如下所示，我们采用GLSL的vec4存储包围盒的最小顶点和最大顶点。 12345678910vec4 calcAABB(vec4 pos[3], vec2 pixelDiagonal)&#123; vec4 aabb; aabb.xy = min(pos[2].xy, min(pos[1].xy, pos[0].xy)); aabb.zw = max(pos[2].xy, max(pos[1].xy, pos[0].xy)); // enlarge by half-pixel aabb.xy -= pixelDiagonal; aabb.zw += pixelDiagonal; return aabb;&#125; &emsp;&emsp;接下来对于给定的三角形的三个顶点，我们要适当地扩大三角形。总体的思路就是：首先计算三角形的三条边与原点构成的齐次空间的平面，然后适当挪动这三个平面，接着就计算偏移后的这三个齐次平面的交线，最后计算三条交线与三角形平面的交点，从而得到扩大后的三角形的三个顶点。整个计算过程都是在裁剪空间中进行的，所以我们忽略顶点的$z$分量，但是上面又提到了齐次平面一词，我们采用一个齐次平面来描述三角形边的线段。所谓齐次平面，就是我们把顶点的齐次分量$w$和$x$、$y$分量合并一起来表示一条线段，直观来看，这就是一个齐次空间的平面，但实际上就是一段二维空间的直线。如下所示： Ax_c+By_c+Cw_c=0 \\tag {1}&emsp;&emsp;公式$(1)$就是一个齐次空间的过原点的平面方程，但是它实际上就是一个二维空间的直线方程。这是因为我们采用的都是正交投影，正交投影并没有透视除法之类的处理，因为正交投影都是线性变换，故而$w_c=1$，所以公式$(1)$表示的过原点的齐次空间的平面方程就是如下所示的二维直线方程： Ax_c+By_c+C=0 \\tag {2}&emsp;&emsp;之所以采用齐次空间的平面方程，是为了方便我们的计算。首先我们根据三角形的三条边计算三个齐次空间的平面，我们已知该齐次空间的平面过原点，平面方程的$(A,B,C)$就是该平面的法线向量，我们直接做叉乘计算可得平面的法线，如下所示： 1234vec3 edgePlanes[3];edgePlanes[0] = cross(pos[0].xyw - pos[2].xyw, pos[2].xyw);edgePlanes[1] = cross(pos[1].xyw - pos[0].xyw, pos[0].xyw);edgePlanes[2] = cross(pos[2].xyw - pos[1].xyw, pos[1].xyw); &emsp;&emsp;然后对这三个平面分别进行偏移。直观上来说，我们分别令三角形的三条边在其法线的方向上挪一段距离，这个距离由像素单元格的大小（即下面的halfPixel）在法线方向的投影决定，如下所示： 123edgePlanes[0].z -= dot(halfPixel[projectIndex], abs(edgePlanes[0].xy));edgePlanes[1].z -= dot(halfPixel[projectIndex], abs(edgePlanes[1].xy)); edgePlanes[2].z -= dot(halfPixel[projectIndex], abs(edgePlanes[2].xy)); &emsp;&emsp;接着计算三个齐次平面的交线向量，这个不难理解，两个平面的交线必然垂直于这两个平面的法线向量，因而交线向量可由这两个平面的法线向量做叉乘得到： 1234567vec3 intersection[3];intersection[0] = cross(edgePlanes[0], edgePlanes[1]);intersection[1] = cross(edgePlanes[1], edgePlanes[2]);intersection[2] = cross(edgePlanes[2], edgePlanes[0]);intersection[0] /= intersection[0].z;intersection[1] /= intersection[1].z;intersection[2] /= intersection[2].z; &emsp;&emsp;最后我们根据上面的三条射线向量与初试三角形所在的平面求交点，从而得到最终扩大后的三角形的三个顶点。由于我们是正交投影，所以上面求到的三条射线向量的$x$分量和$y$分量就是扩大三角形顶点的$x$分量和$y$分量，即交点的$x$、$y$已知，需要求$z$值。一个三维平面方程如下所示，从直观的几何意义上来说，$(A,B,C)$就是平面的法线向量，$D$就是原点到平面的直线距离。 Ax+By+Cz+D=0 \\tag {3}&emsp;&emsp;已知初始三角形的三个点，我们可以求出它的法线向量，然后原点到平面的直线距离就等于平面上的点在法线向量方向上的投影长度，这里要特别注意符号，具体看下面的代码： 123vec4 trianglePlane;trianglePlane.xyz = normalize(cross(pos[1].xyz - pos[0].xyz, pos[2].xyz-pos[0].xyz));trianglePlane.w = -dot(pos[0].xyz, trianglePlane.xyz); &emsp;&emsp;然后还需要提一点的是，我们要确保输入的三角形的顶点环绕顺序都是逆时针方向，这个逆时针方向是针对当前的相机投影方向。对于背向的面片，我们要做一个纠正的过程。判断是否是背向面片很简单，只需通过计算三角形法线向量与$(0,0,1)$做点乘，判断其符号即可。 1234567// change winding, otherwise there are artifacts for the back faces.if (dot(trianglePlane.xyz, vec3(0.0, 0.0, 1.0)) &lt; 0.0)&#123; vec4 vertexTemp = pos[2]; pos[2] = pos[1]; pos[1] = vertexTemp;&#125; &emsp;&emsp;已知交点的$x$和$y$，我们代入平面方程$(3)$求得$z$值。 z=-\\frac{Ax+By+D}{C} \\tag {4}12345678// calculate dilated triangle verticesfloat z[3];z[0] = -(intersection[0].x * trianglePlane.x + intersection[0].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z;z[1] = -(intersection[1].x * trianglePlane.x + intersection[1].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z;z[2] = -(intersection[2].x * trianglePlane.x + intersection[2].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z;pos[0].xyz = vec3(intersection[0].xy, z[0]);pos[1].xyz = vec3(intersection[1].xy, z[1]);pos[2].xyz = vec3(intersection[2].xy, z[2]); &emsp;&emsp;最终，我们求得到扩大后的三角形的三个顶点，我们还需要对三个顶点做逆视图投影变换，将裁剪空间的顶点变换到世界空间，得到扩大后的三角形的世界坐标，因为我们最终目的是根据世界空间坐标做体素化的处理。与此同时，我们还将在裁剪空间的扩大三角形的顶点传到片元着色器，因为我们要剔除不必要的片元。以下是保守光栅化算法的几何着色器代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117#version 430 corelayout (triangles) in;layout (triangle_strip, max_vertices = 3) out;out vec3 FragPos;out vec3 ProjectPos;out vec4 BoundingBox;uniform vec2 halfPixel[3];uniform mat4 viewProject[3];uniform mat4 viewProjectInverse[3];uint selectViewProject()&#123; vec3 p1 = gl_in[1].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 p2 = gl_in[2].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 faceNormal = cross(p1, p2); float nDX = abs(faceNormal.x); float nDY = abs(faceNormal.y); float nDZ = abs(faceNormal.z); if( nDX &gt; nDY &amp;&amp; nDX &gt; nDZ ) &#123; return 0; &#125; else if( nDY &gt; nDX &amp;&amp; nDY &gt; nDZ ) &#123; return 1; &#125; else &#123; return 2; &#125;&#125; vec4 calcAABB(vec4 pos[3], vec2 pixelDiagonal)&#123; vec4 aabb; aabb.xy = min(pos[2].xy, min(pos[1].xy, pos[0].xy)); aabb.zw = max(pos[2].xy, max(pos[1].xy, pos[0].xy)); // enlarge by half-pixel aabb.xy -= pixelDiagonal; aabb.zw += pixelDiagonal; return aabb;&#125;void main() &#123; uint projectIndex = selectViewProject(); vec4 pos[3] = vec4[3] ( viewProject[projectIndex] * gl_in[0].gl_Position, viewProject[projectIndex] * gl_in[1].gl_Position, viewProject[projectIndex] * gl_in[2].gl_Position ); vec4 trianglePlane; trianglePlane.xyz = normalize(cross(pos[1].xyz - pos[0].xyz, pos[2].xyz - pos[0].xyz)); trianglePlane.w = -dot(pos[0].xyz, trianglePlane.xyz); // change winding, otherwise there are artifacts for the back faces. if (dot(trianglePlane.xyz, vec3(0.0, 0.0, 1.0)) &lt; 0.0) &#123; vec4 vertexTemp = pos[2]; pos[2] = pos[1]; pos[1] = vertexTemp; &#125; if(trianglePlane.z == 0.0f) return; BoundingBox = calcAABB(pos, halfPixel[projectIndex]); vec3 edgePlanes[3]; edgePlanes[0] = cross(pos[0].xyw - pos[2].xyw, pos[2].xyw); edgePlanes[1] = cross(pos[1].xyw - pos[0].xyw, pos[0].xyw); edgePlanes[2] = cross(pos[2].xyw - pos[1].xyw, pos[1].xyw); edgePlanes[0].z -= dot(halfPixel[projectIndex], abs(edgePlanes[0].xy)); edgePlanes[1].z -= dot(halfPixel[projectIndex], abs(edgePlanes[1].xy)); edgePlanes[2].z -= dot(halfPixel[projectIndex], abs(edgePlanes[2].xy)); vec3 intersection[3]; intersection[0] = cross(edgePlanes[0], edgePlanes[1]); intersection[1] = cross(edgePlanes[1], edgePlanes[2]); intersection[2] = cross(edgePlanes[2], edgePlanes[0]); intersection[0] /= intersection[0].z; intersection[1] /= intersection[1].z; intersection[2] /= intersection[2].z; // calculate dilated triangle vertices float z[3]; z[0] = -(intersection[0].x * trianglePlane.x + intersection[0].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z; z[1] = -(intersection[1].x * trianglePlane.x + intersection[1].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z; z[2] = -(intersection[2].x * trianglePlane.x + intersection[2].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z; pos[0].xyz = vec3(intersection[0].xy, z[0]); pos[1].xyz = vec3(intersection[1].xy, z[1]); pos[2].xyz = vec3(intersection[2].xy, z[2]); vec4 voxelPos; ProjectPos = pos[0].xyz; gl_Position = pos[0]; voxelPos = viewProjectInverse[projectIndex] * gl_Position; FragPos = voxelPos.xyz; EmitVertex(); ProjectPos = pos[1].xyz; gl_Position = pos[1]; voxelPos = viewProjectInverse[projectIndex] * gl_Position; FragPos = voxelPos.xyz; EmitVertex(); ProjectPos = pos[2].xyz; gl_Position = pos[2]; voxelPos = viewProjectInverse[projectIndex] * gl_Position; FragPos = voxelPos.xyz; EmitVertex(); EndPrimitive();&#125; &emsp;&emsp;最后的最后，我们还需要在片元着色器剔除无关的片元，具体原因我已经在前面说了，如果不做这一步的剔除操作，将出现如下图12所示的情况。在片元着色器中，我们根据传入的三角形包围盒与当前的片元位置判断是否需要丢弃该片元。具体看下面代码的第19行、第20行。 图12 保守光栅化出现的边边角角 12345678910111213141516171819202122232425262728#version 430 corein vec3 FragPos;in vec3 ProjectPos;in vec4 BoundingBox;layout (std430, binding = 0) buffer CountBuffer&#123; int cnts[];&#125;;uniform bool conservate;uniform float step;uniform vec3 boxMin;uniform vec3 resolution;out vec4 color;void main()&#123; if(ProjectPos.x &lt; BoundingBox.x || ProjectPos.y &lt; BoundingBox.y || ProjectPos.x &gt; BoundingBox.z || ProjectPos.y &gt; BoundingBox.w) discard; uint x = uint((FragPos.x - boxMin.x)/step); uint y = uint((FragPos.y - boxMin.y)/step); uint z = uint((FragPos.z - boxMin.z)/step); uint index = uint(y * (resolution.z * resolution.x) + z * resolution.x + x); atomicAdd(cnts[index], 1); color = vec4(0.0,1.0,0.0,1.0);&#125; &emsp;&emsp;最终，修复了孔洞的效果的如下图，可以看到，对比前面的图9，孔洞基本都被“补”上了。 图13 保守光栅化出现的边边角角 &emsp;&emsp;下面就是一些模型的体素化效果。 参考资料：$[1]$ The Basics of GPU Voxelization $[2]$ 《GPU Gems 2》： Chapter 42. Conservative Rasterization $[3]$ https://blog.csdn.net/xiewenzhao123/article/details/79875855","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Voxelization","slug":"Voxelization","permalink":"http://yoursite.com/categories/Voxelization/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"http://yoursite.com/categories/Position-Based-Dynamics/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Voxelization","slug":"Voxelization","permalink":"http://yoursite.com/tags/Voxelization/"}]},{"title":"流体模拟Fluid Simulation：Position Based Fluid","slug":"PositionBasedFluid","date":"2019-06-04T07:43:02.767Z","updated":"2019-06-04T08:29:48.113Z","comments":true,"path":"2019/06/04/PositionBasedFluid/","link":"","permalink":"http://yoursite.com/2019/06/04/PositionBasedFluid/","excerpt":"本篇文章主要是关于Position Based Dynamics的流体模拟方法，这类方法依旧采用基于拉格朗日的视角，把流体看成由一个一个粒子组成，易于并行化，适用于实时的流体模拟。目前实现的只是CPU版本，考虑在后面利用cuda挪到GPU上做模拟计算。相关的完整代码请看这里。","text":"本篇文章主要是关于Position Based Dynamics的流体模拟方法，这类方法依旧采用基于拉格朗日的视角，把流体看成由一个一个粒子组成，易于并行化，适用于实时的流体模拟。目前实现的只是CPU版本，考虑在后面利用cuda挪到GPU上做模拟计算。相关的完整代码请看这里。 基于位置动力学的物理模拟 基于位置动力学的流体模拟 流体模拟算法实现 实现效果 参考资料 一、基于位置动力学的物理模拟&emsp;&emsp;传统的物理模拟方法都是基于力的方法，这类方法通过计算内部力（如流体内部的粘性力、压力）和外部力（如重力和碰撞力）的合力，然后根据牛顿第二定律计算出加速度，最后根据数值计算方法求出物体的速度和位置。这种方法基本上针对每一种动态物体，会由一个独立的求解器，各种求解器按照一定的顺序计算，从而得到模拟的结果，这样会带来大量冗余的工作。基于位置动力学（Position Based Dynamics）的方法将这些物理运动通过约束表达出来，这样只需要一个求解器即可，更加方便地进行物理模拟。 &emsp;&emsp;下图1是基于力和基于位置动力学的物体碰撞更新过程的对比，可以看到基于力的碰撞检测首先在穿透发生时更新物体的速度，然后更新物体的位置。而基于位置动力学的碰撞检测首先只检测是否发生穿透，然后移动位置使之不发生穿透，最后再据此更新物体的速度信息。 图1 两种碰撞更新过程对比 1、基于位置动力学的模拟算法&emsp;&emsp;基于位置动力学英文全称为Position Based Dynamics，以下简称为PBD。接下来我们介绍经典的PBD算法。在PBD算法中，运动的物体由$N$个顶点和$M$个约束组成。顶点$i\\in [1,…,N]$的质量为$m_i$，位置为$x_i$，速度为$v_i$，每个约束$j\\in [1,…,M]$有如下五个性质： 约束的基数为$n_j$，即第$j$个约束所影响的顶点数目为$n_j$个； 约束函数$C_j:\\ R^{3n_j}\\to R$； 受约束影响的顶点索引值集合$\\{i_1,…,i_{n_j}\\},i_k\\in [1,…N]$； 每个约束都有对应的刚度参数$k_j\\in [0,1]$，这里我们可以理解为约束的强度； 约束分为两种，一类是等式约束即$C_j(x_{i1},x_{i_2},…,x_{i_{n_j}})=0$，另一类是不等式约束$C_j(x_{i_1},x_{i_2},…,x_{i_{n_j}})\\geq 0$。 &emsp;&emsp;给定时间步长$\\Delta t$，PBD的运动物体模拟的算法伪代码如下所示： \\begin{align} &1.forall\\ \\ vertices\\ \\ i:\\\\ &2.\\ \\ \\ \\ initialize\\ \\ x_i=x_i^0,v_i=v_i^0,w_i=1/m_i\\\\ &3.endfor\\\\ &4.loop\\\\ &5.\\ \\ \\ \\ forall\\ \\ vertices\\ \\ i\\ \\ do\\ \\ v_i\\leftarrow v_i+\\Delta tw_if_{ext}(x_i)\\\\ &6.\\ \\ \\ \\ dampVelocities(v_1,...,v_N)\\\\ &7.\\ \\ \\ \\ forall\\ \\ vertices\\ \\ i\\ \\ do\\ \\ p_i\\leftarrow x_i+\\Delta t v_i\\\\ &8.\\ \\ \\ \\ forall\\ \\ vertices\\ \\ i\\ \\ do\\ \\ generateCollisionConstraints(x_i\\to p_i)\\\\ &9.\\ \\ \\ \\ loop\\ \\ solverIterations\\ \\ times\\\\ &10.\\ \\ \\ \\ \\ \\ \\ \\ projectConstraints(C1,...,C_{M+M_{coll}},p_1,...,p_N)\\\\ &11.\\ \\ \\ endloop\\\\ &12.\\ \\ \\ forall\\ \\ vertices\\ \\ i\\\\ &13.\\ \\ \\ \\ \\ \\ \\ \\ v_i\\leftarrow (p_i-x_i)\\Delta t\\\\ &14.\\ \\ \\ \\ \\ \\ \\ \\ x_i\\leftarrow p_i\\\\ &15.\\ \\ \\ endfor\\\\ &16.\\ \\ \\ velocityUpdate(v1,...,v_N)\\\\ &17.endloop\\\\ \\end{align}&emsp;&emsp;在上面的算法第1步到第3步中，我们首先对顶点的位置、速度和质量倒数进行初始化，其中质量的倒数$w_i=1/m_i$，除了可以避免冗余的除法操作外，还可以使用于静态的物体，对于静态的物体我们设为$w_i=0$，这样在后续的更新中都不会产生位置和速度的变化量。第5步中的$f_{ext}$代表不能转换成约束形式的力（如重力），我们根据$f_{ext}$进行一次数值计算预测在$f_{ext}$的作用下的速度$v_i$。紧接着在第6步中我们添加阻尼的作用，阻尼可以理解为物体在运动中发生了能量耗散，从而导致速度有所衰减。第8行主要是生成碰撞约束，物体会与周围的环境发生碰撞，例如布料落在地板上，水碰上一面墙等，这些碰撞约束在每个时间步长都发生改变，所以每一次都需要重新生成碰撞约束。有了内部约束（如不可压缩流体的密度约束）和外部约束（如流体与地面的碰撞约束）之后，我们需要根据这些约束做一个迭代求解，也就是上面伪代码中的第9行到第11行，这里我们称为约束投影步骤。从约束投影步骤我们得到服从给定约束的粒子位置，然后再第12行到第15行更新顶点粒子的速度和位置信息。最后在第16行根据摩擦系数（friction）和恢复系数（restitution）更新速度，如下图2所示。这样，一个完整的PBD物理模拟步骤就完成了。 图2 friction和restitution 2、约束投影步骤&emsp;&emsp;接下来我们就针对约束投影步骤详细展开相关的内容，约束投影是PBD中的最难理解的核心部分，涉及的数学内容比较多一点。设有一个基数为n（也就是前面提到的$n_j$，受到该约束影响的顶点数目或者说粒子数目）的约束，关联的粒子点为$p_1,…,p_n$，约束函数记为$C$，刚度系数（stiffness）为$k$。记$p=[p_1^T,…,p_n^T]^T$，则等式约束函数表示为： C(p)=0 \\tag {1}&emsp;&emsp;我们的目标是计算这样的一个位移偏移量$\\Delta p$，使得粒子顶点在$p+\\Delta p$处约束条件依然满足，即： C(p+\\Delta p)=0 \\tag {2}&emsp;&emsp;对约束函数$C$做一阶泰勒展开（或者导数的定义），则可得: C(p+\\Delta p)\\approx C(p)+\\nabla_pC(p)\\cdot\\Delta p=0 \\tag {3}&emsp;&emsp;为了使粒子在$p+\\Delta p$处依然满足约束条件，我们要求解方程$(3)$得到$\\Delta p$。PBD算法的一个巧妙之处在于它将$\\Delta p$的方向限制在约束函数的梯度方向$\\nabla_p C(p)$上。如下图3所示，约束$C$所涉及到的粒子位置会形成一个高维空间，下图为该空间中满足不同约束条件的粒子位置形成的二维等值线示意图，其中满足$C$约束条件的是黑色等值线。故当粒子处于下图的黑色点的位置时，不满足约束条件，如果我们沿着点所在的等值线（灰色曲线）移动，此时刚体模态（Rigid body modes）的方向与该等值线相同，新得到的位置仍然在该灰色等值线上，依然不在黑色曲线 $C=0$上，即不满足约束条件。这可以理解为，约束中存在的误差依然没有得到修正。以两个粒子形成的距离约束为例，就好比同时移动了两个粒子或者该约束绕自身旋转，但是存在的误差并没有得到更正。而且这样一来还会引入系统中不存在的一种外力，导致系统动量不守恒。所以，我们希望该点的位移方向与刚体模态方向垂直，从而保证系统动量守恒，即从黑点指向红点的方向$\\nabla C$。 图3 约束等值线 &emsp;&emsp;因此，我们令位移向量$\\Delta p$为约束函数的梯度向量$\\nabla_p C$再乘上一个标量缩放系数$\\lambda$： \\Delta p=\\lambda \\nabla_p C(p) \\tag {4}&emsp;&emsp;其中的标量缩放系数$\\lambda$我们称之为拉格朗日乘子（Lagrange multiplier）。联立公式$(3)$和$(4)$我们可得： \\lambda=-\\frac{C(p)}{|\\nabla_pC(p)|^2} \\tag {5}&emsp;&emsp;然后将$\\lambda$再代入公式$(4)$我们可得$\\Delta p$的表达式： \\Delta p=\\lambda \\nabla_pC(p)=-\\frac{C(p)}{|\\nabla_pC(p)|^2}\\nabla_pC(p) \\tag {6}&emsp;&emsp;具体到粒子$i$，约束投影后其对应的位移向量为： \\Delta p_i=-s\\nabla_{p_i}C(p_1,...,p_n) \\tag {7}&emsp;&emsp;其中的$s$为如下所示，$s$的值对于约束函数$C$作用范围内的所有点都一样。 s=\\frac{C(p_1,...,p_n)}{\\Sigma_j|\\nabla_{p_j}C(p_1,...,p_n)|^2} \\tag {8}&emsp;&emsp;前面我们假定所有的粒子质量都相同，现在考虑粒子质量不同的情况。记粒子$i$的质量为$m_i$，其质量的倒数为$w_i=1/m_i$，则公式$(4)$变为： \\Delta p_i=\\lambda w_i\\nabla_{p_i}C(p) \\tag {9}&emsp;&emsp;公式$(7)$和公式$(8)$变为： \\Delta p_i=-s w_i\\nabla_{p_i}C(p_1,...,p_n) \\tag {10} s=\\frac{C(p_1,...,p_n)}{\\Sigma_jw_j|\\nabla_{p_j}C(p_1,...,p_n)|^2} \\tag {11}&emsp;&emsp;为了便于理解，接下来我们举个简单的例子应用约束投影方法。如下图4所示。 图4 简单的约束例子 &emsp;&emsp;上面的约束可以表示为$C(p_1,p_2)=|p_1-p_2|-d$，位移向量记为$\\Delta p_i$。根据约束投影方法，我们首先约束函数$C(p_1,p_2)$关于$p_1$和$p_2$的梯度，也就是求偏导数。注意到$C(p_1,p_2)=|p_1-p_2|-d=(\\sqrt{(p_1-p_2)^2})-d$，我们可以求得以下的梯度向量表达式： \\nabla_{p_1}C(p_1,p_2)=\\frac{p_1-p_2}{|p_1-p_2|}\\\\ \\nabla_{p_2}C(p_1,p_2)=-\\frac{p_1-p_2}{|p_1-p_2|} \\tag {12}&emsp;&emsp;注意，上面求到的是一个矢量，也就是我们说的梯度向量。将公式$(12)$代入公式$(11)$可得： \\begin{align} s=&\\frac{C(p_1,...,p_n)}{\\Sigma_jw_j|\\nabla_{p_j}C(p_1,...,p_n)|^2}\\\\ =&\\frac{|p_1-p_2|-d}{w_1|\\nabla_{p_1}C(p_1,p_2)|^2+w_2|\\nabla_{p_2}C(p_1,p_2)|^2}\\\\ =&\\frac{|p_1-p_2|-d}{w_1+w_2} \\tag {13} \\end{align}&emsp;&emsp;最后，将公式$(13)$代入到公式$(10)$，可得约束投影计算得到的位移： \\begin{align} \\Delta p_1=&-\\frac{|p_1-p_2|-d}{w_1+w_2}w_1\\nabla_{p_1}C(p_1,p_2)\\\\ =&-\\frac{w_1}{w_1+w_2}(|p_1-p_2|-d)\\frac{p_1-p_2}{|p_1-p_2|} \\end{align}&emsp;&emsp;同理$\\Delta p_2$如下所示： \\Delta p_2=+\\frac{w_2}{w_1+w_2}(|p_1-p_2|-d)\\frac{p_1-p_2}{|p_1-p_2|}&emsp;&emsp;前面我们提到每个约束都有对应的刚度系数$k$，令$k’=1-(1-k)^{1/n_s}$去乘$\\Delta p$，这里$n_s$迭代之后误差为$\\Delta p(1-k’)^{n_s}=\\Delta p(1-k)$，与刚度系数成线性关系，而与迭代次数$n_s$无关。下一个时间步的位置如下所示： p_1^{t+1}=p_1^t+k'\\Delta p_1\\\\ p_2^{t+1}=p_2^t+k'\\Delta p_23、约束投影求解器&emsp;&emsp;前面的伪代码中我们可以看到约束投影的输入为$M+M_{coll}$个约束和$N个$点的预测位置$p1,…,p_N$，所需要求解的方程组是非线性非对称方程组或不等式组（碰撞约束产生的）。约束投影步骤的主要任务就是修正预测位置使新得到的校正位置满足所有约束。但是一般情况下很难找到一个适当的$\\Delta p=[\\Delta p_1^T,…,\\Delta p_n^T]^T$恰好使得所有的约束都能够同时得到满足，故我们通常采用迭代的方法按顺序依次对约束进行求解。 &emsp;&emsp;我们可以采用非线性高斯-赛德尔（Non-Linear Gauss-Seidel，简称NGS）迭代方法。高斯赛德尔（Gauss-Sedel，简称GS）迭代方法只能求解线性方程组，NGS在依次求解德基础上，加入了约束投影求解这一非线性操作。与雅可比迭代方法（Jacobi method）不同，NGS求解器在一次迭代中对于顶点位置的修正立即被应用到下一个约束求解中，这样的好处就是显著加快了收敛速度。 &emsp;&emsp;但是NGS虽然稳定且容易实现，但是该方法收敛速度依然不是很快，不宜并行化。 二、基于位置动力学的流体模拟&emsp;&emsp;前面部分主要介绍了Position Based Dynamics算法相关的内容，接下来我们就看看如何将其PBD算法应用到流体模拟当中，主要是如何针对流体的物理特性构建相应的约束函数。基于位置动力学的流体全称为Position Based Fluid，简称PBF。 1、不可压缩约束&emsp;&emsp;在不可压缩性的流体模拟中，我们需要使粒子$i$的密度$\\rho_i$尽量与静态的密度$\\rho_0$相同，即$\\rho_i=\\rho_0$。因此，我们需要对每一个流体粒子都施加一个常量密度约束，PBF的常量密度约束如下所示： C_i(p_1,...,p_n)=\\frac{\\rho_i}{\\rho_0}-1 \\tag {14}&emsp;&emsp;公式$(14)$中，我们记粒子$i$的位置为$p_i$，$p_1,…,p_n$是与粒子$i$相邻的粒子。可以看到当密度约束$C_i(p_1,…,p_n)=0$时有$\\rho_i=\\rho_0$，此时流体的体积即不压缩也不膨胀，从而保证了流体的不可压缩条件，这就是公式$(14)$的由来。流体粒子$i$的密度根据SPH（Smoothed Particle Hydrodynamics，光滑粒子流体动力学，简称SPH）方法的计算公式如下所示： \\rho_i=\\Sigma_jm_jW(p_i-p_j,h) \\tag {15}&emsp;&emsp;在公式$(15)$中，$m_j$是邻居粒子$j$的质量，$h$是指定的光滑核半径。$W$函数我们接下来会提到。将公式$(15)$代入公式$(14)$，我们有： C_i(p_1,...,p_n)=\\frac{\\Sigma_j m_jW(p_i-p_j,h)}{\\rho_0}-1 \\tag {16}&emsp;&emsp;在公式$(15)$的密度计算中，PBF方法采用了Poly6核函数： W_{poly6}(r,h)=\\frac{315}{64\\pi h^9} \\begin{cases} (h^2-|r|^2)^3\\ \\ \\ \\ 0\\leq|r|\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ otherwise \\end{cases} \\tag {17}&emsp;&emsp;但是在计算密度的梯度时，却又采用了Spiky核函数： W_{spiky}(r,h)=\\frac{15}{\\pi h^6} \\begin{cases} (h-|r|)^3\\ \\ \\ \\ 0\\leq|r|\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 0therwise \\end{cases} \\tag {18}&emsp;&emsp;对公式$(18)$求关于$r$的导数（注意，$|r|=\\sqrt{r^2}$，不能直接对$|r|$求导），从而流体粒子密度的梯度如下所示： \\nabla W_{spiky}(r,h)=-\\frac{45}{\\pi h^6} \\begin{cases} (h-|r|)^2\\frac{r}{|r|}\\ \\ \\ \\ 0\\leq|r|\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ otherwise \\end{cases} \\tag {19}&emsp;&emsp;因此，粒子$i$的约束函数$(16)$是一个关于$p_1,…,p_n$的非线性方程组$C_i(p_1,…,p_n)=0$，所有粒子$i$的约束组成了一个非线性方程组。在PBF方法中，我们只考虑粒子质量相同的情况，故我们可以省去公式$(15)$和公式$(16)$中的质量$m_j$，即： \\rho_i=\\Sigma_jW(p_i-p_j,h) \\tag {20} C_i(p_1,...,p_n)=\\frac{\\Sigma_j W(p_i-p_j,h)}{\\rho_0}-1 \\tag {21}&emsp;&emsp;然后求约束函数$C_i$关于$p_k$的梯度如下，其中$k\\in\\{1,2,…,n\\}$： \\nabla_{p_k}C_i=\\frac1\\rho_0\\Sigma_j\\nabla_{p_k}W(p_i-p_j,h) \\tag {22}&emsp;&emsp;显然，针对$k$的不同，分为两种情况。当$k=i$也就是粒子本身的时候，连加符号中的$W$均为关于$p_k$的函数；当$k=j$即邻居粒子的时候，只有$W(p_i-p_k,h)$才有意义，其他相对于$p_k$来说都是常量，故导数为0（注意用到了求导的链式法则）： \\nabla_{p_k}C_i=\\frac1\\rho_0 \\begin{cases} \\Sigma_j\\nabla_{p_k}W(p_i-p_j,h)\\ \\ \\ \\ if\\ \\ k=i\\\\ -\\nabla_{p_k}W(p_i-p_j,h)\\ \\ \\ \\ \\ if\\ \\ k=j \\end{cases} \\tag {23}&emsp;&emsp;既然求出了约束函数的梯度，我们就把它应用到前面提到的拉格朗日乘子的计算公式中，联立公式$(5)$和公式$(23)$，我们有： \\lambda_i=-\\frac{C_i(p_1,...,p_n)}{\\Sigma_k|\\nabla_{p_k}C_i|^2} \\tag {24}2、混合约束&emsp;&emsp;如果一个约束条件不能被违背，我们称之为硬约束；而能一定程度上被违背的约束称为软约束。在理想的情况下，我们都希望约束始终是硬约束，但是由于误差或者数值方法的不稳定等原因，我们有时不得不向软约束妥协。 &emsp;&emsp;在PBF中，当$|r|=h$，粒子$i$与粒子$j$之间的距离等于光滑核半径时，粒子$i$和粒子$j$处于即将分离的状态。注意观察公式$(19)$的密度梯度计算公式，此时$\\nabla W_{spiky}(r,h)=0$。若所有的邻居粒子与粒子$i$都处于这种状态，那么必将导致约束函数的梯度即公式$(22)$取值为0： \\nabla_{p_k}C_i=\\frac1\\rho_0\\Sigma_j\\nabla_{p_k}W(p_i-p_j,h) = 0&emsp;&emsp;从而导致公式$(24)$中的分母$\\Sigma_k|\\nabla_{p_k}C_i|^2$为0，出现除零错误，这将导致PBF方法出现潜在的不稳定性。为了解决这个问题，PBF采用混合约束的方法，使密度硬约束转变成软约束。具体的做法就是将根据密度函数求解得到的约束力再加入到原始的约束函数中，这里在PBF的常量密度约束中得到的拉格朗日乘子$\\lambda$有类似的作用，故将$\\lambda$加入到初始的约束方程（即公式$(3)$）： C(p+\\Delta p)\\approx C(p)+\\nabla C^T\\nabla C \\lambda + \\epsilon\\lambda=0 \\tag {25}&emsp;&emsp;公式$(25)$中的$\\epsilon$是松弛参数，可以由用户指定。引入公式$(25)$后，拉格朗日乘子的计算公式$(24)$就变为： \\lambda_i=-\\frac{C_i(p_1,...,p_n)}{\\Sigma_k|\\nabla_{p_k}C_i|^2+\\epsilon} \\tag {26}&emsp;&emsp;从而可得粒子$i$在经过上述约束投影后对应的位移向量（包括自身密度约束以及邻居粒子密度约束共同作用的结果。注意，这里对应的上面的公式$(4)$，结合公式$(23)$）： \\begin{align} \\Delta p_i&=\\lambda_i \\nabla_{p_i}C_i+\\Sigma_j\\lambda_j\\nabla_{p_j}C_i\\\\ &=\\frac1\\rho_0\\Sigma_j\\lambda_i\\nabla_{p_i}W(r,h)+(-\\frac1\\rho_0\\Sigma_j\\lambda_j\\nabla_{p_j}W(r,h))\\\\ &=\\frac1\\rho_0\\Sigma_j\\lambda_i\\nabla_{p_i}W(r,h)+\\frac1\\rho_0\\Sigma_j\\lambda_j\\nabla_{p_i}W(r,h)\\\\ &=\\frac{1}{\\rho_0}\\Sigma_j(\\lambda_i+\\lambda_j)\\nabla_{p_i}W(r,h) \\end{align} \\tag {27}3、拉伸不稳定性&emsp;&emsp;PBF采用SPH的方法来计算流体粒子的密度，但是该方法通常需要30~40个邻居粒子才能使密度求值结果趋于静态密度。在邻居粒子数量较少的情况下，通过该方法计算得到的流体密度低于静态密度，由此会造成流体内部压强为负数，原本粒子间的压力变为吸引力，使得流体粒子凝聚在一起，导致流体表面的模拟效果失真。PBF采用了一种人工排斥力的计算模型，当流体粒子距离过近时该排斥力会使它们分开，避免产生粒子聚集的现象。在公式$(24)$的基础上，加入一个排斥项（repulsive term）$s_{corr}$： \\Delta p_i=\\frac1\\rho_0\\Sigma_j(\\lambda_i+\\lambda_j+s_{corr})\\nabla_{p_i}W(p_i-p_j,h) \\tag {28}&emsp;&emsp;其中的$s_{corr}$计算方式如下： s_{corr}=-k(\\frac{W(p_i-p_j,h)}{W(\\Delta q,h)})^n \\tag {29}&emsp;&emsp;公式$(29)$中，$\\Delta q$表示到粒子$i$的一个固定距离，通常取$|\\Delta q|=0.1h,…,0.3h$，$h$即前面提到的光滑核半径。此外，公式中的$k$可以看作表面张力参数，取值$k=0.1$，而$n=4$。公式$(28)$中的排斥项会使得流体粒子的密度稍微低于静态密度，从而产生类似于表面张力的效果，使得流体表面的的粒子分布均匀。通过这个排斥项，我们不再需要硬性规定流体的邻居数量必须在30~40个，进一步提升算法的流体模拟效率。 4、涡轮控制和人工粘性&emsp;&emsp;由于数值耗散，PBD的方法会引入额外的阻尼，使得整个系统的能量损耗太快，导致本来应该由的一些涡流细节迅速消失。在这里，PBF通过涡轮控制方法向整个系统重新注入能量： f_i^{vorticity}=\\epsilon (N\\times \\omega_i) \\tag {30}&emsp;&emsp;上述的公式中，$N=\\frac{\\eta}{|\\eta|},\\ \\eta=\\nabla|\\omega|_i$，而流体粒子的旋度$\\omega_i$计算公式如下： \\omega_i=\\nabla\\times v=\\Sigma_j(v_j-v_i)\\times \\nabla_{p_j}W(p_i-p_j,h) \\tag {31}&emsp;&emsp;涡轮控制方法的基本思路就是：通过添加一个体积力$f_i^{vorticity}$（在算法的第一步），在旋度粒子（可直观理解为比周围粒子旋转快的粒子，旋度$\\omega_i$指向粒子$i$的旋转轴）处加速粒子的旋转运动，通过这种方式来增加系统的旋度细节。公式$(30)$中的$\\epsilon$用于控制涡轮控制力的强度。 &emsp;&emsp;最后，PBF方法采用XSPH的粘度方法直接更新速度，从而产生粘性阻尼。人工粘性除了可以增加模拟的数值稳定性，还可以消除非物理的流体振荡。拉格朗日流体模拟方法中，人工粘性本质上会对流体粒子的相对运动产生阻尼作用，使流体的动能转化为热能： v_i^{new}=v_i+c\\Sigma_j(v_i-v_j)\\cdot W(p_i-p_j,h) \\tag {32}&emsp;&emsp;在流体模拟中，我们取公式$(32)$中的$c=0.01$。 5、PBF算法&emsp;&emsp;PBF算法的总体框架就是按照前面提到的PBD算法，只是经典PBD算法采用了顺序高斯-赛德尔（Sequential Gauss-Seidel，SGS）迭代求解，而SGS不容易被GPU并行化，因此基于CUDA实现的PBF求解器使用了雅克比（Jacobi）迭代方法并行求解。 &emsp;&emsp;PBF的算法伪代码如下所示： \\begin{align} &1.\\ forall\\ \\ particles\\ \\ i\\ \\ do\\\\ &2.\\ \\ \\ \\ \\ apply\\ \\ force\\ \\ v_i\\leftarrow v_i+\\Delta tf_{ext}(x_i)\\\\ &3.\\ \\ \\ \\ \\ predict\\ \\ position\\ \\ x_i^*\\leftarrow x_i+\\Delta t v_i\\\\ &4.\\ endfor\\\\ &5.\\ forall\\ \\ particles\\ \\ i\\ \\ do\\\\ &6.\\ \\ \\ \\ \\ find\\ \\ neighboring\\ \\ particles\\ \\ N_i(x_i^*)\\\\ &7.\\ endfor\\\\ &8.\\ while\\ \\ iter\\ \\","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"http://yoursite.com/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"http://yoursite.com/tags/Fluid-Simulation/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"http://yoursite.com/tags/Position-Based-Dynamics/"}]},{"title":"光线追踪器Ray Tracer：进阶篇","slug":"RayTracer-Advance","date":"2019-05-23T13:08:38.407Z","updated":"2019-05-26T03:37:23.595Z","comments":true,"path":"2019/05/23/RayTracer-Advance/","link":"","permalink":"http://yoursite.com/2019/05/23/RayTracer-Advance/","excerpt":"本篇文章在前面的基础上，丰富光线追踪器的各种特性。本篇内容主要包含添加纹理映射、平面光源和球形光源、三角网格模型渲染、增加天空盒背景、构建BVH树、tbb多线程渲染加速、蒙特卡罗积分方法、重要性采样，后面部分涉及的高等数学和概率论内容较多。相关的全部代码在这里。","text":"本篇文章在前面的基础上，丰富光线追踪器的各种特性。本篇内容主要包含添加纹理映射、平面光源和球形光源、三角网格模型渲染、增加天空盒背景、构建BVH树、tbb多线程渲染加速、蒙特卡罗积分方法、重要性采样，后面部分涉及的高等数学和概率论内容较多。相关的全部代码在这里。 纹理映射 三角网格模型 添加光源 天空盒背景 构建BVH树 tbb多线程渲染 蒙特卡罗积分 重要性采样 MC光线追踪 程序效果 参考资料 一、纹理映射&emsp;&emsp;纹理映射对渲染的重要性不言而喻，为了丰富物体表面的细节，我们在这里创建一个纹理加载和采样的类。实际上，除了图片纹理，还有一些过程式产生的纹理。我们创建一个虚类$Texture$，并将$sample$类作为虚接口。然后创建子类$ImageTexture$，图片的加载我采用了stb_image库。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Texture&#123;public: typedef std::shared_ptr&lt;Texture&gt; ptr; Texture() = default; virtual ~Texture() = default; virtual Vector3D sample(const float &amp;u, const float &amp;v, const Vector3D &amp;p) const = 0;&#125;;class ImageTexture : public Texture&#123;private: unsigned char *m_data; int m_width, m_height, m_channel; public: typedef std::shared_ptr&lt;ImageTexture&gt; ptr; ImageTexture() = default; ImageTexture(const std::string &amp;path); virtual ~ImageTexture(); virtual Vector3D sample(const float &amp;u, const float &amp;v, const Vector3D &amp;p) const; &#125;;ImageTexture::ImageTexture(const std::string &amp;path)&#123; m_data = stbi_load(path.c_str(), &amp;m_width, &amp;m_height, &amp;m_channel, 0); if (m_data == nullptr) std::cout &lt;&lt; \"Failed to load the image-&gt;\" &lt;&lt; path.c_str() &lt;&lt; std::endl;&#125;Vector3D ImageTexture::sample(const float &amp;u, const float &amp;v, const Vector3D &amp;p) const&#123; int i = static_cast&lt;int&gt;(u * m_width); int j = static_cast&lt;int&gt;((1.0f - v)*m_height) - 0.001; if (i &lt; 0) i = 0; if (j &lt; 0) j = 0; if (i &gt; m_width - 1) i = m_width - 1; if (j &gt; m_height - 1) j = m_height - 1; int index = (j * m_width + i) * m_channel; float r = static_cast&lt;int&gt;(m_data[index + 0]) / 255.0f; float g = static_cast&lt;int&gt;(m_data[index + 1]) / 255.0f; float b = static_cast&lt;int&gt;(m_data[index + 2]) / 255.0f; return Vector3D(r, g, b);&#125; &emsp;&emsp;纹理坐标转换为像素数组下标要注意是否越界了，这里实现的纹理环绕方式是clamp。然后对于球体，我们要计算球体上每个点的纹理坐标，这里采用球面坐标的一个技巧。球体的上每一个点，都对应着一组唯一的方向角和天顶角$(\\theta,\\phi)$，我们把$(\\theta,\\phi)$映射到二维纹理坐标即可。映射方法如下： u = \\phi/(2\\pi) \\\\ v = \\theta/\\pi \\tag {1}&emsp;&emsp;那么如何根据一个球面的点计算它的方向角和天顶角呢？从球面坐标$(\\theta, \\phi)$转到笛卡尔坐标$(x,y,z)$，不难理解，有如下关系： x = cos(\\phi)cos(\\theta) \\\\ y = sin(\\phi)cos(\\theta) \\\\ z = sin(\\theta) \\tag {2}&emsp;&emsp;注意到$y/x=tan(\\phi)$，所以我们可以采用下面的方式得到球面上点的天顶角和方位角： \\phi=atan2(y,x)\\\\ \\theta=asin(2) \\tag {3}&emsp;&emsp;需要注意的是，$atan2$函数返回的角度范围是$[-\\pi,+\\pi]$，$asin$返回的角度范围是$[-\\pi/2,\\pi/2]$。 1234567static void getSphereUV(const Vector3D &amp;p, Vector2D &amp;tex)&#123; float phi = atan2(p.z, p.x); float theta = asin(p.y); tex.x = 1 - (phi + M_PI) / (2*M_PI); tex.y = (theta + M_PI/2) / M_PI;&#125; 二、三角网格模型&emsp;&emsp;除了像球体、圆柱、圆锥等等这类有显示数学表达式的几何体，我们接触到更多的是没有表达式的网格模型。有显示的数学表达式当然好，因为我们直接直接求交点的解析解，非常准确。这里我们构建一个通用的网格模型类，它由一个个三角形构成。obj模型的导入我不再赘述，这里重点讲述了射线与三角形求交的推导过程。 &emsp;&emsp;一个三角形由空间中的三个顶点$P_0$、$P_1$、$P_2$的位置表示，三角形所在平面的法向量$N$可由下式计算而得： N=(P_1-P_0)\\times(P_2-P_0) \\tag {4}&emsp;&emsp;平面与原点的距离$d$等于平面法向量$N$与平面中任意一点的内积的负数，这里选$P_0$，则$d$为： d = -N\\cdot P_0 \\tag {5}&emsp;&emsp;则三角形所在的平面可以用四维向量$(N, -N\\cdot P_0)$表示，实际上三角形所在平面的表达式为$N\\cdot(x,y,z)+d=0$，首先我们求射线与该平面的交点，然后再判断交点是否在三角形内部。将射线方程$P(t) = S+tV$带入平面的方程，则有： N\\cdot P(t)+d=0\\\\ \\to N\\cdot S +(N\\cdot V)t +d=0\\\\ \\to t=-\\frac{(N\\cdot S+d)}{N\\cdot V}&emsp;&emsp;通过以上的方程我们就可以得到射线在平面$L$上的交点处的$t$值。需要注意的是，当$N\\cdot V=0$时，射线与平面平行，不存在交点。然后我们把$t$值带入射线方程即可求出射线与平面的交点$P$。接下来的问题是判断点$P$是否位于三角形内部，通过计算点$P$对于三角形的三个顶点$P_0$、$P_1$、$P_2$的重心坐标可以完成该判断。重心坐标是三角形顶点加权平均值，由三个标量$\\omega_0$、$\\omega_1$和$\\omega_2$组成，有： P=\\omega_0 P_0+\\omega_1 P_1 + \\omega_2 P_2 \\tag {6}&emsp;&emsp;其中，$\\omega_0+\\omega_1+\\omega_2 =1$，用$1-\\omega_1-\\omega_2$代替$\\omega_0$，可得： P=(1-\\omega_1-\\omega_2)P_0+\\omega_1P_1+\\omega_2P_2 \\\\ =P_0+\\omega_1(P_1-P_0)+\\omega_2(P_2-P_0) \\tag {7}&emsp;&emsp;定义以下的等式： R=P-P_0\\\\ Q_1=P_1-P_0\\\\ Q_2=P_2-P_0 \\tag {8}&emsp;&emsp;将公式$(9)$带入公式$(8)$，可得： R=\\omega_1Q_1+\\omega_2Q_2 \\tag {9}&emsp;&emsp;分别给式$(10)$两边乘$Q_1$和$Q_2$可得以下两个方程： R\\cdot Q_1=\\omega_1Q_1^2+\\omega_2(Q_1\\cdot Q_2)\\\\ R\\cdot Q_2=\\omega_1(Q_1\\cdot Q_2)+\\omega_2Q_2^2 \\tag {10}&emsp;&emsp;写成矩阵形式如下： \\left[ \\begin{matrix} Q_1^2 & Q_1\\cdot Q_2\\\\ Q_1\\cdot Q_2 & Q^2_2 \\end{matrix} \\right] \\left[ \\begin{matrix} \\omega_1\\\\ \\omega_2 \\end{matrix} \\right] = \\left[ \\begin{matrix} R\\cdot Q_1\\\\ R\\cdot Q_2 \\end{matrix} \\right] \\tag {11}&emsp;&emsp;解以上关于$\\omega_1$和$\\omega_2$的方程，可得： \\left[ \\begin{matrix} \\omega_1\\\\ \\omega_2 \\end{matrix} \\right] = \\left[ \\begin{matrix} Q_1^2 & Q_1\\cdot Q_2\\\\ Q_1\\cdot Q_2 & Q^2_2 \\end{matrix} \\right]^{-1} \\left[ \\begin{matrix} R\\cdot Q_1\\\\ R\\cdot Q_2 \\end{matrix} \\right] \\\\ =\\frac1{Q^2_1Q^2_2-(Q_1\\cdot Q_2)^2} \\left[ \\begin{matrix} Q_2^2 & -Q_1\\cdot Q_2 \\\\ -Q_1\\cdot Q_2 & Q_1^2 \\end{matrix} \\right] \\left[ \\begin{matrix} R\\cdot Q_1\\\\ R\\cdot Q_2 \\end{matrix} \\right] \\tag {12}&emsp;&emsp;当且仅当$\\omega_0$、$\\omega_1$和$\\omega_2$三个权值均为非负值时，点$R$位于三角形内部，由于$\\omega_0=1-\\omega_1-\\omega_2$，则此时应有$\\omega_1+\\omega_2\\leq 1$且$\\omega1 \\geq 0\\ and\\ \\omega_2\\geq0$。若顶点$P_0$、$P_1$和$P_2$上关联有一些属性信息，如颜色、法向量或者纹理坐标，则可以利用权值$\\omega_0$、$\\omega_1$和$\\omega_2$对这些属性信息进行插值。 123456789101112131415161718192021222324252627282930313233343536bool MeshHitable::triangleHit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret, const Vertex &amp;p0, const Vertex &amp;p1, const Vertex &amp;p2, const Vector3D &amp;normal) const&#123; float n_dot_dir = normal.dotProduct(ray.getDirection()); // no intersection. if (equal(n_dot_dir, 0.0)) return false; float d = -normal.dotProduct(p0.m_position); float t = -(normal.dotProduct(ray.getOrigin()) + d) / n_dot_dir; if (t &lt; t_min || t &gt; t_max) return false; ret.m_t = t; ret.m_position = ray.pointAt(t); ret.m_material = m_material; // judge inside or not. Vector3D r = ret.m_position - p0.m_position; Vector3D q1 = p1.m_position - p0.m_position; Vector3D q2 = p2.m_position - p0.m_position; float q1_squaredLen = q1.getSquaredLength(); float q2_squaredLen = q2.getSquaredLength(); float q1_dot_q2 = q1.dotProduct(q2); float r_dot_q1 = r.dotProduct(q1); float r_dot_q2 = r.dotProduct(q2); float determinant = 1.0f / (q1_squaredLen * q2_squaredLen - q1_dot_q2 * q1_dot_q2); float omega1 = determinant * (q2_squaredLen * r_dot_q1 - q1_dot_q2 * r_dot_q2); float omega2 = determinant * (-q1_dot_q2 * r_dot_q1 + q1_squaredLen * r_dot_q2); if (omega1 + omega2 &gt; 1.0f || omega1 &lt; 0.0f || omega2 &lt; 0.0f) return false; ret.m_normal = p0.m_normal * (1.0f - omega1 - omega2) + p1.m_normal * omega1 + p2.m_normal * omega2; ret.m_texcoord = p0.m_texcoord * (1.0f - omega1 - omega2) + p1.m_texcoord * omega1 + p2.m_texcoord * omega2; if (ret.m_normal.dotProduct(ray.getDirection()) &gt; 0.0f) ret.m_normal = -ret.m_normal; return true;&#125; &emsp;&emsp;既然模型是由一个个三角形组成，那么在判断射线与当前的模型是否存在交点时，我们就遍历所有的三角形，一个一个三角形与射线做相交判断： 1234567891011121314151617181920212223bool MeshHitable::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const&#123; HitRecord tmpRec; bool hitAny = false; float closestSoFar = t_max; for (int x = 0; x &lt; m_indices.size(); x += 3) &#123; int index1 = m_indices[x + 0]; int index2 = m_indices[x + 1]; int index3 = m_indices[x + 2]; if (triangleHit(ray, t_min, closestSoFar, tmpRec, m_vertices[index1], m_vertices[index2], m_vertices[index3], m_faceNormal[x / 3])) &#123; hitAny = true; closestSoFar = tmpRec.m_t; ret = tmpRec; &#125; &#125; return hitAny;&#125; 三、添加光源&emsp;&emsp;光源是一种特殊的物体，一般情况下它不反射、折射光线，而是自身发射光线。因此，为了实现一个光源，当我们的射线碰撞到光源表面时，我们直接返回光源的碰撞点的颜色，不再做折射和反射。我们将发光的逻辑放到材质中，并将发光这一行为抽象为$emitted$函数。对于非光源物体，我们可以看成发出的光rgb均为0。 123456789101112131415161718192021222324252627282930313233343536373839404142class Material&#123;public: typedef std::shared_ptr&lt;Material&gt; ptr; Material() = default; virtual ~Material() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, ScatterRecord &amp;srec) const &#123; return false; &#125; virtual float scattering_pdf(const Ray &amp;in, const HitRecord &amp;rec, const Ray &amp;scattered) const &#123; return 1.0f; &#125; virtual Vector3D emitted(const Ray &amp;in, const HitRecord &amp;rec, const float &amp;u, const float &amp;v, const Vector3D &amp;p) const &#123; return Vector3D(0.0f, 0.0f, 0.0f); &#125;&#125;;class DiffuseLight : public Material&#123;private: unsigned int m_emitTex; public: typedef std::shared_ptr&lt;DiffuseLight&gt; ptr; DiffuseLight(unsigned int a) : m_emitTex(a) &#123; &#125; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, ScatterRecord &amp;srec) const &#123; return false; &#125; virtual Vector3D emitted(const Ray &amp;in, const HitRecord &amp;rec, const float &amp;u, const float &amp;v, const Vector3D &amp;p) const;&#125;;Vector3D DiffuseLight::emitted(const Ray &amp;in, const HitRecord &amp;rec, const float &amp; u, const float &amp; v, const Vector3D &amp; p) const&#123; return TextureMgr::getSingleton()-&gt;getTexture(m_emitTex)-&gt;sample(u, v, p);&#125; &emsp;&emsp;这样，对于任意的物体，我们都可以把它当作一个光源，只要给这个物体赋予的材质为$DiffuseLight$即可，同时要注意给发光材质设置一个纹理。 四、天空盒背景&emsp;&emsp;之前在光线投射到背景中时，我们是直接返回设定的背景颜色（或通过插值、或直接指定背景）。同样，我们可以通过天空盒来丰富我们的场景细节。天空盒的相关原理比较简单，不再赘述。一个天空盒用边长为1的立方体表示，一个立方体我采用多个三角形构成立方体网格。这里有个问题，就是如何实现天空盒永远无法靠近的效果。在实时渲染时我们直接移除视图矩阵的位移，在光追这里我们直接将光源的出发点设为原点，方向保持不变，这样的一条射线再与天空盒立方体做求交并采样纹理即可。 1234567891011121314151617181920212223242526272829Vector3D Skybox::sampleBackground(const Ray &amp;ray)&#123; HitRecord rec; Ray r(Vector3D(0,0,0), ray.getDirection()); TextureMgr::ptr texMgr = TextureMgr::getSingleton(); int index = -1; for (int x = 0; x &lt; m_indices.size(); x += 3) &#123; int index1 = m_indices[x + 0]; int index2 = m_indices[x + 1]; int index3 = m_indices[x + 2]; if (triangleHit(r, 0.001f, FLT_MAX, rec, m_vertices[index1], m_vertices[index2], m_vertices[index3], m_vertices[index1].m_normal)) &#123; index = x; break; &#125; &#125; if(index != -1) &#123; int map = index / 6; return texMgr-&gt;getTexture(m_cubemap[map]) -&gt;sample(rec.m_texcoord.x, rec.m_texcoord.y, rec.m_position); &#125; else return Vector3D(0.0,0.0,0.0);&#125; 五、构建BVH树&emsp;&emsp;在整个光线追踪算法的渲染过程中，计算量最大的就是光线与场景图元的求交过程。如果不采用一些特殊的数据结构而只是用线性表存储场景物体的话，那么每一条射线都需要对这个存储场景物体的线性表遍历一次，这个射线碰撞检测的算法时间复杂度是$O(n)$的，当$n$比较大时，那么射线碰撞检测需要耗费绝大部分的光线追踪算法时间。射线相交检测的时间是目前光线追踪算法从理论到大规模实际应用过渡的主要瓶颈。为此，我们需要一些特殊的场景管理数据结构来加速这个过程，BVH树（全称为bounding volume hierachy，即层次包围体）是光线追踪领域常用的一种3D场景管理数据结构。它的启发思路就是通过一个简单的包围盒把物体包围起来，射线和场景中的物体求交之前，会先和这个包围盒进行求交，如果该射线没有碰到该包围盒，表明该直线一定不会和包围盒里的物体相交；如果该射线碰到该包围盒，那么再来计算射线是否和包围盒中的物体相交。我们采用包围体是AABB包围盒（即axis-aligned minimum bounding box，轴对齐的最小包围盒，简称轴向包围盒）。 &emsp;&emsp;BVH树本质上是对空间做分割，然后采用二分搜索快速判断射线会与哪些包围盒发生碰撞，从而使得算法的时间复杂度从$O(n)$降到了$O(log(n))$，这是一个非常明显的算法效率的提升，特别是当$n$数量逐渐增大的时候。每一次的判断过程如下列伪代码所示。如果射线与父节点的包围盒有交点，则进一步判断子节点与射线的相交情况，否则直接退出。 1234if (ray hits bounding object) return whether ray hits bounded objectselse return false &emsp;&emsp;BVH树全称是层次包围盒，故名思意，它是一个树形的层次结构，父节点的包围盒包围全部子节点所在的空间，正如下图11所示。蓝色和红色的包围盒被包含在紫色的大包围盒中，它们可能重叠，并且它们不是有序的，它们只是单纯地被包含在内部。 图1 层次包围盒 &emsp;&emsp;对于图1，检测的伪代码如下： 123456if (hits purple) hit0 = hits blue enclosed objects hit1 = hits red enclosed objectsif (hit0 or hit1) return true and info of closer hitreturn false 1、射线与包围盒相交判断&emsp;&emsp;我们采用的紧凑的包围盒是AABB包围盒，计算出了包围盒之后，我们还需要一个判断射线是否与包围盒相交的办法，不需要求出射线与包围盒的交点，只需判断是否存在交点即可！我们采用一种常见的”slab“方法，它是基于AABB包围盒的。三维的AABB包围盒由三个轴的区间表示，假设分别为$[x_0,x_1]$、$[y_0,y_1]$、$[z_0,z_1]$。 &emsp;&emsp;对于每一个区间，我们首先判断射线在边界的投影交点情况。三维空间中，$x=x_0$和$x=x_1$是一个平面，射线在这两个平面上的交点的$x$值可以通过将$x=x_0$和$x=x_1$带入射线的方程$P(t)=S+tV$的$x$分量得到： x_0=S_x+t_0*V_x \\\\ x_1=S_x+t_1*V_x \\tag {13} 图2 射线与边界的交点 &emsp;&emsp;从而可以求出$t_0$和$t_1$如下所示： t_0=\\frac{x_0-S_x}{V_x} \\\\ t_1=\\frac{x_1-S_x}{V_x} \\tag {14}&emsp;&emsp;关于$y$轴和$z$轴同理，我们求出了每条轴的交点分量，那么如何快速判断射线与包围盒区域是否存在相交的情况呢？为了便于理解，我们以二维的情况为例，则射线与二维的包围区域相交由如下三种情况： 图3 射线与边界相交的三种情况 &emsp;&emsp;我们求得$t$值是关于射线上的电到射线原点的距离，通过仔细观察上面的三张图片，我们可以发现在二维的情况下，当$max(t_0,t_2)&gt;min(t_1,t_3)$时，射线一定和区域存在交点，即射线与每个轴区间的左端点中的最大$t$值大于射线与每个轴区域间的右端点中的最小$t$值。 123456789101112131415161718192021222324252627282930313233343536bool hit(const Ray &amp;ray, float tmin, float tmax) const&#123; float t0, t1, invD; // x invD = 1.0f / ray.getDirection().x; t0 = (m_min.x - ray.getOrigin().x) * invD; t1 = (m_max.x - ray.getOrigin().x) * invD; if (invD &lt; 0.0f) std::swap(t0, t1); tmin = t0 &gt; tmin ? t0 : tmin; tmax = t1 &lt; tmax ? t1 : tmax; if (tmax &lt;= tmin) return false; // y invD = 1.0f / ray.getDirection().y; t0 = (m_min.y - ray.getOrigin().y) * invD; t1 = (m_max.y - ray.getOrigin().y) * invD; if (invD &lt; 0.0f) std::swap(t0, t1); tmin = t0 &gt; tmin ? t0 : tmin; tmax = t1 &lt; tmax ? t1 : tmax; if (tmax &lt;= tmin) return false; // z invD = 1.0f / ray.getDirection().z; t0 = (m_min.z - ray.getOrigin().z) * invD; t1 = (m_max.z - ray.getOrigin().z) * invD; if (invD &lt; 0.0f) std::swap(t0, t1); tmin = t0 &gt; tmin ? t0 : tmin; tmax = t1 &lt; tmax ? t1 : tmax; if (tmax &lt;= tmin) return false; return true;&#125; 2、BVH树的构建&emsp;&emsp;首先我们要考虑如何构建一颗BVH树，BVH数据结构本质就是一颗二叉树。每个树节点右两个子节点，当然子节点之间不存在空间上的顺序关系。树的内部节点都不存储实际的场景物体，仅存储一个包围盒，叶子节点才存储真正的场景物体。构建BVH树的工作考虑的是如何构造一棵可以有效描述当前场景信息的二叉树。这当中的关键是如何对毫无规律地散落在场景中的众物体进行划分，即决定哪些物体该划分到左子树上，哪些物体该划分到右子树上。我们可以把这个问题抽象成一个”划分策略“——我们总会按照某种”策略“划分场景的，待会再考虑具体有哪些策略。另外，由于我们是在3D空间中工作，为了将问题简化，用分而治之的角度看，我们可以首先建立一个”原则“：即决定在哪根轴（x,y,z）上进行划分。”原则“与”策略“的不同之处在于，不管用何种”策略“，总是遵守同一种”原则“。 &emsp;&emsp;决定在哪根轴（x,y,z）上进行划分，取决于场景中的物体在各个轴上分布的“散度”。如果这些物体沿着某根轴分布得最为“松散”（即落在该轴上靠一侧最近的物体与另一侧最近的物体，二者距离为最大），那么就沿该轴进行划分。还有一种方式，即采用随机的方式选取划分的轴，这样当场景物体分散的很随机时，实现的效果还不错。这里我采用随机选取一个轴的方法进行划分。 &emsp;&emsp;确定了以哪根轴进行划分，接下来就要考虑“怎么划分”。我们目前暂时实现按终点划分的策略，顾名思义，取中点划分的意思就是在先前选取的轴上取其中点作为划分点，中点以左划分到左子树，中点以右划分到右子树。这种划分的实现方式最为简单，但往往效果不是太好：因为物体的分布往往不是均匀的。其中一种糟糕的情况（a）是，某侧子树上可能会拥挤过多的物体，而另一侧子树上却太少，这对查找效率影响很大。另外还有一种糟糕的情况（b），就是包围盒之间互相“重叠”（overlapped）的情况。如果两棵子树对应的包围盒“重叠”的越多，那么一条射线穿过该区域时同时击中两子树的概率也就越大，这就意味着两棵子树都得进行相交测试。当然我们目前实现的BVH树没有考虑那么多。 1234567891011121314151617181920212223242526272829BVHNode::BVHNode(std::vector&lt;Hitable *&gt; &amp;list, int start, int end)&#123; // sort it randomly depend on int axis = static_cast&lt;int&gt;(3 * drand48()); if (axis == 0) sort(&amp;list[start], &amp;list[end], boxCompareX); else if (axis == 1) sort(&amp;list[start], &amp;list[end], boxCompareY); else if (axis == 2) sort(&amp;list[start], &amp;list[end], boxCompareZ); int length = end - start; if (length == 1) m_left = m_right = list[start]; else if (length == 2) &#123; m_left = list[start]; m_right = list[start + 1]; &#125; else &#123; m_left = new BVHNode(list, start, start + length / 2); m_right = new BVHNode(list, start + length / 2, end); &#125; // bounding box. AABB boxLeft, boxRight; if (!m_left-&gt;boundingBox(0, 0, boxLeft) || !m_right-&gt;boundingBox(0, 0, boxRight)) std::cerr &lt;&lt; \"no bounding box in BVHNode constructor\\n\"; m_box = AABB::surroundingBox(boxLeft, boxRight);&#125; 3、BVH树的遍历&emsp;&emsp;遍历BVH差不多是件直截了当的事情。在遍历的过程中，当发现射线与某个子节点相交的话，那么有无必要再检测下与另一子节点是否相交？答案是要的。因为两个节点无法保证完全“不重叠”，如下图所示，很有可能在检测另一子节点时发现了更近的交点。 123456789101112131415161718192021222324252627282930313233bool BVHNode::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const&#123; if (m_box.hit(ray, t_min, t_max)) &#123; HitRecord leftRec, rightRec; bool hitLeft = m_left-&gt;hit(ray, t_min, t_max, leftRec); bool hitRight = m_right-&gt;hit(ray, t_min, t_max, rightRec); // both hit. if (hitLeft &amp;&amp; hitRight) &#123; if (leftRec.m_t &lt; rightRec.m_t) ret = leftRec; else ret = rightRec; return true; &#125; // only left child. else if (hitLeft) &#123; ret = leftRec; return true; &#125; else if (hitRight) &#123; ret = rightRec; return true; &#125; else return false; &#125; else return false;&#125; 六、tbb多线程渲染&emsp;&emsp;到目前为止我们实现的光追渲染逻辑都是串行的，只能利用单核cpu运行我们的渲染程序。对于简单的场景来说，渲染的速度还是挺快的。但是当我们渲染复杂的模型时，单核光追的渲染速度慢到爆炸，渲染时间随着模型的面片数迅速增长，渲染时间动不动就数十小时！为此，我们迫切需要加速渲染程序。我们可以看到，每个像素着色之间是没有联系的，一个像素的着色值与其周围的像素计算无关，所以像素的着色计算是可以并行计算的。我们首先实现在cpu上利用多核加速我们的渲染程序。直接操纵原生的线程API不是非常好，因为这样的话我们必须知道当前电脑的核心数，并据此将循环做一个分割，以便充分利用每个cpu核心。Intel开发的TBB是非常有用的线程库，它屏蔽了底层的线程细节，自动根据我们给定的工作量做线程分割，充分利用电脑的全部cpu资源，而且使用起来也非常简单。这里利用多核线程的渲染速度加速比大致是当前电脑的核心数，也就是说，电脑的cpu核心越多，渲染速度越快。tbb的官方网站请看这里。 &emsp;&emsp;tbb的全称是Thread Building Blocks，这里我们只用了tbb的parallel_for接口，它对一个给定的for循环做划分，然后每个划分并行计算。我采用的parallel_for接口函数如下所示： 12template&lt;typename Range, typename Body&gt;void parallel_for( const Range&amp; range, const Body&amp; body, const auto_partitioner&amp; partitioner ) &emsp;&emsp;可以看到出现了三个参数：range、body和partitioner。range就是我们要做并行的for循环下标范围，通常采用一维的迭代器blocked_range指定。这里我把二重循环展开成一重循环。然后body就是函数执行体，这里我通过c++11的lambda表达式指定。最后的partitioner是线程的划分方法，通常直接采用auto_partitioner。并行版的光追渲染如下所示： 12345678910111213141516171819202122232425262728void Tracer::parallelThreadRender(Hitable *scene)&#123; parallel_for(blocked_range&lt;size_t&gt;(0, m_config.m_height * m_config.m_width, 10000), [&amp;](blocked_range&lt;size_t&gt;&amp; r) &#123; for (size_t i = r.begin(); i != r.end(); ++i) &#123; Vector4D color; size_t col = i % m_config.m_width; size_t row = i / m_config.m_width; for (int x = 0; x &lt; m_config.m_samplings; ++x) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = m_config.m_camera-&gt;getRay(u, v); color += deNan(tracing(ray, scene, &amp;m_samplingList,0)); &#125; color /= static_cast&lt;float&gt;(m_config.m_samplings); color.w = 1.0f; // gamma correction. color = Vector4D(sqrt(color.x), sqrt(color.y), sqrt(color.z), color.w); if(color.x &gt; 1.0f) color.x = 1.0f; if(color.y &gt; 1.0f) color.y = 1.0f; if(color.z &gt; 1.0f) color.z = 1.0f; drawPixel(col, row, color); &#125; &#125;, auto_partitioner());&#125; 七、蒙特卡罗积分&emsp;&emsp;蒙特卡罗积分方法（Monte Carlo method）是数值分析中的一个重要分支，它的核心概念是使用随机性来解决确定性的问题。大数定律告诉我们，对于满足某个概率分布的随机变量，其数学期望所描述的积分可以使用这个随机变量随机抽样的样本均值来近似，因此在一定的误差范围内，我们能够使用大量的随机数来近似积分运算的结果。在计算机图形学中， 蒙特卡罗方法主要被应用于物理模拟以及光照传输中的积分运算，在离线渲染领域， 渲染方程几乎只能使用蒙特卡洛方法来进行计算。为了深入理解蒙特卡罗方法，我们首先要复习概率论相关的一些基础内容。以下的内容主要参考秦春林的那本书《全局光照技术：从离线到实时渲染》。 1、概率密度函数、概率分布函数&emsp;&emsp;概率密度函数（probability density function, 简称PDF）用于描述连续型随机变量所服从的概率分布，对于连续随机变量$X$，其概率密度函数$p(x)$是通过落于$x$附近的区间$[x,x+dx]$内的随机数的概率$p(x)dx$来定义的，然而这种定义方式并不直观，所以连续随机变量的概率分布一般通过更直观的称为概率分布函数或者累积分布函数（cumulative distribution function, 简称CDF）来定义，连续随机变量$X$的累积分布函数用大写字母$P$表示，其定义如下： P(y)=Pr\\{x\\leq y\\}=\\int_{-\\infty}^yp(x)dx \\tag {15}&emsp;&emsp;可以看到，概率分布函数$P(y)$定义的是所有随机数的值中小于或等于$y$的随机变量的概率的积分，即理解成对于一个随机数$x$，其小于等于$y$的概率。因此，概率分布函数是一个递增函数。连续随机变量的概率密度函数$p(x)$具有以下的属性： \\forall x:p(x) \\geq 0 \\tag {16} \\int _{-\\infty}^{+\\infty}p(x)dx = 1 \\tag {17} p(x)=\\frac{dP(x)}{dx} \\tag {18}&emsp;&emsp;其中，式$(8)$说明了$p(x)$和$P(x)$的关系，前者是后者的导数。那么给定一个随机变量的区间范围$[a,b]$，随机变量的值$x$落在这个区间的概率计算如下： Pr\\{a\\leq x\\leq b\\}=Pr(x\\leq b)-Pr(x\\leq a)\\\\ =P(b)-P(a)=\\int_a^b p(z)dz \\tag {19}&emsp;&emsp;注意，这里的$Pr$函数是概率函数，而不是概率分布函数。直观来讲，概率密度函数$p(x)$给定的并不是随机变量取值$x$的概率，概率密度函数与轴围成的面积才是给定的概率。如下所示，图(a)是概率分布函数，而图$(b)$则是概率密度函数，给定区间的$[a,b]$的概率就是下图(b)中的面积，这也对应了公式$(19)$中的积分形式（积分的几何意义就是面积）。 &emsp;&emsp;在这里，我们要特别关注的一个分布，那就是均匀分布！对于$[a,b]$区间上的均匀分，其概率密度函数为常数$\\frac{1}{b-a}$，它表示随机抽样结果落于区间$[x,x+dx]$的概率在每个$x$处都相同。均匀分布的随机变量是整个蒙特卡罗方法的基础，在计算机模拟中，通过都是由系统提供的random()函数生成某个区间内的均匀分布，然后通过一些方法将均匀分布的随机变量转换为具有任意概率密度分布的随机变量。 2、数学期望&emsp;&emsp;对于离散随机变量$X$，假设其值$x_i$对应的抽样概率为$p_i$，则该随机变量$X$的数学期望，或称为均值，为： E[X]=\\Sigma_{i=1}^np_ix_i \\tag {20}&emsp;&emsp;数学期望代表的是对一个随机变量$X$进行抽样的平均结果。例如，对于骰子的例子，它的数学期望为： E[X_{die}]=\\Sigma_{i=1}^6p_i x_i\\\\ =\\Sigma_{i=1}^6\\frac16x_i=\\frac16(1+2+3+4+5+6)=3.5 \\tag {21}&emsp;&emsp;相应地，对于连续随机变量$X$，其期望值为随机变量值$x$与其概率密度函数$p(x)$的乘积在全定义域上的积分： E[X]=\\int_{-\\infty}^{+\\infty}xp(x)dx \\tag {22}&emsp;&emsp;连续随机变量$X$的数学期望为什么上面的公式$(22)$形式呢？这其实可以通过离散划分连续随便变量的定义域，然后按照离散数学期望得到一个近似的公式，当划分数趋向于无穷大且划分区间趋向于无穷小时，就是公式$(22)$的积分定义。如下所示： E[X]\\approx\\frac{b-a}{n}\\Sigma_{i=1}^{n}x_ip(x_i) \\\\ n\\to+\\infty,\\ \\frac{b-a}{n}\\Sigma_{i=1}^{n}x_ip(x_i)=\\int_a^bxp(x)dx=E[X]&emsp;&emsp;通常我们对随机变量的函数更感兴趣。考虑以随机变量$X$为自变量的函数$Y=g(X)$，我们只知道随机变量$X$的概率分布，怎样求出随机变量$Y$的数学期望值呢？我们可以通过无意识的统计规律（law of the unconsicious statistician）来求随机变量函数的数学期望：设$Y$是随机变量$X$的函数$Y=g(X)$，且函数$g$是连续函数。若$X$是离散型随机变量，它的概率函数为$P\\{X=x_i\\}=p_i,i=1,2,…$，则有： E[Y]=E[g(X)]=\\Sigma_{i=1}^{\\infty}g(x_i)p_i \\tag {23}&emsp;&emsp;若$X$是连续型随机变量，它的概率密度函数为$p(x)$，则有： E[Y]=E[g(X)]=\\int_{-\\infty}^{+\\infty}g(x)p(x)dx \\tag {24}&emsp;&emsp;该方法的重要意义在于：当求$E[Y]$时，我们不必求出$Y$的分布律或概率密度函数，只需利用$X$的分布律或概率密度即可。 3、大数定律&emsp;&emsp;在统计学中，很多问题涉及对大量独立的随机变量抽样$x_i$的和进行处理，这些随机变量拥有相同的概率密度函数$p(x)$，这样的随机变量称为独立同分布的随机变量。当这些随机变量抽样的和被除以这些随机变量抽样的数量$N$时，我们就得到该随机变量的期望值的一个估计： E[X]\\approx\\overline X=\\frac1N\\Sigma_{i=1}^Nx_i \\tag {25}&emsp;&emsp;随着抽象数量$N$的增大，该估计的方差逐渐减小。当$N$的值足够大时，该估计的值就能够充分接近实际数学期望的值，这样我们就能够将统计方法用于解决确定性问题。大数定律（law of large numbers）告诉我们，当$N\\to\\infty$时，我们可以确定随机变量的统计平均值趋近于数学期望的值，即： P\\{E[X]=lim_{N\\to \\infty}\\frac1N\\Sigma_{i=1}^Nx_i\\} = 1 \\tag {26}&emsp;&emsp;因此，随机变量的数学期望可以通过对随机变量执行大量的重复抽样来近似计算得到。 4、蒙特卡罗积分&emsp;&emsp;假设我们要计算一个一维函数的积分，如$\\int_a^bf(x)dx$，数值分析方法通常采用一些近似方法来计算积分。一种最简单的求积分的方法就是采用梯形法，它通过将被积函数沿作用域上划分成多个区域，然后计算这些区域面积的和。这种方法不适用于多维积分的计算，计算机图形学领域用的最多的还是蒙特卡罗方法。大数定律用于对数学期望的积分公式进行估计，即对积分$\\int_{-\\infty}^{+\\infty}xf(x)dx$进行估计。但是通常情况下我们要求的积分公式是对任意的一个函数积分，假设函数$g(x)$的定义域为$x\\in S$（可以是一个多维空间），我们希望计算如下的积分： I=\\int_{x\\in S}g(x)dx \\tag {27}&emsp;&emsp;现在先不管公式$(27)$。由前面我们知道，给定任意一个关于随机变量的实数函数$f$以及服从$p(x)$概率密度函数的随机变量$x$，我们可以采用如下的公式来近似计算随机变量函数$f(x)$的数学期望： E[f(x)]=\\int_{x\\in S}f(x)p(x)dx\\approx\\frac1N\\Sigma_{i=1}^Nf(x_i) \\tag {28}&emsp;&emsp;现在我们令公式$(27)$的被积函数$g(x)=f(x)p(x)$，则$f(x)=\\frac{g(x)}{p(x)}$，那么公式$(28)$即可转变对公式$(27)$的形式，如下所示： \\int_{x\\in S}f(x)p(x)dx=\\int_{x\\in S}g(x)dx\\approx\\frac1N\\Sigma_{i=1}^N\\frac{g(x_i)}{p(x_i)} \\tag {29}&emsp;&emsp;可以看到通过这个变换，我们巧妙地转换成我们要求的积分公式，这就是蒙特卡洛方法求积分的核心思想。公式$(29)$的期望值为： E[\\frac1N\\Sigma_{i=1}^N\\frac{g(x_i)}{p(x_i)}]=\\frac1N\\Sigma_{i=1}^NE[\\frac{g(x_i)}{p(x_i)}]\\\\ =\\frac1NN\\int\\frac{g(x)}{p(x)}p(x)dx=\\int g(x)dx \\tag {30}&emsp;&emsp;而公式$(29)$的估计方差为： \\sigma^2=\\frac1N\\int(\\frac{g(x)}{p(x)}-I)^2p(x)dx \\tag {31}&emsp;&emsp;可以看到，随着$N$的增大，公式$(31)$的方差随之降低（成反比），这就是一般蒙特卡罗方法的特点。实际上蒙特卡罗方法最大的问题就是估计逼近正确结果的速度非常慢。理论上，公式$(29)$的$p(x)$函数的选择可以是任意的，这也是蒙特卡罗方法的优点，因为通常很难生成与被积函数具有一致分布的随机数。从公式$(31)$也可以看出，通过使$g(x_i)$和$p(x_i)$的比值尽可能地小也可以减少估计误差，在实践上通常我们尽可能地使$p(x)$的分布接近于$g(x)$。综上，蒙特卡洛积分方法计算任意函数的积分步骤如下： 首先对一个满足某种概率分布的随机数进行抽样； 使用该抽样值计算$\\frac{g(x_i)}{p(x_i)}$的值，这称为该样本的贡献值； 最后对所有抽样点计算的结果求平均值。 &emsp;&emsp;上面的步骤中，最困难的就是怎么样对一个具有任意分布函数的随机变量进行抽样。 5、随机抽样&emsp;&emsp;首先定义什么是抽样。给定一个定于域空间$\\Omega_0$及其概率密度函数$p(x)$，其中$x\\in \\Omega_0$，则应有： \\int_{\\Omega_0}p(x)dx=1 \\tag {32}&emsp;&emsp;抽样是这样的一个算法，它能够从$p(x)$对应的随机变量$X$中产生一系列随机数$X1,X2,…$，使得对任意的$\\Omega \\in \\Omega_0$满足如下： P\\{X_k\\in\\Omega\\}=\\int_{\\Omega}p(x)dx\\leq 1 \\tag {33}&emsp;&emsp;在实现中我们并不能直接从$p(x)$产生随机数，在计算机程序中这个过程必须要求首先具有某些基础随机数的一个序列。我们通常采用均匀随机数random来产生一个均匀分布的随机数，然后用来作为抽象所需的基础随机数。目前抽象方法根据不同情况有不同的方法，这里目前只介绍逆变换算法。 &emsp;&emsp;逆变换算法的定义为：设$X$是连续随机变量，其概率分布函数为$P_X$，若随机变量$Y$是一个$[0,1]$上的均匀分布，则随机变量$P_X^{-1}(Y)$具有和$X$一样的概率分布。即我们通过概率分布函数的反函数来获取服从$p(x)$概率密度函数的随机变量，注意是概率分布函数$P(x)$的反函数，而不是概率密度函数$p(x)$的反函数。有时我们不知道概率分布函数，这时我们可以通过概率密度函数来求它的概率分布函数。 &emsp;&emsp;逆变换算法从一个概率密度函数$p(x)$产生随机数$X_i$的步骤如下： 首先计算$p(x)$的概率分布函数：$P(x)=\\int_0^xp(t)dt$； 其次计算累计分布函数的反函数：$P^{-1}(x)$； 然后从一个$[0,1]$上的均匀分布产生一个随机数$\\phi$； 最后将随机数$\\phi$代入$P^{-1}(x)$求出服从$p(x)$分布的随机数：$X_i=P^{-1}(\\phi)$。 八、重要性采样&emsp;&emsp;重要性采样（importance sampling）是蒙特卡罗方法中最重要的方差缩减方法，它通过选择对一个与目标概率分布具有相似形状的分布函数进行抽样来减少方差。重要性采样试图在被积函数中贡献较多的区域放置更多的采样点，以体现这部分区域的重要性。给定一个概率密度函数$p(x)$以及根据该概率密度函数抽样得到的$N$个随机数$x_i$，根据蒙特卡洛方法，被积函数$f(x)$的积分$I$（即前面的公式（27），被积函数换成$f(x)$）可以通过以下公式来近似估计： I_{N}=\\frac1N\\Sigma_{i=1}^N\\frac{f(x_i)}{p(x_i)} \\tag {34}&emsp;&emsp;一个理想的估计的方差应该为$0$，即： \\sigma^2=\\frac1N\\int(\\frac{f(x)}{p(x)}-I)^2p(x)dx=0 \\tag {35}&emsp;&emsp;注意到公式$(35)$中，被积函数部分的$p(x)&gt;0$，故应有$(\\frac{f(x)}{p(x)}-I)^2=0$，从而有如下的推导： p(x)=\\frac{|f(x)|}{I} \\tag {36}&emsp;&emsp;若我们采用公式$(36)$得到的概率密度函数进行采样，那么方差就会被完全消除。但是公式$(36)$要求我们首先计算$I$的值，而这正是我们尝试去求解的，因而行不通。但是我们可以通过选取与被积函数$f(x)$具有相似形状的概率密度函数来减少方差。选择用于抽样的概率密度函数非常重要，尽管蒙特卡罗方法本身没有限制对概率密度函数的选择，但是选择不好的概率密度函数会大大增加蒙特卡罗估计的方差。 &emsp;&emsp;直观来讲，重要性采样就是根据被积函数$f(x)$的值来调整$p(x)$的概率分布。$f(x)$值大的地方，就多采样几个点；$f(x)$值小的地方，就少采样一些点。$p(x)$概率密度函数越是接近$f(x)$，蒙特卡罗方法估算的结果就越精确。 1、复合重要性采样&emsp;&emsp;在实际的情景中，计算机图形学中的被积函数通常非常复杂，它们可能是不连续的，通常在少数区间拥有奇点或者一些较大的值，所以很难找到一个简单的与被积函数相似的分布来做重要性采样。例如，我们考虑渲染中最普通的直接光源的计算公式，如下所示： L_o(p,v)=\\int_{\\Omega}f_r(l,v)\\times L_i(p,l)cos\\theta_id\\omega_i \\tag {37}&emsp;&emsp;我们可以选取$L_i$或者$f_r$来做重要性采样，但是这种方式表现效果并不佳。考虑一个接近镜面的BRDF表面被一个球形面积光照亮的例子。如下所示： &emsp;&emsp;若将面积光源的分布$L_i$作为重要性采样概率密度函数，因为物体表面几乎是镜面的，所以除了沿镜面反射光方向$\\omega_i$，大部分光源上的采样对在最终的光照贡献都为0，因此估计的方差会非常大；而若采用BRDF分布作为重要性采样分布，那么对于小面积光源，依然会导致很大的方差。 &emsp;&emsp;因此，我们通常使用更复杂的采样方式，从而降低估算的方差。通常是根据被积函数的分布特征对其进行区域划分，然后在不同特征的区域上使用不同的分布函数进行采样，最后将这些结果以某种方式进行混合。复合重要性采样就是这一类的采样方法，它提供了一个策略使得可以从多个不同的分布中采样，然后对这些不同的采样结果进行加权组合。复合重要性采样可以简单地分成以下几步： 首先，选取一系列的重要性分布$p1,…,p_n$，使得对于被积函数$f$的每一个函数值比较大的区域$\\Omega_i$，在这个区域$\\Omega_i$，分布函数$p_i$近似为被积函数$f$。通常一个复杂的被积函数是多个不相关的简单分布的乘积形式，所以这些重要性分布来源于这些简单分布。 然后，从每个分布$p_i$产生$n_i$个随机数$X_{i,1},…,X_{i,n_i}$； 最后，将所有的分布估算结果通过加权组合起来。 &emsp;&emsp;复合重要性采样加权组合公式如下所示： I_N=\\Sigma_{i=1}^n\\frac1n_i\\Sigma_{j=1}^{n_i}\\omega_i(X_{i,j})\\frac{f(X_{i,j})}{p_i(X_{i,j})} \\tag {38}&emsp;&emsp;公式$(38)$中的$\\Sigma_{i=1}^n$表明结果是由$n$个采样技术的叠加，$\\frac1n_i\\Sigma_{j=1}^{n_i}\\omega_i(X_{i,j})\\frac{f(X_{i,j})}{p_i(X_{i,j})}$即表示一种特定的采样分布$p_i$的蒙特卡罗估算结果。可以看到，这里还乘上了一个权重系数$\\omega_i(X_{i,j})$，$w_i(x)$可以在每个$x$处的值不一样，只要保证对于给定的$x$值，满足$\\Sigma_{i=1}^n\\omega_i(x)=1$即可。 2、平衡启发式&emsp;&emsp;现在我们需要确定公式$(38)$中的权重系数计算方法。假设我们采用两个采样分布$p_1(x)$和$p_2(x)$，两个采样分布单独的采样估算结果分别为$\\frac1{n_1}\\Sigma\\frac{f(x)}{p_1(x)}$和$\\frac{1}{n_2}\\Sigma\\frac{f(x)}{p_2(x)}$，它们各自的方差都很大，所以我们给它们各自乘上一个系数进行加权组合。为了尽可能地发挥每个采样分布的优势，我们往往尽可能地保证在每个区域贡献较大的采样分布拥有更大的权值系数。考虑如下的权重系数函数： \\omega_i(x)=\\frac{c_ip_i(x)}{\\Sigma_j^nc_jp_j(x)} \\tag {39}&emsp;&emsp;其中$c_i$是每个采样分布$p_i$对应的采样数量占比，即$c_i=\\frac{n_i}{N}$，故$\\Sigma_ic_i=1$，$c_i$在采样之前我们就可以确定得到。公式$(39)$被称为平衡启发式，将$c_i=\\frac{n_i}{N}$和公式$(39)$代入到公式$(38)$，可以推导出如下的标准的蒙特卡洛估算方法（做一些消去）： I_N= \\Sigma_{i=1}^n\\frac1n_i\\Sigma_{j=1}^{n_i}\\omega_i(X_{i,j})\\frac{f(X_{i,j})}{p_i(X_{i,j})}\\\\ =\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac1n_i\\frac{c_ip_i(X_{i,j})}{\\Sigma_j^nc_jp_j(X_{i,j})}\\frac{f(X_{i,j})}{p_i(X_{i,j})}\\\\ =\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac1n_i\\frac{\\frac{n_i}{N}p_i(X_{i,j})}{\\Sigma_j^nc_jp_j(X_{i,j})}\\frac{f(X_{i,j})}{p_i(X_{i,j})}\\\\ =\\frac{1}{N}\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac{f(X_{i,j})}{\\Sigma_j^nc_jp_j(X_{i,j})} \\\\ =\\frac{1}{N}\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac{f(X_{i,j})}{\\overline p(X_{i,j})} \\tag {40}&emsp;&emsp;其中，$\\overline p(x)$又被称为联合抽样分布，其数学公式如下所示。总的采样数$N$，每个分布$p_i$采集$n_i$个随机数$X_{i,j}$。以上就是平衡启发式的核心思想，一种很自然地组合多种采样分布的方式。我们采用一个单一的与$i$无关的分布$\\overline p(x)$来表述这种组合方式。 \\overline p(x)=\\Sigma_{i=1}^nc_ip_i(x) \\tag {41}九、MC光线追踪&emsp;&emsp;了解了相关的原理，接下来我们就实现一个MC（Monte Carlo，蒙特卡罗）光线追踪，主要的参考资料是Peter Shirley的《Ray Tracing_ the Rest of Your Life.pdf》。采样方法是复合重要性采样，复合的采样分布为Lambertian材质BRDF采样分布加上光源采样分布。 1、立体角&emsp;&emsp;在球面坐标中，一个方向向量我们通常采用$(\\theta, \\phi)$来唯一地表示，分别是天顶角和方位角。在衡量发光强度和辐射辐射度量学中，立体角有着广泛的应用。立体角描述了站在某一点的观察者观测到的物体大小的尺度，它被定义为球表面截取的面积微分与球半径平方之比，单位为球面度，写作$sr$。显然，立体角是二维圆心角的三维扩展： d\\omega=\\frac{dA}{r^2} \\tag {42}&emsp;&emsp;更一般的情况，立体角通常转换为$(\\theta, \\phi)$来表示，在单位球体上，$d\\omega=dA$，我们转换成用$(\\theta,\\phi)$求微分面积$dA$。我们知道二维的弧长公式为：圆心角弧度数*半径（注意圆心角要换成弧度制）。如下所示，$\\theta$和$\\phi$对应的弧长为： s_{\\theta}=\\theta * r_{\\theta} \\\\ s_{\\phi}=\\phi * r_{\\phi} \\tag {43} 图4 求弧长 &emsp;&emsp;公式$(43)$中的$r_\\theta$其实就是球体半径，$r_\\phi$与$r_\\theta$的关系为：$r_\\phi=r_\\theta * sin\\theta$。微分面积$dA$可以看成是一个矩形，宽和高分别为对应的弧长$d_{r_\\phi}$和$d_{r_\\theta}$，根据公式$(43)$我们可知$d_{r_\\phi}$、$d_{r_\\theta}$计算方法如下： d_{s_\\theta}=r_{\\theta}d\\theta\\\\ d_{s_\\phi}=r_{\\phi}d\\phi \\tag {44}&emsp;&emsp;对于单位球体，$r_\\theta=r=1,r_\\phi=r_\\theta*sin\\theta=sin\\theta$，从而立体角微分可转换成如下表示： d\\omega=dA=d_{s_\\theta}*d_{s_\\phi}=sin\\theta d\\theta d\\phi \\tag {45}2、Lambertian材质BRDF采样&emsp;&emsp;对于Lambertian材质我们假定其光线的散射分布与$cos\\theta$成正比，这里的$\\theta$是光线与表面法向量的夹角，也就是说在靠近法线的方向光线散射得比较多。当光线与表面法线夹角大于90度时，不发生光线散射。我们记得光线得散射概率密度函数pdf为$C*cos\\theta$，其中$C$为某个常数。对于概率密度函数，我们必须保证其在全定义域上的（这里就是整个半球方向）积分为1，即有（涉及到了立体角转球面坐标表示形式和求定积分）： \\int_{\\Omega}C*cos\\theta d\\omega=C\\int_0^{2\\pi}d\\phi\\int_0^{\\frac\\pi2}cos\\theta sin\\theta d\\theta\\\\ =C*2\\pi\\int_0^{\\frac\\pi2}sin\\theta d(\\sin\\theta)=C*2\\pi * \\frac12=1\\\\ \\to C=\\frac{1}{\\pi} \\tag {46}&emsp;&emsp;从而，Lambertian材质的光线散射概率密度函数PDF，记为$pS(direction)$，如下所示： pS(direction)=\\frac{cos\\theta}{\\pi} \\tag {47}&emsp;&emsp;现在我们要根据这个概率密度函数生成服从该分布的随机半球向量，根据前面随机抽样部分，我们首先要求出它的概率分布函数。根据定义，概率分布函数就是对概率密度函数积分： P=\\int_{\\Omega}\\frac{cos\\theta}{\\pi}d\\omega =\\int_0^{2\\pi}d\\phi\\int_0^\\theta \\frac{cos\\ t}{\\pi}sin\\ t\\ dt\\\\ =2\\pi * \\frac1\\pi \\int_0^{\\theta}sin\\ td(sin\\ t)=sin^2\\theta=1-cos^2\\theta \\tag {48}&emsp;&emsp;根据逆变换算法，我们要取概率分布函数的反函数。这里有个小技巧，我们不需要调用反三角函数得到反函数，我们只需得到$cos\\theta$即可。因为即便调用反三角函数得到$\\theta$，后面我们将$(\\theta,\\phi)$转换成笛卡尔坐标向量的时候还是要调用三角函数$cos$，我们直接避免这个比较费时的过程。所以，任取一个$[0,1]$上均匀随机数$r_2$： cos\\theta =\\sqrt{1-r_2} \\tag {49}&emsp;&emsp;公式$(49)$只得到随机方向向量的$\\theta$，我们还需要$\\phi$。对于Lamberatian材质，光线在方向角上是均匀分布的，故其概率密度函数为$\\frac{1}{2\\pi}$，概率分布函数为$\\frac{\\phi}{2\\pi}$。故对$\\phi$的随机采样如下，任取一个$[0,1]$上的均匀随机数$r_1$： \\frac{\\phi}{2\\pi}=r_1 \\to \\phi=r_1*2\\pi \\tag {50}&emsp;&emsp;采样得方向向量的$(\\theta,\\phi)$，我们还要将其转换到笛卡尔坐标系的形式，这个过程不难理解，仔细观察图4，不再赘述。从而，服从公式$(47)$采样方向向量的代码如下所示： 12345678910 static Vector3D randomCosineDir()&#123; float r1 = drand48(); float r2 = drand48(); float z = sqrt(1-r2); float phi = 2 * M_PI * r1; float x = cos(phi) * 2 * sqrt(r2); float y = sin(phi) * 2 * sqrt(r2); return Vector3D(x,y,z);&#125; &emsp;&emsp;值得注意的是，我们的采样是以物体表面的切线和法线构成的坐标轴为参考系的，其中z轴方向是表面的法线向量。因此，通过上面的代码采样的得到方向向量还要转到该局部坐标系下。这个过程可以构建矩阵，也可以直接将方向向量三个分量与轴向量相乘，最后相加得到。我们采用了后者，首先构建一个局部坐标的正交基（Ortho-normal Bases）： 123456789101112131415161718192021222324252627282930313233343536373839class ONB&#123;private: Vector3D m_axis[3];public: ONB() = default; Vector3D u() const &#123; return m_axis[0]; &#125; Vector3D v() const &#123; return m_axis[1]; &#125; Vector3D w() const &#123; return m_axis[2]; &#125; Vector3D operator[](int i) const &#123; return m_axis[i]; &#125; Vector3D local(float a, float b, float c) const &#123; return u() * a + v() * b + w() * c; &#125; Vector3D local(const Vector3D &amp;a) const &#123; return u() * a.x + v() * a.y + w() * a.z; &#125; void buildFromW(const Vector3D &amp;n);&#125;;void ONB::buildFromW(const Vector3D &amp;n)&#123; m_axis[2] = n; m_axis[2].normalize(); Vector3D a; if(fabs(w().x) &gt; 0.9f) a = Vector3D(0,1,0); else a = Vector3D(1,0,0); m_axis[1] = w().crossProduct(a); m_axis[1].normalize(); m_axis[0] = w().crossProduct(v()); m_axis[0].normalize();&#125; &emsp;&emsp;再构建一个PDF虚类，将PDF函数的函数值和采样抽线为$value$接口和$generate$接口。并继承它创建CosinePDF类，可以看到CosinePDF的$value$是按照公式$(47)$计算的： 123456789101112131415161718192021222324252627282930313233343536class PDF&#123;public: virtual float value(const Vector3D &amp;driection) const = 0; virtual Vector3D generate() const = 0;&#125;;class CosinePDF : public PDF&#123;private: ONB uvw;public: CosinePDF(const Vector3D &amp;w) &#123; uvw.buildFromW(w); &#125; virtual float value(const Vector3D &amp;driection) const; virtual Vector3D generate() const;&#125;;float CosinePDF::value(const Vector3D &amp;direction) const&#123; Vector3D dir = direction; dir.normalize(); float cosine = dir.dotProduct(uvw.w()); if(cosine &gt; 0.0f) return cosine / M_PI; else return 0.0f;&#125;Vector3D CosinePDF::generate() const&#123; return uvw.local(Vector3D::randomCosineDir());&#125; 3、直接光源采样&emsp;&emsp;显然在靠近光源的方向上，光照值对物体表面的颜色贡献更大，因此直接对光源采样对减少蒙特卡洛积分的方差有非常重要的作用。直接光源采样需要我们首先求采样分布的概率密度函数，目前我们先讨论一个最简单的光源例子，即矩形光源。假设矩形光源的面积为A，那么这个矩形光源的直接均匀采样的概率密度函数PDF为$\\frac1A$，但是通常我们采样的单位是立体角微分，如下所示， 图5 直接光源采样 &emsp;&emsp;$d\\omega$与$dA$存在着一个对应关系，实际上我们可以通过前面提到的立体角定义（即公式$(42)$）得到$d\\omega$与$dA$的关系如下所示，这个公式不难理解。其中$\\alpha$夹角是采样方向向量与矩形表面的法线向量的夹角，$dAcos\\alpha$实际上是将矩形的微分面积$dA$投影到采样方向$pq$上，这是因为从$pq$方向看去只能看到$dAcos\\alpha$这个大小的面积，然后再比上半径长度的平方$||pq||^2$，这是立体角的定义。 d\\omega=\\frac{dA*cos\\alpha}{||pq||^2} \\tag {51}&emsp;&emsp;现在对$dA$的采样概率为$\\frac{dA}{A}$，在球体方向上对立体角$d\\omega$采样的概率为$p(direction)d\\omega$，其中$p(direction)$是我们假定的对光源直接采样的概率密度函数。理论上来说，$\\frac{dA}{A}$应该等于$p(direction)d\\omega$，即有： p(direction)*\\frac{dA*cos\\alpha}{||pq||^2}=\\frac{dA}{A}\\\\ \\to p(direction)=\\frac{||pq||^2}{Acos\\alpha} \\tag {52}&emsp;&emsp;公式$(52)$推导出了我们要找的直接光源采样的概率密度函数。根据逆变换算法，我们还要求它的概率分布函数从而生成服从公式$(52)$概率密度函数的随机采样方向，但是这里其实没有必要。我们直接在矩形光源上随机采样一个点，然后将这个采样点与物体表面上的点连接起来就是我们的直接光源采样方向，通过这个方法省去了比较复杂的高数推导过程。 &emsp;&emsp;有了以上的理论基础，我们接下来就实现矩形的直接光源采样。我这里的定义的一个矩形平面是由两个三角形组成，默认是在xz平面上的边长为2的正方形。 1234567891011121314151617181920212223242526272829Vector3D Plane::random(const Vector3D &amp;o) const&#123; Vector3D center = m_transformation.translation(); Vector3D leftCorner; float width = m_transformation.scale().x * 2.0f; float height = m_transformation.scale().z * 2.0f; leftCorner.x = center.x - m_transformation.scale().x; leftCorner.z = center.z - m_transformation.scale().z; leftCorner.y = center.y; Vector3D random_point(leftCorner.x + drand48() * width, leftCorner.y, leftCorner.z + drand48() * height); return random_point - o;&#125;float Plane::pdfValue(const Vector3D &amp;o, const Vector3D &amp;v) const&#123; HitRecord rec; if(this-&gt;hit(Ray(o,v), 0.001f, FLT_MAX, rec)) &#123; float area = m_transformation.scale().x * 2.0f * m_transformation.scale().z * 2.0f; float distance_squared = v.getSquaredLength(); float cosine = fabs(v.dotProduct(rec.m_normal) / v.getLength()); float ret = distance_squared / (cosine * area); return ret; &#125; else return 0.0f;&#125; &emsp;&emsp;除了矩形区域光源，我们接下来还添加一个对球形区域光源的重要性采样。我们采取的坐标系依然是球形光源的局部坐标，而且依然是对光源区域做均匀采样。设想我们从物体表面上的一点望向一个球形区域光源，我们能够看到的区域就是我们要做均匀采样的区域，采样方法依然是围绕$(\\theta,\\phi)$展开，其中$\\theta$是采样方向向量与物体表面的点与球心构成的方向向量的夹角。 &emsp;&emsp;显然方位角$\\phi$依然是$[0,2\\pi]$的范围，不然我们不可能看到一个圆形。而$\\theta$则需要做一些限制，它现在有个上界，如下图6所示，P是物体表面上的一点，C为球形光源的球心，R是球形光源的半径。 图6 球形区域光源采样 &emsp;&emsp;由图6可知，$sin(\\theta_{max})=\\frac{R}{||C-P||}$，相应的$\\theta_{max}$的余弦值如下所示： cos(\\theta_{max})=\\sqrt{1-\\frac{R^2}{||C-P||^2}} \\tag {53}&emsp;&emsp;然后我们是对$\\theta$和$\\phi$做均匀采样，$\\phi$的采样与前面Lambertian采样一样，这里不再赘述。对于$\\theta$，因为是均匀采样，那么它的概率密度函数必然也是一个常数，我们设为$C$，那么其概率分布函数计算如下： P=\\int_{\\Omega}Cd\\omega =\\int_0^{2\\pi}d\\phi \\int_0^{\\theta}Csint\\ dt\\\\ =2\\pi C(1-cos\\theta) \\tag {54}&emsp;&emsp;根据逆变换算法，取$[0,1]$上的均匀随机数$r_2$，并结合公式$(54)$的反函数，可得采样的$cos\\theta$如下： cos(\\theta)=1-\\frac{r_2}{2\\pi C} \\tag {55}&emsp;&emsp;现在有个问题就是$C$这个具体是多少？我们已经知道$\\theta$的上界$\\theta_{max}$，当$\\theta=\\theta_{max}$时，应该取概率分布函数值$P(\\theta_{max})$为1，也就是$r_2=1$。故将其代入公式$(55)$我们可以得到$C$的具体表达式： C=\\frac{1}{2\\pi (1-cos\\theta_{max})} \\tag {56}&emsp;&emsp;然后再将公式$(56)$和公式$(53)$代入公式$(55)$，可得： cos(\\theta)=1+r_2(cos(\\theta_{max})-1)\\\\ =1+r_2(\\sqrt{1-\\frac{R^2}{||C-P||^2}}-1) \\tag {57}&emsp;&emsp;公式$(56)$j就是我们所需的概率密度函数，可以看起来不是很直观，这里我稍微解释一下。公式$(56)$其实就是我们从物体表面上的一点观测到的球形光源所占的立体角的倒数（注意，这里的立体角是以物体表面上的一点为球心而不是球形光源的球面上的立体角）！立体角的几何意义是就是单位球体上的面积，然后做一个倒数是因为我们是做均匀随机采样。立体角的求法如下所示： SolidAngle =\\int_0^{2\\pi}d\\phi\\int_0^{\\theta_{max}}sin\\theta d\\theta=2\\pi(1-cos(\\theta_{max})) \\tag {58}&emsp;&emsp;可以看到公式$(58)$求得的结果就是公式$(56)$中的分母。取球形光源上的随机一点采样算法如下，就是公式$(57)$的实现。 12345678910static Vector3D randomToSphere(float radius, float distance_squared)&#123; float r1 = drand48(); float r2 = drand48(); float z = 1 + r2 * (sqrt(1 - radius * radius/distance_squared) - 1); float phi = 2 * M_PI * r1; float x = cos(phi) * sqrt(1 - z * z); float y = sin(phi) * sqrt(1 - z * z); return Vector3D(x, y, z);&#125; &emsp;&emsp;对球形光源的随机采样以及求取概率密度函数的值如下所示： 123456789101112131415161718192021float Sphere::pdfValue(const Vector3D &amp;o, const Vector3D &amp;v) const&#123; HitRecord rec; if(this-&gt;hit(Ray(o,v), 0.001f, FLT_MAX, rec)) &#123; float cos_theta_max = sqrt(1- m_radius * m_radius/(m_center - o).getSquaredLength()); float solid_angle = 2 * M_PI * (1 - cos_theta_max); return 1.0f / solid_angle; &#125; else return 0.0f;&#125;Vector3D Sphere::random(const Vector3D &amp;o) const&#123; Vector3D dir = m_center - o; float distance_squared = dir.getSquaredLength(); ONB uvw; uvw.buildFromW(dir); return uvw.local(Vector3D::randomToSphere(m_radius, distance_squared));&#125; 2、复合重要性采样&emsp;&emsp;上面我们分别讨论了Lambertian采样和直接光源采样，然后我们要把它复合到一起。场景中通常有多个光源，所以直接光源采样应该对多个光源进行采样，我们采取均匀随机的策略，对于一束光线，它采样哪个光源由均匀的随机数决定，这样就能雨露均沾。复合的权重套用前面提到的平衡启发式，Lambertian采样和直接光源采样的权值各0.5，也就是各占一半。 123456789101112131415161718192021class MixturePDF : public PDF&#123;private: PDF* m_pdf[2];public: MixturePDF(PDF *p0, PDF *p1) &#123; m_pdf[0] = p0;m_pdf[1] = p1; &#125; virtual float value(const Vector3D &amp;direction) const &#123; return 0.5f * m_pdf[0]-&gt;value(direction) + 0.5f * m_pdf[1]-&gt;value(direction); &#125; virtual Vector3D generate() const &#123; if(drand48() &lt; 0.5f) return m_pdf[0]-&gt;generate(); else return m_pdf[1]-&gt;generate(); &#125;&#125;; &emsp;&emsp;对于多个光源的直接采样，我们采取均匀随机的策略，那么PDF值也应该是这些直接光源采样概率密度函数的平均值。 1234567891011121314float HitableList::pdfValue(const Vector3D &amp;o, const Vector3D &amp;v) const&#123; float weight = 1.0f / m_list.size(); float sum = 0; for(int x = 0;x &lt; m_list.size();++ x) sum += m_list[x]-&gt;pdfValue(o, v); return sum * weight;&#125;Vector3D HitableList::random(const Vector3D &amp;o) const&#123; int index = static_cast&lt;int&gt;(drand48() * m_list.size()); return m_list[index]-&gt;random(o);&#125; &emsp;&emsp;最后在光线追踪递归函数中加上我们的复合重要性采样。 12345678910111213141516171819202122232425262728293031323334353637383940414243Vector4D Tracer::tracing(const Ray &amp;r, Hitable *world, Hitable *light, int depth)&#123; HitRecord rec; if (world-&gt;hit(r, 0.001f, FLT_MAX, rec)) &#123; ...... if (depth &lt; m_config.m_maxDepth &amp;&amp; material-&gt;scatter(r, rec, srec)) &#123; if(srec.m_isSpecular) &#123; return srec.m_attenuation * tracing(srec.m_scatterRay, world, light, depth + 1); &#125; else &#123; Vector3D dir; float pdf_val; if(!m_samplingList.isEmpty()) &#123; HitablePDF light_pdf(light, rec.m_position); MixturePDF mix_pdf(&amp;light_pdf, srec.m_pdf.get()); dir = mix_pdf.generate(); pdf_val = mix_pdf.value(dir); &#125; else &#123; dir = srec.m_pdf-&gt;generate(); pdf_val = srec.m_pdf-&gt;value(dir); &#125; Ray scattered = Ray(rec.m_position, dir); return emitted + srec.m_attenuation * material-&gt;scattering_pdf(r, rec, scattered) * tracing(scattered, world, light, depth + 1) / pdf_val; &#125; &#125; else return emitted; &#125; else &#123; // background color. ...... &#125;&#125; 程序效果 参考资料$[1$ http://www.thegibook.com/ $[2]$ Peter Shirley. Ray Tracing in One Weekend. Amazon Digital Services LLC, January 26, 2016. $[3]$ https://software.intel.com/en-us/node/506045?_ga=2.114625223.1582767698.1558613799-2057498546.1558613799 $[4]$ https://blog.csdn.net/zoufeiyy/article/details/1887579 $[5]$ https://www.jianshu.com/p/b570b1ba92bb $[6]$ https://blog.csdn.net/libing_zeng/article/details/74989755 $[7]$ https://www.qiujiawei.com/solid-angle/","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"http://yoursite.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"http://yoursite.com/tags/Ray-Tracer/"}]},{"title":"光线追踪器Ray Tracer：入门篇","slug":"RayTracer-Basis","date":"2019-05-08T14:12:14.121Z","updated":"2019-05-27T03:53:48.425Z","comments":true,"path":"2019/05/08/RayTracer-Basis/","link":"","permalink":"http://yoursite.com/2019/05/08/RayTracer-Basis/","excerpt":"光线追踪技术是计算机图形学的一类全局光照算法，目前的影视行业大多都采用光线追踪做离线渲染。本章开始构建一个光线追踪离线渲染器（路径追踪），深入理解光线追踪的技术原理。主要参考资料为Peter Shirley的《Ray Tracing in One Weekend》。数学库沿用之前自己写的3D数学库，这方面的东西不再赘述。相关的完全代码在这里。","text":"光线追踪技术是计算机图形学的一类全局光照算法，目前的影视行业大多都采用光线追踪做离线渲染。本章开始构建一个光线追踪离线渲染器（路径追踪），深入理解光线追踪的技术原理。主要参考资料为Peter Shirley的《Ray Tracing in One Weekend》。数学库沿用之前自己写的3D数学库，这方面的东西不再赘述。相关的完全代码在这里。 光线追踪纵览 实现光线追踪渲染器 程序结果 一、光线追踪纵览&emsp;&emsp;光线追踪 (Ray Tracing) 算法是一种基于真实光路模拟的计算机三维图形渲染算法，相比其它大部分渲染算法，光线追踪算法可以提供更为真实的光影效果。此算法由 Appel 在 1968 年初步提出，1980 年由Whitted 改良为递归算法并提出全局光照模型。直到今天，光线追踪算法仍是图形学的热点，大量的改进在不断涌现。基于对自然界光路的研究, 光线追踪采取逆向计算光路来还原真实颜色。追踪的过程中涵盖了光的反射、折射、吸收等特性 (精确计算)， 并辅以其它重要渲染思想 (进一步模拟)。 其中包含了重要方法，诸如冯氏光照模型 (Phong Shading)、辐射度(Radiosity)、光子映射 (Photon Mapping)、蒙特卡罗方法 (Monte Carlo) 等等。鉴于光线追踪算法对场景仿真程度之高，其被普遍认为是计算机图形学的核心内容， 以及游戏设计、电影特效等相关领域的未来方向。 近年来由于硬件系统的迅速改良， 基于分布式、GPU， 甚至实时渲染的光线追踪显卡也纷纷出现（本人就是入手了一块实时光追显卡rtx2070）。 &emsp;&emsp;光线追踪算法是一种非常自然的技术，相比于光栅化的方法，它更加简单、暴力、真实。与光栅化根据物体计算所在的像素的方式不同，光线路径追踪的方法是一个相反的过程，它在于用眼睛去看世界而不是世界如何到达眼中。如下图所示，从视点出发向屏幕上每一个像素发出一条光线View Ray，追踪此光路并计算其逆向光线的方向，映射到对应的像素上。通过计算光路上颜色衰减和叠加，即可基本确定每一个像素的颜色。 图1 光线追踪示意图 &emsp;&emsp;可以看到光线追踪是一个递归的过程。发射一束光线到场景，求出光线和几何图形间最近的交点，如果该交点的材质是反射性或折射性的，可以在该交点向反射方向或折射方向继续追踪，如此递归下去，直到设定的最大递归深度或者射线追踪到光源处（或者背景色），如此便计算处一个像素的着色值。 &emsp;&emsp;基本的光线追踪tracing()递归算法如下所示： &emsp;&emsp;Algorithm 1: 光线追踪递归算法 &emsp;&emsp;Input: 射线ray &emsp;&emsp;Output: 反向光颜色 &emsp;&emsp;Function tracing(): &emsp;&emsp;if no intersection with any object then&emsp;&emsp;&emsp;&emsp;return background color&emsp;&emsp;else&emsp;&emsp;&emsp;&emsp;obj $\\leftarrow$ find nearest object from the ray;&emsp;&emsp;&emsp;&emsp;reflect ray $\\leftarrow$getReflectRay(obj);&emsp;&emsp;&emsp;&emsp;refract ray $\\leftarrow$ getRefractRay(obj);&emsp;&emsp;&emsp;&emsp;main color $\\leftarrow$ the radiance of obj;&emsp;&emsp;&emsp;&emsp;reflect color $\\leftarrow$ tracing(reflect ray);&emsp;&emsp;&emsp;&emsp;refract color $\\leftarrow$ tracing(refract ray); &emsp;&emsp;&emsp;&emsp;return mix(main color, reflect color, refract color); 二、实现光线追踪渲染器&emsp;&emsp;采用C++语言不借助第三方图形渲染API实现一个简易的光线追踪器，为了将最后的结果显示出来，我采用stb_image将计算得到的像素矩阵保存为png图片。本篇实现的光线追踪只包含求交运行、计算光线反射和折射向量、反走样、景深等较为初级的方面，而实现的材质包含磨砂材质、玻璃材质和金属材质。 1、摄像机&emsp;&emsp;与光栅化的空间变换过程相反，光线追踪大部分操作都是在世界空间中进行，因而需要将屏幕空间的像素坐标变换到世界空间中，并相应地发射出一条射线。在这里我们不再构建矩阵，直接求解出摄像机的三个坐标轴，然后根据视锥体的视域fov和屏幕的宽高比aspect得到每个像素发射出来的射线。 &emsp;&emsp;首先我们创建一个射线类$Ray$，射线通常用一个射线原点$m_origin$和射线方向$m_direction$表示，射线上的每个点则表示为$p(t)=m_origin+t*m_direction$，射线上每一个独立的点都有一个自己唯一的$t$值。因而创建的$Ray$类如下所示，其中$pointAt$函数根据给定的$t$值返回相应的射线上的点： 12345678910111213141516171819202122class Ray&#123;private: Vector3D m_origin; Vector3D m_direction;public: // ctor/dtor. Ray() = default; ~Ray() = default; Ray(const Vector3D &amp;org, const Vector3D &amp;dir) :m_origin(org), m_direction(dir) &#123; m_direction.normalize(); &#125; // Getter. Vector3D getOrigin() const &#123; return m_origin; &#125; Vector3D getDirection() const &#123; return m_direction; &#125; // p(t) = origin + t*dir; Vector3D pointAt(const float &amp;t)const &#123; return m_origin + m_direction * t; &#125;&#125;; &emsp;&emsp;我们实现的基于cpu的光线追踪核心渲染流程是对给定分辨率的像素矩阵，逐行逐列地遍历每个像素坐标，如下所示： 1234567891011unsigned char *RayTracing::render()&#123; for(int row = 0;row &lt; m_height;++ row) &#123; for(int col = 0;col &lt; m_width;++ col) &#123; ...... &#125; &#125; return m_image;&#125; &emsp;&emsp;因而对于每个给定的像素坐标$(x,y)$，我们需要获取这个像素坐标对应的发射出去的射线，首先我们把值域为$[0,m_width]$和$[0,m_height]$的像素坐标映射到$[0,1]$，正如如下所示： 12float u = static_cast&lt;float&gt;(col) / static_cast&lt;float&gt;(m_config.m_width);float v = static_cast&lt;float&gt;(row) / static_cast&lt;float&gt;(m_config.m_height); &emsp;&emsp;接下来我们根据$u$和$v$获取射线方向向量，这涉及到两个方面，一个摄像机的坐标系统，另一个是关于视锥的大小设置。摄像机的坐标轴决定了当前的朝向，视锥的大小设定决定了当前视域的大小。为此，我把摄像机与视锥合并一起，坐标系类型依然是右手坐标系。创建的摄像机类如下所示： 12345678910111213141516171819202122232425262728293031class Camera&#123;public: Vector3D m_pos; Vector3D m_target; Vector3D m_lowerLeftCorner; Vector3D m_horizontal; Vector3D m_vertical; float m_fovy, m_aspect; Vector3D m_axisX, m_axisY, m_axisZ; Camera(const Vector3D &amp;cameraPos, const Vector3D &amp;target,float vfov, float aspect); // Getter. Ray getRay(const float &amp;s, const float &amp;t) const; Vector3D getPosition() const &#123; return m_pos; &#125; Vector3D getTarget() const &#123; return m_target; &#125; Vector3D getAxisX() const &#123; return m_axisX; &#125; Vector3D getAxisY() const &#123; return m_axisY; &#125; Vector3D getAxisZ() const &#123; return m_axisZ; &#125; // Setter. void setPosition(const Vector3D &amp;pos) &#123; m_pos = pos; update(); &#125; void setTarget(const Vector3D &amp;_tar) &#123; m_target = _tar; update(); &#125; void setFovy(const float &amp;fov) &#123; m_fovy = fov; update(); &#125; void setAspect(const float &amp;asp) &#123; m_aspect = asp; update(); &#125;private: void update();&#125;; &emsp;&emsp;其中$m_pos$即摄像机的世界坐标位置，$m_target$即目标位置，而$m_lowerLeftCorner$表示视锥近平面的左下角位置，$m_horizontal$表示近平面在摄像机坐标系下水平方向的跨度，$m_vertical$则是近平面在摄像机坐标系下垂直方向的跨度。$m_fovy$和$m_aspect$分别是视锥的垂直视域和屏幕的宽高比。初始时我们传入摄像机坐标、目标点以及垂直视域和视口宽高比，然后我们根据这些计算摄像机的三个坐标轴，以及近平面的位置： 123456789101112131415161718192021void Camera::update()&#123; const Vector3D worldUp(0.0f, 1.0f, 0.0f); // frustum. float theta = radians(m_fovy); float half_height = static_cast&lt;float&gt;(tan(theta * 0.5f)); float half_width = m_aspect * half_height; // camera coordinate system. m_axisZ = m_pos - m_target; m_axisZ.normalize(); m_axisX = worldUp.crossProduct(m_axisZ); m_axisX.normalize(); m_axisY = m_axisZ.crossProduct(m_axisX); m_axisY.normalize(); // view port. m_lowerLeftCorner = m_pos - m_axisX * half_width - m_axisY * half_height - m_axisZ; m_horizontal = m_axisX * 2.0f * half_width; m_vertical = m_axisY * 2.0f * half_height;&#125; &emsp;&emsp;然后我们对于给定在$[0,1]$的$u$和$v$，就可以计算出一条对应的射线向量了。 1234Ray Camera::getRay(const float &amp;s, const float &amp;t) const&#123; return Ray(m_pos , m_lowerLeftCorner + m_horizontal * s + m_vertical * t - m_pos );&#125; 12345678910for (int row = m_config.m_height - 1; row &gt;= 0; --row)&#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); ...... &#125;&#125; 2、物体求交&emsp;&emsp;射线发射出去之后要与物体进行求交运行，对于这类能够被射线碰撞到的物体我们把它抽象为$Hitable$，并用一个虚函数$Hit$作为所有的碰撞求交的接口，创建$Hitable$虚类如下： 12345678910111213141516class Material;struct HitRecord&#123; float m_t; Vector3D m_position; Vector3D m_normal; Material *m_material;&#125;;class Hitable&#123;public: Hitable() = default; virtual ~Hitable() &#123;&#125; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const = 0;&#125;; &emsp;&emsp;可以看到我们还创建了一个$HitRecord$结构体，它包含一次射线碰撞求交的结果记录，其中$m_t$是射线方程的参数$t$，$m_position$是交点的位置，$m_normal$是交点的法向量，而$m_material$则是交点所在物体的材质，求交之后我们需要根据这些记录来计算物体的折射、反射。 &emsp;&emsp;$Hitable$中的$hit$接口以一条射线$ray$作为输入参数，以一个$Hitable$的引用$ret$作为求交的结果记录，函数返回布尔值以表示是否发生了射线碰撞。此外，值得一提的是我们还输入了两个参数，分别是$t_min$和$t_max$，这个是我们自己对射线线段长度做的一个限制，可以分别去掉太近和太远的物体。 &emsp;&emsp;然后我们需要向场景中添加物体，光线追踪器的一个”Hello, world!”是球体。我们知道，一个球体的数学表达式为如下所示： (x-cx)^2+(y-cy)^2+(z-cz)^2=R^2 \\tag {1}&emsp;&emsp;其中$c=(cx,cy,cz)$是球体的球心，$R$为球体半径。我们现在要求的就是，对于射线上的一点$P(t)=S+tV$，使得$(x,y,z)=P(t)=S+tV$带入公式$(1)$成立，公式$(1)$可以写成点乘的形式如下： (P-c)\\cdot (P-c) = R^2 \\tag {2}&emsp;&emsp;将$P=P(t)=S+tV$带入公式$(2)$可得： (V\\cdot V)t^2+2(V\\cdot(S-c))t+(S-c)\\cdot(S-c)-R^2=0 \\tag {3}&emsp;&emsp;可以看到公式$(3)$中只有$t$未知，它是一个一元二次方程。对于任意的一元二次方程$at^2+bt+c=0$，其解有如下形式： t=\\frac{-b\\pm \\sqrt{b^2-4ac}}{2a} \\tag {4}&emsp;&emsp;其中根号内的$D=b^2-4ac$称为根的判别式，它可以反应多项式根的数量。若$D&gt;0$则有两个实数根，若$D=0$则只有一个实数根，若$D&lt;0$则没有实数根。我们首先可以根据判别式判断是否存在交点，然后再求出具体的交点坐标。下面的$Hit$函数，我们首先求出多项式方程的常数项$a$、$b$和$c$，然后求判别式，最后再有解的情况下求取交点。注意，在有两个交点的情况下，我们首先取较近的点，不符合再取较远的那个点。只有一个交点的情况下（如下图2所示），我们不当作射线发生了碰撞（擦边而过）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Sphere : public Hitable &#123; public: float m_radius; Vector3D m_center; Material *m_material; Sphere(const Vector3D &amp;cen, const float r, Material *mat) :m_center(cen), m_radius(r), m_material(mat) &#123;&#125; ~Sphere() &#123; if (m_material)delete m_material; m_material = nullptr; &#125;; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const; &#125;; bool Sphere::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const &#123; Vector3D oc = ray.getOrigin() - m_center; float a = ray.getDirection().dotProduct(ray.getDirection()); float b = oc.dotProduct(ray.getDirection()); float c = oc.dotProduct(oc) - m_radius * m_radius; // discriminant float discriminant = b * b - a * c; if (discriminant &gt; 0) &#123; float temp = (-b - sqrt(b * b - a * c)) / a; if (temp &gt; t_min &amp;&amp; temp &lt; t_max) &#123; ret.m_t = temp; ret.m_position = ray.pointAt(ret.m_t); ret.m_normal = (ret.m_position - m_center) / m_radius; ret.m_material = m_material; return true; &#125; temp = (-b + sqrt(b * b - a * c)) / a; if (temp &gt; t_min &amp;&amp; temp &lt; t_max) &#123; ret.m_t = temp; ret.m_position = ray.pointAt(ret.m_t); ret.m_normal = (ret.m_position - m_center) / m_radius; ret.m_material = m_material; return true; &#125; &#125; return false; &#125; 图2 射线与球体的相交情况 &emsp;&emsp;当场景中有多个物体时，当前的做法是在每次求交时遍历所有的物体，我们需要一个$HitableList$来存储这些物体。我们令$HitableList$继承自$Hitable$，这样$HitableList$就表现得好像只有一个很大的物体一样，并在实现$hit$函数中对场景得所有物体遍历调用他们的$Hit$方法： 1234567891011121314151617181920212223242526272829303132333435363738class HitableList : public Hitable&#123;public: std::vector&lt;Hitable*&gt; m_list; HitableList() = default; ~HitableList() = default; void addHitable(Hitable *target) &#123; m_list.push_back(target); &#125; void clearHitable() &#123; for (int x = 0; x &lt; m_list.size(); ++x) &#123; delete m_list[x]; m_list[x] = nullptr; &#125; &#125; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const;&#125;;bool HitableList::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const&#123; HitRecord tempRec; bool hitAny = false; double closestSoFar = t_max; for (unsigned int x = 0; x &lt; m_list.size(); ++x) &#123; if (m_list[x]-&gt;hit(ray, t_min, closestSoFar, tempRec)) &#123; hitAny = true; closestSoFar = tempRec.m_t; ret = tempRec; &#125; &#125; return hitAny;&#125; &emsp;&emsp;此外，值得一提的是，在$HitableList$的$hit$函数中我们需要做一个类似于深度测试的步骤，我们从摄像机发射的射线只能跟最靠近摄像机的那个交点做反射、折射，一条射线发射出去可能会与多个物体相交，我们必须取最近的交点。这个距离我们用射线方程中的$t$来描述，显然$t$越大则交点越远，因此用$closestSoFar$来记录当前获取的交点的最小$t$，以此作为$t$的上限，这样最终求出来的必然就是最近的交点。 3、物体材质&emsp;&emsp;现在我们的一个问题就是求出交点之后，光线在交点上做什么样的反射和折射？这取决于物体的材质。若物体的材质是透明的玻璃，那么光线一般做折射；而若物体是光滑的镜面，则光线做完美的反射。针对不同物体的材质，光线的散射情况各不相同，为此我们创建一个虚类$Material$，并把光线散射的这一过程抽象为$sactter$函数接口。 123456789class Material&#123;public: Material() = default; virtual ~Material() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const = 0;&#125;; &emsp;&emsp;可以看到，$scatter$函数接收入射射线$Ray$以及求交获得的$HitRecord$，计算散射光线的向量，返回的结果表示是否发生了散射。其中的$attenuation$本质上是物体自身的反射颜色，之所以叫$attenuation$是因为光线照射到物体上，物体一般会吸收光线中的大部分颜色，然后仅反射自身颜色的部分，这个过程使得光线在反射过程中不断衰减。 3.1 Lambertian反射材质&emsp;&emsp;首先我们要实现的是Lambertian反射的材质，Lambertian反射也叫理想散射。Lambertian表面是指在一个固定的照明分布下从所有的视场方向上观测都具有相同亮度的表面，Lambertian表面不吸收任何入射光。Lambertian反射也叫散光反射，不管照明分布如何，Lambertian表面在所有的表面方向上接收并发散所有的入射照明，结果是每一个方向上都能看到相同数量的能量。这是一种理想情况，现实中不存在完全漫反射，但Lambertian可以用来近似的模拟一些粗糙表面的效果，比如纸张。 图3 Lambertian反射 &emsp;&emsp;为了实现Lambertian表面的均匀反射现象，我们令射线碰撞到表面之后，在交点的半球方向上随机地反射，只要随机性够均匀，我们就能模拟出理想散射的情况。为此，我们取一个正切于交点$P$表面的单位球体，在这个球体内随机取一个点$S$，则反射的向量就为$S-P$。这个正切于交点$P$表面的单位球体不难求得，设交点$P$的单位法向量为$N$，那么该正切球体的球心为$P+N$。我们首先在球心为原点的单位球内随机求得一个方向向量，然后将这个方向向量加上正切球体的球心即可得出反射的方向向量。（$drand48$是生成$[0,1)$之间的均匀随机数函数，一般linux下才有这个内建函数，windows下没有，所以我们就自己写了。） 123456789101112131415161718192021222324252627282930313233343536373839404142434445#define rndm 0x100000000LL#define rndc 0xB16#define rnda 0x5DEECE66DLL static unsigned long long seed = 1; inline double drand48(void) &#123; seed = (rnda * seed + rndc) &amp; 0xFFFFFFFFFFFFLL; unsigned int x = seed &gt;&gt; 16; return ((double)x / (double)rndm); &#125;=============================================================== static Vector3D randomInUnitSphere() &#123; Vector3D pos; do &#123; pos = Vector3D(drand48(), drand48(), drand48()) * 2.0f - Vector3D(1.0, 1.0, 1.0); &#125; while (pos.getSquaredLength() &gt;= 1.0); return pos; &#125;=============================================================== class Lambertian : public Material &#123; private: Vector3D m_albedo; public: Lambertian(const Vector3D &amp;a) : m_albedo(a) &#123;&#125; virtual ~Lambertian() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const; &#125;; bool Lambertian::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const &#123; Vector3D target = rec.m_position + rec.m_normal + Vector3D::randomInUnitSphere(); scattered = Ray(rec.m_position, target - rec.m_position); attenuation = m_albedo; return true; &#125; &emsp;&emsp;其中的$m_albedo$为物体自身的反射颜色。 3.2 金属镜面反射材质&emsp;&emsp;金属的表面比较光滑，因而不会呈现出光线随机散射的情况。对于一个完美镜面的材质来说，入射光线和反射光线遵循反射定律，即光射到镜面上时，反射线跟入射线和法线在同一平面内，反射线和入射线分居法线两侧，并且与界面法线的夹角（分别叫做入射角和反射角）相等。反射角等于入射角。 &emsp;&emsp;求反射向量如下图4所示，比较简单，不再赘述。 图4 反射向量 R = I-2(N\\cdot I)N \\tag {5}1234static Vector3D reflect(const Vector3D &amp;ray, const Vector3D &amp;normal)&#123; return ray - normal * (ray.dotProduct(normal)) * 2.0f;&#125; &emsp;&emsp;对于一个完美镜面的金属材质来说，我们只需求出反射向量，然后按照这个反射向量递归下去就行了。但是有些金属并没有那么光滑，它的高光反射并没有那么锐利，为此我们对求出的反射向量做一定的扰动，使反射向量在一定的波瓣内随机，这个波瓣有多大由用户决定（波瓣越大则金属越粗糙）。废话不多说直接上图就明白了。 &emsp;&emsp;我们在反射向量的终点上取一个给定半径的球体，在这个球体内随机选一个点作为新的反射向量的终点即可。这个半径的大小我们用$m_fuzz$变量存储，交给用户决定。 12345678910111213141516171819202122class Metal : public Material&#123;private: float m_fuzz; Vector3D m_albedo;public: Metal(const Vector3D &amp;a, const float &amp;f) : m_albedo(a), m_fuzz(f) &#123; if (f &gt; 1.0f)m_fuzz = 1.0f; &#125; virtual ~Metal() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const;&#125;;bool Metal::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const&#123; Vector3D reflectedDir = Vector3D::reflect(in.getDirection(), rec.m_normal); scattered = Ray(rec.m_position, reflectedDir + Vector3D::randomInUnitSphere() * m_fuzz); attenuation = m_albedo; return (scattered.getDirection().dotProduct(rec.m_normal) &gt; 0.0f);&#125; 3.3 透明玻璃折射材质&emsp;&emsp;对于水、玻璃和钻石等等物体的材质，光线照射到它们的表面时，它会把光线分成折射（也叫透射）光线和反射光线两部分。我们实现的材质采用随机的策略， 就是在折射和反射两个部分中随机选取一种。首先我们要根据入射向量、法线以及入射介质系数和折射介质系数计算折射方向向量，相比反射向量，推导计算的过程稍微有点复杂。折射表面有折射系数属性，根据Snell定律，如图5所示，入射角$\\theta _L$和折射角$\\theta _T$之间的关系有： \\eta _Lsin\\theta _L=\\eta _rsin\\theta _r \\tag {6} 图5 折射向量的计算 &emsp;&emsp;其中，$\\eta _L$时光线离开的介质的折射系数，$\\eta _r$是光线进入的介质的折射系数。空气的折射系数通常定位$1.00$，折射系数越大，则在两种不同介质之间光线弯曲效果越明显。$N$和$L$都是单位方向向量。折射向量$T$可为与法向量平行的向量$-Ncos\\theta_T$和垂直的向量$-Gsin\\theta _T$，$G$是上图所示的单位向量。而$perp_NL$与$G$向量平行，且$||perp_NL=sin\\theta_L||$，故有： G=\\frac{perp_NL}{sin\\theta_L}=\\frac{L-(N\\cdot L)N}{sin\\theta_L} \\tag {7}&emsp;&emsp;折射向量$T$可以表示为： T=-Ncos\\theta_T-Gsin\\theta_T\\\\ =-Ncos\\theta_T-\\frac{sin\\theta_T}{sin\\theta_L}[L-(N\\cdot L)N] \\tag {8}&emsp;&emsp;利用公式$(6)$，我们可以将上式中的正弦商替换为$\\eta _L/\\eta _T$，可得： T=-Ncos\\theta_T-\\frac{\\eta _L}{\\eta _T}[L-(N\\cdot L)N] \\tag {9}&emsp;&emsp;注意到公式$(9)$中的$cos\\theta_T$未知，用$\\sqrt{1-sin^2\\theta_T}$替换$cos\\theta_T$，再用$(\\eta_L/\\eta_r)sin\\theta_L$代替$sin\\theta_T$，可得： T=-N\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}sin^2\\theta_L}-\\frac{\\eta_L}{\\eta_T}[L-(N\\cdot L)N] \\tag {10}&emsp;&emsp;最后再用$1-cos^2\\theta_L=1-(N\\cdot L)^2$代替$sin^2\\theta_L$，得到最终的表达式为： T=(\\frac{\\eta_L}{\\eta_T}N\\cdot L-\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}[1-(N\\cdot L)^2]}\\ )N-\\frac{\\eta_L}{\\eta_T}L \\tag {11}&emsp;&emsp;如果$\\eta_L&gt;\\eta_T$，则上式平方根里的数值可能为负，这种情况发生在当光线从一个大折射率的介质进入一个小折射率的介质时，此时光线与表面之间的入射角较大。特别的，若仅当$sin\\theta_L\\leq \\eta_r/\\eta_L$时，公式$(11)$有效，如果平方根里的数值为负，则会出现所谓的全内反射现象，也就是光线不被折射，仅在介质内部反射。此外，需要注意的是，我们在程序实现中的入射向量与图5中$L$是相反的，所以需要将公式中的$(11)$的入射向量取反，如下所示： T=\\frac{\\eta_L}{\\eta_T}(L-(N\\cdot L)N)-N\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}[1-(N\\cdot L)^2]}\\ \\tag {12}123456789101112131415static bool refract(const Vector3D &amp;ray, const Vector3D &amp;normal, float niOvernt, Vector3D &amp;refracted)&#123; Vector3D uv = ray; uv.normalize(); float dt = uv.dotProduct(normal); float discriminant = 1.0f - niOvernt * niOvernt * (1.0f - dt * dt); if (discriminant &gt; 0.0f) &#123; refracted = (uv - normal * dt) * niOvernt - normal * sqrt(discriminant); return true; &#125; else return false;&#125; &emsp;&emsp;然后创建一个$Dielectric$类，它有一个私有变量$refIdx$，它表面该物体的材质折射系数。在实现玻璃材质物体的散射函数$scatter$时，我们需要判断当前射线是从外部折射到内部还是从内部折射到外部，这可以通过计算入射向量与法向量的夹角余弦值来判断（通常法向量朝外），然后相应地将法向量的方向扭正。这里用$ni-over-nt$变量来记录$\\frac{\\eta_L}{\\eta_T}$，我们知道空气的折射系数为$1.00$，所以从外面折射入物体内部时其取值等于$1.0/refIdx$，从内部折射到外部时取值为$refIdx$。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 class Dielectric : public Material &#123; private: float refIdx; public: Dielectric(float ri) : refIdx(ri) &#123;&#125; virtual ~Dielectric() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const; &#125;; bool Dielectric::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const &#123; Vector3D outward_normal; Vector3D reflected = Vector3D::reflect(in.getDirection(), rec.m_normal); float ni_over_nt; attenuation = Vector3D(1.0f, 1.0f, 1.0f); Vector3D refracted; float reflect_prob; float cosine; // from inside to outside. if (in.getDirection().dotProduct(rec.m_normal) &gt; 0.0f) &#123; outward_normal = -rec.m_normal; ni_over_nt = refIdx; cosine = refIdx * in.getDirection().dotProduct(rec.m_normal) / in.getDirection().getLength(); &#125; // from outside to inside. else &#123; outward_normal = rec.m_normal; ni_over_nt = 1.0 / refIdx; cosine = -in.getDirection().dotProduct(rec.m_normal) / in.getDirection().getLength(); &#125; if (Vector3D::refract(in.getDirection(), outward_normal, ni_over_nt, refracted)) &#123; reflect_prob = schlick(cosine, refIdx); &#125; else &#123; scattered = Ray(rec.m_position, reflected); reflect_prob = 1.0f; &#125; if (drand48() &lt; reflect_prob) scattered = Ray(rec.m_position, reflected); else scattered = Ray(rec.m_position, refracted); return true; &#125;&#125; &emsp;&emsp;这里还要引入一个菲涅尔反射现象（仅对电介质和非金属表面有定义）。生活中，当我们以垂直的视角观察时，任何物体或者材质表面都有一个基础反射率(Base Reflectivity)，但是如果以一定的角度往平面上看的时候所有反光都会变得明显起来。你可以自己尝试一下，用垂直的视角观察你自己的木制桌面，此时一定只有最基本的反射性。但是如果你从近乎与法线成90度的角度观察的话反光就会变得明显的多。如果从理想的90度的视角观察，所有的平面理论上来说都能完全的反射光线。这种现象因菲涅尔而闻名，并体现在了菲涅尔方程之中。菲涅尔方程是一个相当复杂的方程式，不过幸运的是菲涅尔方程可以用Fresnel-Schlick近似法求得近似解： F_{schlick(h,v,F_0)}=F_0+(1-F_0)(1-(h\\cdot v))^5 \\tag {13}&emsp;&emsp;这里的$F_0$y由物体的折射系数得到，$h$是入射向量的负向量（因为我们定义的入射向量方向朝向交点），$v$则是交点处的法向量$v$，我们实现一个$schlick$函数如下： 123456float schlick(float cosine, float ref_idx) const&#123; float r0 = (1.0f - ref_idx) / (1.0f + ref_idx); r0 = r0 * r0; return r0 + (1.0f - r0) * pow((1.0f - cosine), 5.0f);&#125; &emsp;&emsp;我们还定义了一个reflect_prob变量，它介于0~1之间。我们根据reflect_prob与介于$[0,1)$的随机数做比较确定选择反射还是折射，这个还是很合理的，为什么呢？因为我们做了100次采样！那么我们可以理直气壮的说，我们的透明电介质真正做到了反射和折射的混合（除了全反射现象），而且这样符合光线照射透明电介质时，它会分裂为反射光线和折射光线的物理现象。（在程序中，教程作者在从内部折射到外部的时候将$cosine$值还乘上了个$refIdx$，这个操作没明白作者的意图，不乘上$refIdx$好像也没有发现渲染结果有明显的错误）。 &emsp;&emsp;最后，我们实现的玻璃球球内图像是颠倒的，这属于正常现象，原因如下图所示。光线经过两次折射最终导致了图像的颠倒。 4、抗锯齿&emsp;&emsp;为了减少光线追踪方法的噪声点和锯齿，我们需要做一些抗锯齿处理。方法就是在计算一个像素坐标的像素值时，发射很多条射线，射线的取值范围在一个像素之内，然后将所有光线获取的像素值累加起来，最后除以总的采样数。代码如下： 123456789101112131415161718192021int samples = 100;for (int row = m_config.m_height - 1; row &gt;= 0; --row)&#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; Vector4D color; for (int sps = 0; sps &lt; samples; ++sps) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); color += tracing(ray, world, 0); &#125; color /= static_cast&lt;float&gt;(samples); color.w = 1.0f; // gamma correction. color = Vector4D(sqrt(color.x), sqrt(color.y), sqrt(color.z), color.w); drawPixel(col, row, color); &#125;&#125; &emsp;&emsp;这里还提到了gamma矫正，关于gamma矫正请看这里)。我们对计算得到的像素做了一个简单的gamma矫正，gamma矫正系数取为$2.0$。不进行gamma矫正的话，渲染出来的图片明显偏暗。 5、景深&emsp;&emsp;关于现实生活中摄像机的景深原理，我不再详细说明。在光线追踪中实现景深并不复杂。实现的方法：首先是射线的出发点视点，我们的眼睛（或者相机）不再是一个点而是眼睛所在的周围圆盘上的随机点，因为实际的相机是有摄像镜头的，摄像镜头是一个大光圈（很大一个镜片），并不是针孔类的东东，所以，我们要模拟镜头，就要随机采针孔周围的光圈点。 &emsp;&emsp;此外还有一个焦距的问题，我们一开始假设成像平面在摄像机坐标系的$z=-1$上，为了实现摄像机的景深效果，现在我们要引入现实摄像机的焦距概念。简单的说焦距是焦点到面镜的中心点之间的距离。因此我们提供了一个焦距的参数给用户调整，以确定所需的景深效果。通常情况下焦距$focusDist$等于$length(target-cameraPos)$。这个时候我们将成像平面挪到了摄像机坐标系的$z=-focusDist$上，相应地需要调整计算成像平面的$halfHeight$（在前面的基础上再乘上个$focusDist$）。 12345678910111213141516171819202122232425262728293031323334Camera::Camera(const Vector3D &amp;cameraPos, const Vector3D &amp;target, float vfov, float aspect, float aperture, float focus_dist)&#123; m_pos = cameraPos; m_target = target; m_fovy = vfov; m_aspect = aspect; m_lensRadius = aperture * 0.5f; m_focusDist = focus_dist; update();&#125;void Camera::update()&#123; const Vector3D worldUp(0.0f, 1.0f, 0.0f); // frustum. float theta = radians(m_fovy); float half_height = static_cast&lt;float&gt;(tan(theta * 0.5f)) * m_focusDist; float half_width = m_aspect * half_height; // camera coordinate system. m_axisZ = m_pos - m_target; m_axisZ.normalize(); m_axisX = worldUp.crossProduct(m_axisZ); m_axisX.normalize(); m_axisY = m_axisZ.crossProduct(m_axisX); m_axisY.normalize(); // view port. m_lowerLeftCorner = m_pos - m_axisX * half_width - m_axisY * half_height - m_axisZ * m_focusDist; m_horizontal = m_axisX * 2.0f * half_width; m_vertical = m_axisY * 2.0f * half_height;&#125; 6、递归光线追踪&emsp;&emsp;最后，我们实现的光线追踪器$Tracer$如下，追踪器的核心实现主要在$tracing$函数和$render$函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168class Hitable;class Vector3D;class Vector4D;class Tracer&#123;private: class Setting &#123; public: int m_maxDepth; int m_width, m_height, m_channel; Setting():m_maxDepth(50), m_channel(4) &#123;&#125; &#125;; Setting m_config; unsigned char *m_image;public: Tracer(); ~Tracer(); void initialize(int w, int h, int c = 4); unsigned char *render(); int getWidth() const &#123; return m_config.m_width; &#125; int getHeight() const &#123; return m_config.m_height; &#125; int getChannel() const &#123; return m_config.m_channel; &#125; int getRecursionDepth() const &#123; return m_config.m_maxDepth; &#125; unsigned char *getImage() const &#123; return m_image; &#125; void setRecursionDepth(int depth); void setCamera(const Vector3D &amp;cameraPos, const Vector3D &amp;target, const Vector3D &amp;worldUp, float fovy, float aspect, float aperture, float focus_dist);private: Hitable *randomScene(); Vector4D tracing(const Ray &amp;r, Hitable *world, int depth); float hitSphere(const Vector3D &amp;center, const float &amp;radius, const Ray &amp;ray); void drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color);&#125;;void Tracer::initialize(int w, int h, int c)&#123; m_config.m_width = w; m_config.m_height = h; if (m_image != nullptr) delete m_image; m_image = new unsigned char[m_config.m_width * m_config.m_height * m_config.m_channel];&#125;unsigned char *Tracer::render()&#123; // viewport Vector3D lower_left_corner(-2.0, -1.0, -1.0); Vector3D horizontal(4.0, 0.0, 0.0); Vector3D vertical(0.0, 2.0, 0.0); Vector3D origin(0.0, 0.0, 0.0); // scene Hitable* world = randomScene(); // camera Vector3D lookfrom(3, 4, 10); Vector3D lookat(0, 0, 0); float dist_to_focus = 10.0f; float aperture = 0.0f; Camera camera(lookfrom, lookat, 45, static_cast&lt;float&gt;(m_config.m_width) / m_config.m_height, aperture, dist_to_focus); int samples = 100; for (int row = m_config.m_height - 1; row &gt;= 0; --row) &#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; Vector4D color; for (int sps = 0; sps &lt; samples; ++sps) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); color += tracing(ray, world, 0); &#125; color /= static_cast&lt;float&gt;(samples); color.w = 1.0f; // gamma correction. color = Vector4D(sqrt(color.x), sqrt(color.y), sqrt(color.z), color.w); drawPixel(col, row, color); &#125; &#125; reinterpret_cast&lt;HitableList*&gt;(world)-&gt;clearHitable(); delete world; return m_image;&#125;void Tracer::drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color)&#123; if (x &lt; 0 || x &gt;= m_config.m_width || y &lt; 0 || y &gt;= m_config.m_height) return; unsigned int index = (y * m_config.m_width + x) * m_config.m_channel; m_image[index + 0] = static_cast&lt;unsigned char&gt;(255 * color.x); m_image[index + 1] = static_cast&lt;unsigned char&gt;(255 * color.y); m_image[index + 2] = static_cast&lt;unsigned char&gt;(255 * color.z); m_image[index + 3] = static_cast&lt;unsigned char&gt;(255 * color.w);&#125;Hitable *Tracer::randomScene()&#123; int n = 500; HitableList *list = new HitableList(); list-&gt;addHitable(new Sphere(Vector3D(0, -1000.0, 0), 1000, new Lambertian(Vector3D(0.5, 0.5, 0.5)))); for (int a = -11; a &lt; 11; ++a) &#123; for (int b = -11; b &lt; 11; ++b) &#123; float choose_mat = drand48(); Vector3D center(a + 0.9*drand48(), 0.2, b + 0.9*drand48()); if ((center - Vector3D(4, 0.2, 0)).getLength() &gt; 0.9) &#123; // diffuse. if (choose_mat &lt; 0.4f) list-&gt;addHitable(new Sphere(center, 0.2, new Lambertian (Vector3D(drand48()*drand48(), drand48()*drand48(), drand48()*drand48())))); // metal else if (choose_mat &lt; 0.6f) list-&gt;addHitable(new Sphere(center, 0.2, new Metal (Vector3D(0.5f*(1.0f + drand48()), 0.5f*(1.0f + drand48()), 0.5f*(1.0f + drand48())), 0.5f*drand48()))); // glass else list-&gt;addHitable(new Sphere(center, 0.2, new Dielectric (1.5f))); &#125; &#125; &#125; list-&gt;addHitable(new Sphere(Vector3D(0, 1, 0), 1.0, new Dielectric(1.5f))); list-&gt;addHitable(new Sphere(Vector3D(-4, 1, 0), 1.0, new Lambertian(Vector3D(0.4, 0.2, 0.1)))); list-&gt;addHitable(new Sphere(Vector3D(4, 1, 0), 1.0, new Metal(Vector3D(0.7, 0.6, 0.5), 0.0f))); return list;&#125;Vector4D Tracer::tracing(const Ray &amp;r, Hitable *world, int depth)&#123; HitRecord rec; if (world-&gt;hit(r, 0.001f, FLT_MAX, rec)) &#123; Ray scattered; Vector3D attenuation; if (depth &lt; m_config.m_maxDepth &amp;&amp; rec.m_material-&gt;scatter(r, rec, attenuation, scattered)) return attenuation * tracing(scattered, world, depth + 1); else return Vector4D(0.0f, 0.0f, 0.0f, 1.0f); //return backgroundColor(Ray(rec.m_position, target - rec.m_position), world) * 0.5f; //return rec.normal * 0.5f + Vector3D(0.5f, 0.5f, 0.5f); &#125; else &#123; float t = 0.5f * (r.getDirection().y + 1.0f); Vector4D ret = Vector3D(1.0f, 1.0f, 1.0f) * (1.0f - t) + Vector3D(0.5f, 0.7f, 1.0f) * t; ret.w = 1.0f; return ret; &#125;&#125; 三、程序结果 参考资料$[1]$ https://www.cnblogs.com/jerrycg/p/4941359.html $[2]$ https://blog.csdn.net/baishuo8/article/details/81476422 $[3]$ https://blog.csdn.net/silangquan/article/details/8176855 $[4]$ Peter Shirley. Ray Tracing in One Weekend. Amazon Digital Services LLC, January 26, 2016. $[5]$ https://learnopengl-cn.github.io/07%20PBR/01%20Theory/ $[6]$ https://www.cnblogs.com/lv-anchoret/p/10223222.html","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"http://yoursite.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"http://yoursite.com/tags/Ray-Tracer/"}]},{"title":"软渲染器Soft Renderer：光照着色篇（完结）","slug":"SoftRenderer-Shading","date":"2019-05-05T12:39:50.871Z","updated":"2019-05-25T07:03:43.119Z","comments":true,"path":"2019/05/05/SoftRenderer-Shading/","link":"","permalink":"http://yoursite.com/2019/05/05/SoftRenderer-Shading/","excerpt":"在前面的博客我们已经实现了基本的三维渲染管线流程，这一章主要是在此基础上进行润色，不借助任何库实现obj模型导入、Blin-Phong光照模型、摄像机漫游（第一人称摄像机、第三人称摄像机）。注意：初学者慎入","text":"在前面的博客我们已经实现了基本的三维渲染管线流程，这一章主要是在此基础上进行润色，不借助任何库实现obj模型导入、Blin-Phong光照模型、摄像机漫游（第一人称摄像机、第三人称摄像机）。注意：初学者慎入 obj模型导入 Blinn-Phong光照着色 虚拟场景漫游 程序结果 结语 一、obj模型导入&emsp;&emsp;obj模型文件（这里不是指c++编译得到的.o中间文件）是一种格式简单、清晰的模型文件，这种模型的格式非常容易解析。目前有一个非常流行的开源的模型导入库Assimp，封装了各种各样模型文件的加载，省去很多麻烦。而我因为一方面为了尽量避免引入第三方库，另一方面obj模型的导入不难，所以自己实现了一个obj加载类$ObjModel$。实现obj模型加载并不难，只需简单了解一下obj文件的格式即可。 &emsp;&emsp;obj文件格式有类数据，一类一行，分别以v、vt、vn和f开头。用记事本打开一个简单的obj文件，如下所示： &emsp;&emsp;以v（即vertex的缩写）开头的一行分别为模型顶点的$x$、$y$、$z$坐标，以vt（即vertex texcoord的缩写）开头的一行分别为纹理坐标的$u$、$v$值，以vn（即vertex normal的缩写）开头的一行分别是法向量的$x$、$y$、$z$值。而f（即face的缩写）格式为v/vt/vn，其中对应的是各自的索引值，一个v/vt/vn描述了一个三角形顶点的顶点坐标、纹理坐标、法线向量，通常以f的一行有三列v/vt/vn，组成一个三角形面片。所以我们读取的时候按照这些开头标记读取即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class ObjModel : public Mesh&#123;private: Vector3D minPoint, maxPoint; // Bounding box.public: // ctor/dtor. ObjModel(const std::string &amp;path); virtual ~ObjModel(); // Size setting. Vector3D setSizeToVector(float sx, float sy, float sz) const; Matrix4x4 setSizeToMatrix(float sx, float sy, float sz) const;private: // Obj file loader. void loadObjFile(const std::string &amp;path);&#125;;void ObjModel::loadObjFile(const std::string &amp;path)&#123; // obj loader. ifstream in; in.open(path, ifstream::in); if(in.fail()) &#123; std::cout &lt;&lt; \"Fail to load obj-&gt;\" &lt;&lt; path &lt;&lt; endl; &#125; string line; minPoint = Vector3D(+10000000000,+10000000000,+10000000000); maxPoint = Vector3D(-10000000000,-10000000000,-10000000000); vector&lt;Vector3D&gt; vertices; vector&lt;Vector3D&gt; normals; vector&lt;Vector2D&gt; texcoords; while(!in.eof()) &#123; getline(in, line); istringstream iss(line.c_str()); char trash; //vertex if(!line.compare(0, 2, \"v \")) &#123; iss &gt;&gt; trash; Vector3D vertex; iss &gt;&gt; vertex.x; iss &gt;&gt; vertex.y; iss &gt;&gt; vertex.z; vertices.push_back(vertex); if(minPoint.x &gt; vertex.x)minPoint.x = vertex.x; if(minPoint.y &gt; vertex.y)minPoint.y = vertex.y; if(minPoint.z &gt; vertex.z)minPoint.z = vertex.z; if(maxPoint.x &lt; vertex.x)maxPoint.x = vertex.x; if(maxPoint.y &lt; vertex.y)maxPoint.y = vertex.y; if(maxPoint.z &lt; vertex.z)maxPoint.z = vertex.z; &#125; // normal else if(!line.compare(0, 3, \"vn \")) &#123; iss &gt;&gt; trash &gt;&gt; trash; Vector3D normal; iss &gt;&gt; normal.x; iss &gt;&gt; normal.y; iss &gt;&gt; normal.z; normal.normalize(); normals.push_back(normal); &#125; // texcoord else if(!line.compare(0, 3, \"vt \")) &#123; iss &gt;&gt; trash &gt;&gt; trash; Vector2D texcoord; iss &gt;&gt; texcoord.x; iss &gt;&gt; texcoord.y; texcoords.push_back(texcoord); &#125; // face else if(!line.compare(0, 2, \"f \")) &#123; iss &gt;&gt; trash; int index[3]; while(iss &gt;&gt; index[0] &gt;&gt; trash &gt;&gt; index[1] &gt;&gt; trash &gt;&gt; index[2]) &#123; Vertex data; data.position = vertices[index[0] - 1]; data.texcoord = texcoords[index[1] - 1]; data.normal = normals[index[2] - 1]; data.color = Vector4D(1.0,1.0,1.0,1.0); m_indices.push_back(m_vertices.size()); m_vertices.push_back(data); &#125; &#125; &#125; in.close();&#125; &emsp;&emsp;可以看到这里继承了父类$Mesh$，这样读进来就作为一个网格类，能够传进渲染管线中渲染。测试读取了几个模型文件，效果如下： 二、Blin-Phong光照着色&emsp;&emsp;之前我们的着色器一直都是直接传输数据，没有做一些着色器计算，这里我们给渲染出来的模型加上光照着色。采用的光照模型是Blinn-Phong光照模型，并实现了两种着色器方法，分别是Gouraud着色、Phong着色。注意别混淆了光照模型和着色模型，光照模型是一种理论模型，着色模型则是具体的实现方式。Gouraud着色和Phong着色都是采用Blinn-Phong光照模型，差别在于两者在何处实现光照计算。 &emsp;&emsp;网上的LearnOpenGL教程很详细地介绍了Phong光照模型以及Blinn-Phong光照（Phong和Blinnn的差别只在于高光计算的一小部分），我就不再说太多这些方面的东西了，想具体了解的朋友请看这里)和这里)。概括起来，Phong光照模型包含环境光、漫反射光和镜面高光，其计算方式如下： I=K_aI_a+k_dI_ecos\\alpha+k_sI_scos^n\\lambda \\tag {1}&emsp;&emsp;其中的$k_a$、$k_d$和$k_s$分别为物体的环境光颜色、漫反射颜色和镜面高光颜色数，$n$是物体的高光读，而$I_a$、$I_e$和$I_s$是光源的环境光颜色、漫反射照亮的颜色和镜面反射的颜色。针对物体材质和光照的种类，我们创建一个$Material$和虚类$Light$，并把光照的计算过程抽象为一个函数$lighting$： 123456789101112131415161718192021222324252627282930313233class Material&#123;public: Material() = default; ~Material() = default; double m_shininess; Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_reflect; void setMaterial(Vector3D _amb, Vector3D _diff, Vector3D _spec, double _shin) &#123; m_shininess = _shin; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; &#125;&#125;;class Light&#123;public: Light() = default; virtual ~Light() = default; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const = 0;&#125;; &emsp;&emsp;根据光源的种类不同，通常有平行光、点光源和聚束光三类（关于这类光，请看LearnOpenGL的这篇)）。平行光的特点就是光线束都是平行的，因而只需记录平行光的方向即可： 12345678910111213141516171819202122232425262728293031323334353637383940414243class DirectionalLight : public Light&#123;public: Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_direction; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setDirectionalLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, Vector3D _dir) &#123; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_direction = _dir; m_direction.normalize(); &#125;&#125;;void DirectionalLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; float diff = max(normal.dotProduct(-this-&gt;m_direction), 0.0f); Vector3D halfwayDir = eyeDir + this-&gt;m_direction; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); ambient = m_ambient; diffuse = m_diffuse * diff; specular = m_specular * spec;&#125; &emsp;&emsp;点光源则需要记录光源的位置，用以计算光照的方向。与平行光还有一点不同的是，点光源通常有个照明区域范围，光照的强度随着距离的增加而削弱，且这类减弱不是线性的。因此我们还要衰减因子，把计算得到的光照颜色再乘上这个衰减因子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class PointLight : public Light&#123;public: Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_position; Vector3D m_attenuation; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setPointLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, Vector3D _pos, Vector3D _atte) &#123; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_position = _pos; m_attenuation = _atte; &#125;&#125;;void PointLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; // ambient ambient = this-&gt;m_ambient; // diffuse Vector3D lightDir = (this-&gt;m_position - position); lightDir.normalize(); float diff = max(normal.dotProduct(lightDir), 0.0f); diffuse = this-&gt;m_diffuse * diff; // specular Vector3D halfwayDir = eyeDir + lightDir; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); specular = this-&gt;m_specular * spec; // attenuation float distance = (this-&gt;m_position - position).getLength(); float attenuation = 1.0 / (m_attenuation.x + m_attenuation.y * distance + m_attenuation.z * (distance * distance)); ambient *= attenuation; diffuse *= attenuation; specular *= attenuation;&#125; &emsp;&emsp;聚束光是一种比较特殊的光源（例如手电筒光、舞台灯光），它的特点就是只有在聚光方向的特定半径内的物体才会被照亮，其它的物体都会保持黑暗。我们采用一个光源位置、照明方向和切光角来描述一个聚光灯： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class SpotLight : public Light&#123;public: double m_cutoff, m_outcutoff; Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_position; Vector3D m_direction; Vector3D m_attenuation; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setSpotLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, double _cut, Vector3D _pos, Vector3D _dir, Vector3D _atte) &#123; m_cutoff = cos(_cut * M_PI/180.0); m_outcutoff = cos((_cut + 10.0) * M_PI/180.0); m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_position = _pos; m_direction = _dir; m_attenuation = _atte; m_direction.normalize(); &#125;&#125;;void SpotLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; // ambient ambient = this-&gt;m_ambient; // diffuse Vector3D lightDir = this-&gt;m_position - position; lightDir.normalize(); float diff = max(normal.dotProduct(lightDir), 0.0f); diffuse = this-&gt;m_diffuse * diff ; // specular Vector3D halfwayDir = eyeDir + lightDir; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); specular = this-&gt;m_specular * spec; // spotlight (soft edges) float theta = lightDir.dotProduct(-this-&gt;m_direction); float epsilon = (this-&gt;m_cutoff - this-&gt;m_outcutoff); float intensity = (theta - this-&gt;m_outcutoff) / epsilon; if(intensity &lt; 0.0f)intensity = 0.0f; if(intensity &gt; 1.0f)intensity = 1.0f; diffuse *= intensity; specular *= intensity; // attenuation float distance = (this-&gt;m_position - position).getLength(); float attenuation = 1.0 / (m_attenuation.x + m_attenuation.y * distance + m_attenuation.z * (distance * distance)); ambient *= attenuation; diffuse *= attenuation; specular *= attenuation;&#125; &emsp;&emsp;然后我们就需要把光照计算集成到着色器中，这里提供了两种方式：光照计算集成到顶点着色器，即Gouraud着色方法，逐顶点光照，然后靠线性插值得到每个像素的光照颜色；光照计算集成到片元着色器，即Phong着色法，逐像素光照，根据插值得到的法向量做相应的计算。显然前者计算量少了很多，但是后者更为真实。我们建立一个$Gouraud$着色类如下： 123456789101112131415161718192021222324252627282930313233class GouraudShader : public BaseShader&#123;private: // Those are not created by shader. const Light *m_light; // Light.(just only one) const Material *m_material; // Mesh material. const Texture2D *m_unit; // Texture unit. Vector3D m_eyePos; // Observer's position. Matrix4x4 m_modelMatrix; // Model matrix. Matrix4x4 m_viewMatrix; // View matrix. Matrix4x4 m_projectMatrix; // Projection matrix. Matrix4x4 m_invModelMatrix; // Inverse of model matrix for normal.public: // ctor/dtor. GouraudShader(); virtual ~GouraudShader() = default; // Shader stage. virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); // Shader setting. virtual void bindShaderUnit(Texture2D *unit)&#123;m_unit = unit;&#125; virtual void setModelMatrix(const Matrix4x4 &amp;world) &#123;m_modelMatrix = world;m_invModelMatrix = m_modelMatrix.getInverseTranspose();&#125; virtual void setViewMatrix(const Matrix4x4 &amp;view)&#123;m_viewMatrix = view;&#125; virtual void setProjectMatrix(const Matrix4x4 &amp;project)&#123;m_projectMatrix = project;&#125; virtual void setMaterial(const Material *material)&#123;m_material = material;&#125; virtual void setLight(const Light *light)&#123;m_light = light;&#125; virtual void setEyePos(const Vector3D eye)&#123;m_eyePos = eye;&#125;&#125;; &emsp;&emsp;这里提一下关于顶点法向量的变换矩阵。我们目前已经有顶点的model矩阵，但是顶点做变换之后的法向量却不能直接乘上model矩阵获得。我们知道顶点的切线与法线相互垂直，因而它们的点乘为$0$，即有： N\\cdot T = N^T*T = 0 \\tag {2}&emsp;&emsp;顶点切线必然随着模型矩阵的变换而变换，即模型矩阵为$M$，因而变换后的切线$T’=M\\cdot T$。我们记变换后的法向量为$N’$，其正确的法线变换为$Q$，则$N’=Q\\cdot N$，那么变换后$N’$和$T’$应该依旧保持垂直关系，依旧有$N’\\cdot T’=(Q\\cdot N)\\cdot (M\\cdot T)=(Q\\cdot N)^T\\cdot (M\\cdot T)=N^T\\cdot (Q^T\\cdot M)\\cdot T$，与公式$(2)$对比，我们只要令$Q^T\\cdot M = I$结果为单位矩阵，则有$N’\\cdot T’=N\\cdot T = 0$。从而可得法线的变换矩阵为： Q= (N^{-1})^T \\tag {3}123456789101112131415161718192021222324252627282930313233343536373839404142434445VertexOut GouraudShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.texcoord = in.texcoord; result.normal = m_invModelMatrix * Vector4D(in.normal); // Gouraud shading. if(m_unit) result.color = m_unit-&gt;sample(result.texcoord); Vector3D _amb, _diff, _spec; if(m_light) &#123; Vector3D eyeDir = m_eyePos - result.posTrans; eyeDir.normalize(); m_light-&gt;lighting(*m_material, result.posTrans, result.normal, eyeDir, _amb, _diff, _spec); result.color.x *= (_amb.x + _diff.x + _spec.x); result.color.y *= (_amb.y + _diff.y + _spec.y); result.color.z *= (_amb.z + _diff.z + _spec.z); result.color.w = 1.0f; &#125; // oneDivZ to correct lerp. result.oneDivZ = 1.0 / result.posH.w; result.posTrans *= result.oneDivZ; result.texcoord *= result.oneDivZ; result.color *= result.oneDivZ; return result;&#125;Vector4D GouraudShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor = in.color; return litColor;&#125; &emsp;&emsp;Phong着色方式则在$fragmentShader$中实现光照计算，原理简单，不再赘述。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class PhongShader : public BaseShader&#123;private: // Those are not created by shader. const Light *m_light; // Light.(just only one) const Material *m_material; // Mesh material. const Texture2D *m_unit; // Texture unit. Vector3D m_eyePos; // Observer's position. Matrix4x4 m_modelMatrix; // Model matrix. Matrix4x4 m_viewMatrix; // View matrix. Matrix4x4 m_projectMatrix; // Projection matrix. Matrix4x4 m_invModelMatrix; // Inverse of model matrix for normal.public: // ctor/dtor PhongShader(); virtual ~PhongShader() = default; // Shader stage. virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); // Shader setting. virtual void bindShaderUnit(Texture2D *unit)&#123;m_unit = unit;&#125; virtual void setModelMatrix(const Matrix4x4 &amp;world) &#123;m_modelMatrix = world;m_invModelMatrix = m_modelMatrix.getInverseTranspose();&#125; virtual void setViewMatrix(const Matrix4x4 &amp;view)&#123;m_viewMatrix = view;&#125; virtual void setProjectMatrix(const Matrix4x4 &amp;project)&#123;m_projectMatrix = project;&#125; virtual void setMaterial(const Material *material)&#123;m_material = material;&#125; virtual void setLight(const Light *light)&#123;m_light = light;&#125; virtual void setEyePos(const Vector3D eye)&#123;m_eyePos = eye;&#125;&#125;;VertexOut PhongShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.texcoord = in.texcoord; result.normal = m_invModelMatrix * Vector4D(in.normal); // oneDivZ to correct lerp. result.oneDivZ = 1.0 / result.posH.w; result.posTrans *= result.oneDivZ; result.texcoord *= result.oneDivZ; result.color *= result.oneDivZ; return result;&#125;Vector4D PhongShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor = in.color; // Gouraud shading. if(m_unit) litColor = m_unit-&gt;sample(in.texcoord); Vector3D _amb, _diff, _spec; if(m_light) &#123; Vector3D eyeDir = m_eyePos - in.posTrans; eyeDir.normalize(); m_light-&gt;lighting(*m_material, in.posTrans, in.normal, eyeDir, _amb, _diff, _spec); litColor.x *= (_amb.x + _diff.x + _spec.x); litColor.y *= (_amb.y + _diff.y + _spec.y); litColor.z *= (_amb.z + _diff.z + _spec.z); litColor.w = 1.0f; &#125; return litColor;&#125; &emsp;&emsp;下图分别为Phong着色方式的平行光、点光源、聚束光效果： 三、虚拟场景漫游&emsp;&emsp;虚拟场景漫游是一个三维程序必不可少的，我们比较常用的虚拟摄像机有两类：第一人称摄像机、第三人生摄像机。第三人称摄像机又称为半上帝视角，一般的rpg游戏都是采用的第三人称视角。摄像机一般都是相应键盘按键、鼠标移动、鼠标滚轮事件，为了方便描述，我们创建一个$Camera3D$虚类如下： 12345678910111213141516171819202122class Camera3D&#123;public: // Local axis. // Here LocalForward should (0,0,-1). static const Vector3D LocalForward; static const Vector3D LocalUp; static const Vector3D LocalRight; // ctor/dtor. Camera3D() = default; virtual ~Camera3D()&#123;&#125; // Getter. virtual Matrix4x4 getViewMatrix() = 0; virtual Vector3D getPosition() = 0; // Key/Mouse reaction. virtual void onKeyPress(char key) = 0; virtual void onWheelMove(double delta) = 0; virtual void onMouseMove(double deltaX, double deltaY, std::string button) = 0;&#125;; 1、第一人称相机&emsp;&emsp;LearnOpenGl的这篇)对第一人称相机的构建做了的很详细的描述。不同的是，我不再采用欧拉角来描述渲染，而是采用了四元数（关于四元数，请看知乎的这篇）。理解了四元数，采用欧拉角反而比较繁琐。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class FPSCamera : public Camera3D&#123;private: mutable bool m_dirty; // Should update or not. Vector3D m_translation; // Camera's translation. Quaternion m_rotation; // Camera's rotation. Matrix4x4 m_viewMatrix; // View Matrix.public: // ctor/dtor FPSCamera(Vector3D _pos); virtual ~FPSCamera() = default; // Getter. virtual Vector3D getPosition() &#123;return m_translation;&#125; virtual Matrix4x4 getViewMatrix(); // Key/Mouse reaction. virtual void onKeyPress(char key); virtual void onWheelMove(double delta); virtual void onMouseMove(double deltaX, double deltaY, std::string button); // Transform camera's axis. void translate(const Vector3D &amp;dt); void rotate(const Vector3D &amp;axis, float angle); void setTranslation(const Vector3D &amp;t); void setRotation(const Quaternion &amp;r); // Query for camera's axis. Vector3D forward() const; Vector3D up() const; Vector3D right() const;&#125;;void FPSCamera::onKeyPress(char key)&#123; switch(key) &#123; case 'W': this-&gt;translate(forward() * 0.2f); break; case 'S': this-&gt;translate(-forward() * 0.2f); break; case 'A': this-&gt;translate(-right() * 0.2f); break; case 'D': this-&gt;translate(+right() * 0.2f); break; case 'Q': this-&gt;translate(up() * 0.2f); break; case 'E': this-&gt;translate(-up() * 0.2f); break; default: break; &#125;&#125;void FPSCamera::onWheelMove(double delta)&#123; // nothing now.&#125;void FPSCamera::onMouseMove(double deltaX, double deltaY, std::string button)&#123; double speed = 0.1f; deltaX *= speed; deltaY *= speed; this-&gt;rotate(LocalUp, -deltaX); this-&gt;rotate(right(), -deltaY);&#125; 2、第三人称摄像机&emsp;&emsp;第三人称有一个固定的目标，这个目标通常就是玩家操控的物体。摄像机可以拉远拉近、围绕目标在$xz$平面旋转、绕$x$轴上下旋转，而且摄像机永远在玩家的上方（即俯视）。为此，我们用$distance$（摄像机到玩家的距离）、$pitch$（绕$x$轴的旋转角）、$yaw$（绕$y$轴的旋转角）来获取摄像机的位置，最后获取了摄像机的位置后我们就可以直接用$LookAt$矩阵获得视图矩阵。更多关于第三人称摄像机方面的细节请看youtube上的这个视频。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class TPSCamera : public Camera3D&#123;private: mutable bool m_dirty; // Should update or not. Vector3D m_cameraPos; // Camera's position. Transform3D m_player; // Player's transformation. Matrix4x4 m_viewMatrix; // View matrix. double m_yaw, m_pitch, m_distance; // yaw, pitch and distance to player's space.public: // ctor/dtor. TPSCamera(Vector3D target); virtual ~TPSCamera() = default; // Getter. Matrix4x4 getPlayerMatrix(); virtual Matrix4x4 getViewMatrix(); virtual Vector3D getPosition() &#123;update();return m_cameraPos;&#125; // Key/Mouse reaction. virtual void onKeyPress(char key); virtual void onWheelMove(double delta); virtual void onMouseMove(double deltaX, double deltaY, std::string button);private: // Update view matrix. void update();&#125;;void TPSCamera::onKeyPress(char key)&#123; double speed = 2.0f; switch(key) &#123; case 'W': m_dirty = true; m_player.translate(-m_player.forward() * 0.1f); break; case 'S': m_dirty = true; m_player.translate(+m_player.forward() * 0.1f); break; case 'A': m_dirty = true; m_player.rotate(m_player.up(), +speed); break; case 'D': m_dirty = true; m_player.rotate(m_player.up(), -speed); break; &#125;&#125;void TPSCamera::onWheelMove(double delta)&#123; m_dirty = true; double speed = 0.01; m_distance += -speed * delta; if(m_distance &gt; 35.0)m_distance = 35.0; if(m_distance &lt; 5.00)m_distance = 5.0;&#125;void TPSCamera::onMouseMove(double deltaX, double deltaY, std::string button)&#123; double speed = 0.2; if(button == \"RIGHT\") &#123; m_dirty = true; m_pitch += speed * deltaY; if(m_pitch &lt; 0.0)m_pitch = 0.0; if(m_pitch &gt; 89.9)m_pitch = 89.9; &#125; else if(button == \"LEFT\") &#123; m_dirty = true; m_yaw += -speed * deltaX; fmod(m_yaw, 360.0); &#125;&#125;void TPSCamera::update()&#123; if(m_dirty) &#123; m_dirty = false; Vector3D target = m_player.translation(); float height = m_distance * sin(radians(m_pitch)); float horizon = m_distance * cos(radians(m_pitch)); Vector3D _playerRot = m_player.rotation().eulerAngle(); _playerRot.y = fmod(_playerRot.y, 360); m_cameraPos.y = target.y + height; m_cameraPos.x = target.x + horizon * sin(radians(m_yaw)); m_cameraPos.z = target.z + horizon * cos(radians(m_yaw)); m_viewMatrix.setLookAt(m_cameraPos, m_player.translation(), LocalUp); &#125;&#125; 四、程序结果 五、结语&emsp;&emsp;软渲染器的搭建就此告一段落，不借助任何图形库从零开始搭建这么一个渲染管线的初衷是为了更加深入地了解当前三维渲染的整个流程，很多理论东西需要实践才能彻底地理解。这么几天关于搭建软渲染器的折腾让我收获不少，这为以后的图形学道路打下了深厚的基础。目前我实现的软渲染管线已经包含了一个传统固定管线的基本功能，我借助一些工具统计得软渲染管线的核心代码（不包括空行、注释）共2838行。不再打算加入更多的功能特性如透明融合、阴影等等，因为没必要了。相关的全部源代码已经提交到github上，请点这里。 &emsp;&emsp;由于本人的知识水平有限，若网友发现任何bug或者博文叙述错误，欢迎指正，感谢！ 参考资料$[1]$ https://learnopengl-cn.github.io/02%20Lighting/02%20Basic%20Lighting/ $[2]$ https://learnopengl-cn.github.io/01%20Getting%20started/09%20Camera/ $[3]$ https://www.youtube.com/watch?v=PoxDDZmctnU&amp;list=PLRIWtICgwaX0u7Rf9zkZhLoLuZVfUksDP&amp;index=19 $[4]$ https://github.com/ssloy/tinyrenderer/wiki","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/tags/Soft-Renderer/"},{"name":"3D pipeline","slug":"3D-pipeline","permalink":"http://yoursite.com/tags/3D-pipeline/"}]},{"title":"软渲染器Soft Renderer：进击三维篇","slug":"SoftRenderer-3DPipeline","date":"2019-05-02T06:26:22.186Z","updated":"2019-05-25T07:09:49.817Z","comments":true,"path":"2019/05/02/SoftRenderer-3DPipeline/","link":"","permalink":"http://yoursite.com/2019/05/02/SoftRenderer-3DPipeline/","excerpt":"有了自己实现好的的3D数学库和一个基本的光栅化渲染框架，就可以开始向这个渲染框架填充内容了。本章内容主要关于3维渲染管线的实现、深度测试、背面剔除、几何裁剪、透视纹理映射，这些内容早已被渲染API集成。学习和实现这些算法，是为了彻底了解三维物体的整个渲染流程。注意：初学者慎入","text":"有了自己实现好的的3D数学库和一个基本的光栅化渲染框架，就可以开始向这个渲染框架填充内容了。本章内容主要关于3维渲染管线的实现、深度测试、背面剔除、几何裁剪、透视纹理映射，这些内容早已被渲染API集成。学习和实现这些算法，是为了彻底了解三维物体的整个渲染流程。注意：初学者慎入 进入三维世界 裁剪、剔除优化 透视纹理映射、采样 程序结果 一、进入三维世界&emsp;&emsp;尽管二维的屏幕只能显示二维的像素，但是我们可以通过将三维的物体变换到二维的屏幕上，从而渲染出三维空间的一个投影面。这与我们人类的视觉系统类似，视网膜上最终获取的也只是三维空间某个角度下的投影。为了让三维物体正确地显示到屏幕上，我们需要借助一系列的坐标空间变换。 1、坐标系统&emsp;&emsp;在渲染管线中，三维物体的顶点在最终转换为屏幕坐标之前会被变换到多个坐标系统，这其中有几个过渡性的坐标系，使得整个变换流程逻辑清晰、便于理解。此外在某些特定情况下在这些特定的坐标系中，一些操作更加容易、方便和灵活。通常，渲染管线有$5$个不同的坐标系统，分别是局部空间、世界空间、视觉空间、裁剪空间和屏幕空间，以下是LearnOpenGL CN)的原话： 局部坐标是对象相对于局部原点的坐标，也是物体起始的坐标。 下一步是将局部坐标变换为世界空间坐标，世界空间坐标是处于一个更大的空间范围的。这些坐标相对于世界的全局原点，它们会和其它物体一起相对于世界的原点进行摆放。 接下来我们将世界坐标变换为观察空间坐标，使得每个坐标都是从摄像机或者说观察者的角度进行观察的。 坐标到达观察空间之后，我们需要将其投影到裁剪坐标。裁剪坐标会被处理至-1.0到1.0的范围内，并判断哪些顶点将会出现在屏幕上。 最后，我们将裁剪坐标变换为屏幕坐标，我们将使用一个叫做视口变换(Viewport Transform)的过程。视口变换将位于-1.0到1.0范围的坐标变换到由glViewport函数所定义的坐标范围内。最后变换出来的坐标将会送到光栅器，将其转化为片段 &emsp;&emsp;通过以上的几个步骤，三维的物体坐标最终变换到了屏幕的坐标上，其中视图矩阵和投影矩阵的构建较为复杂一点，前面我的博文软渲染器Soft Renderer：3D数学篇已经推导过这两个矩阵，这里就不再赘述了。若想查看更多关于坐标系统的内容，请查看LearnOpenGL CN的这篇文章：坐标系统)。坐标变换是一般发生在顶点着色器以及顶点着色器输出到光栅化这一阶段，视口变换在顶点着色器输出之后，不在着色器中进行（视口变换已经在前面的光栅化篇提到过了）。所以为了实现坐标变换，我们的着色器要存储$model$、$view$、$project$这三个矩阵，在$SimpleShader$中添加相关的成员变量及方法： 1234567891011121314151617181920212223242526272829class SimpleShader : public BaseShader&#123;private: Matrix4x4 m_modelMatrix; Matrix4x4 m_viewMatrix; Matrix4x4 m_projectMatrix;public: ...... virtual void setModelMatrix(const Matrix4x4 &amp;world); virtual void setViewMatrix(const Matrix4x4 &amp;view); virtual void setProjectMatrix(const Matrix4x4 &amp;project);&#125;;void SimpleShader::setModelMatrix(const Matrix4x4 &amp;world)&#123; m_modelMatrix = world;&#125;void SimpleShader::setViewMatrix(const Matrix4x4 &amp;view)&#123; m_viewMatrix = view;&#125;void SimpleShader::setProjectMatrix(const Matrix4x4 &amp;project)&#123; m_projectMatrix = project;&#125; &emsp;&emsp;这样外部要渲染时，应该向着色器输入这三个矩阵。然后在我们的顶点着色器中填入相关的逻辑： 12345678910VertexOut SimpleShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.normal = in.normal; result.texcoord = in.texcoord; return result;&#125; &emsp;&emsp;$VertexOut$是前面文章定义的顶点着色器输出的类，它存储投影后的顶点$posH$、世界空间中的顶点$posTrans$、物体的颜色、顶点法线以及纹理坐标。接着在视口变换并送入光栅化部件之前执行透视除法，即直接将裁剪空间的顶点坐标除以它的第四个分量$w$即可。然后我们在外部的渲染循环中设置模型矩阵、视图矩阵已经投影矩阵，就能显示出三维的立体感了，以我们前一章画的三角形为例（gif录制的好像有bug，出现绿色它就给我录制成这个模糊的鬼样，实际上是非常清晰，不是渲染的锅）。 &emsp;&emsp;进入3D世界，怎么能少了3D渲染的”hello world!”——立方体呢？在$Mesh.h$手动创建一个立方体的网格数据，然后用立方体替换掉上面丑陋的三角形： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154void Mesh::asBox(double width, double height, double depth)&#123; vertices.resize(24); indices.resize(36); float halfW = width * 0.5f; float halfH = height * 0.5f; float halfD = depth * 0.5f; //front vertices[0].position = Vector3D(halfW, halfH, halfD); vertices[0].normal = Vector3D(0.f, 0.f, 1.f); vertices[0].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[0].texcoord = Vector2D(1.f, 1.f); vertices[1].position = Vector3D(-halfW, halfH, halfD); vertices[1].normal = Vector3D(0.f, 0.f, 1.f); vertices[1].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[1].texcoord = Vector2D(0.f, 1.f); vertices[2].position = Vector3D(-halfW,-halfH, halfD); vertices[2].normal = Vector3D(0.f, 0.f, 1.f); vertices[2].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[2].texcoord = Vector2D(0.f, 0.f); vertices[3].position = Vector3D(halfW, -halfH, halfD); vertices[3].normal = Vector3D(0.f, 0.f, 1.f); vertices[3].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[3].texcoord = Vector2D(1.f, 0.f); //left vertices[4].position = Vector3D(-halfW, +halfH, halfD); vertices[4].normal = Vector3D(-1.f, 0.f, 0.f); vertices[4].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[4].texcoord = Vector2D(1.f, 1.f); vertices[5].position = Vector3D(-halfW, +halfH, -halfD); vertices[5].normal = Vector3D(-1.f, 0.f, 0.f); vertices[5].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[5].texcoord = Vector2D(0.f, 1.f); vertices[6].position = Vector3D(-halfW, -halfH, -halfD); vertices[6].normal = Vector3D(-1.f, 0.f, 0.f); vertices[6].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[6].texcoord = Vector2D(0.f, 0.f); vertices[7].position = Vector3D(-halfW, -halfH, halfD); vertices[7].normal = Vector3D(-1.f, 0.f, 0.f); vertices[7].color = Vector4D(1.f, 1.f, 1.f, 1.f); vertices[7].texcoord = Vector2D(1.f, 0.f); //back vertices[8].position = Vector3D(-halfW, +halfH, -halfD); vertices[8].normal = Vector3D(0.f, 0.f, -1.f); vertices[8].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[8].texcoord = Vector2D(0.f, 0.f); vertices[9].position = Vector3D(+halfW, +halfH, -halfD); vertices[9].normal = Vector3D(0.f, 0.f, -1.f); vertices[9].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[9].texcoord = Vector2D(1.f, 0.f); vertices[10].position = Vector3D(+halfW, -halfH, -halfD); vertices[10].normal = Vector3D(0.f, 0.f, -1.f); vertices[10].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[10].texcoord = Vector2D(1.f, 1.f); vertices[11].position = Vector3D(-halfW, -halfH, -halfD); vertices[11].normal = Vector3D(0.f, 0.f, -1.f); vertices[11].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[11].texcoord = Vector2D(0.f, 1.f); //right vertices[12].position = Vector3D(halfW, +halfH, -halfD); vertices[12].normal = Vector3D(1.f, 0.f, 0.f); vertices[12].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[12].texcoord = Vector2D(0.f, 0.f); vertices[13].position = Vector3D(halfW, +halfH, +halfD); vertices[13].normal = Vector3D(1.f, 0.f, 0.f); vertices[13].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[13].texcoord = Vector2D(1.f, 0.f); vertices[14].position = Vector3D(halfW, -halfH, +halfD); vertices[14].normal = Vector3D(1.f, 0.f, 0.f); vertices[14].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[14].texcoord = Vector2D(1.f, 1.f); vertices[15].position = Vector3D(halfW, -halfH, -halfD); vertices[15].normal = Vector3D(1.f, 0.f, 0.f); vertices[15].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[15].texcoord = Vector2D(0.f, 1.f); //top vertices[16].position = Vector3D(+halfW, halfH, -halfD); vertices[16].normal = Vector3D(0.f, 1.f, 0.f); vertices[16].color = Vector4D(0.f, 0.f, 0.f, 1.f); vertices[16].texcoord = Vector2D(0.f, 0.f); vertices[17].position = Vector3D(-halfW, halfH, -halfD); vertices[17].normal = Vector3D(0.f, 1.f, 0.f); vertices[17].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[17].texcoord = Vector2D(1.f, 0.f); vertices[18].position = Vector3D(-halfW, halfH, halfD); vertices[18].normal = Vector3D(0.f, 1.f, 0.f); vertices[18].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[18].texcoord = Vector2D(1.f, 1.f); vertices[19].position = Vector3D(+halfW, halfH, halfD); vertices[19].normal = Vector3D(0.f, 1.f, 0.f); vertices[19].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[19].texcoord = Vector2D(0.f, 1.f); //down vertices[20].position = Vector3D(+halfW, -halfH, -halfD); vertices[20].normal = Vector3D(0.f, -1.f, 0.f); vertices[20].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[20].texcoord = Vector2D(0.f, 0.f); vertices[21].position = Vector3D(+halfW, -halfH, +halfD); vertices[21].normal = Vector3D(0.f, -1.f, 0.f); vertices[21].color = Vector4D(1.f, 1.f, 1.f, 1.f); vertices[21].texcoord = Vector2D(1.f, 0.f); vertices[22].position = Vector3D(-halfW, -halfH, +halfD); vertices[22].normal = Vector3D(0.f, -1.f, 0.f); vertices[22].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[22].texcoord = Vector2D(1.f, 1.f); vertices[23].position = Vector3D(-halfW, -halfH, -halfD); vertices[23].normal = Vector3D(0.f, -1.f, 0.f); vertices[23].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[23].texcoord = Vector2D(0.f, 1.f); //front indices[0] = 0; indices[1] = 1; indices[2] = 2; indices[3] = 0; indices[4] = 2; indices[5] = 3; //left indices[6] = 4; indices[7] = 5; indices[8] = 6; indices[9] = 4; indices[10] = 6; indices[11] = 7; //back indices[12] = 8; indices[13] = 9; indices[14] = 10; indices[15] = 8; indices[16] = 10; indices[17] = 11; //right indices[18] = 12; indices[19] = 13; indices[20] = 14; indices[21] = 12; indices[22] = 14; indices[23] = 15; //top indices[24] = 16; indices[25] = 17; indices[26] = 18; indices[27] = 16; indices[28] = 18; indices[29] = 19; //down indices[30] = 20; indices[31] = 21; indices[32] = 22; indices[33] = 20; indices[34] = 22; indices[35] = 23;&#125; &emsp;&emsp;结果我们就得到一个如下面所示的奇怪的立方体： &emsp;&emsp;下面是动图gif（再重复一遍，模糊不是渲染的锅）： &emsp;&emsp;这的确有点像是一个立方体，但又有种说不出的奇怪。立方体的某些本应被遮挡住的面被绘制在了这个立方体其他面之上。出现这样结果的原因是因为我们的软渲染器是对一个一个三角形进行绘制的，而且计算像素时时直接覆盖而不管这个像素是否已经有其他值了，所以一个像素的值完全取决于最后赋予它的$RGBA$。除非渲染管线自动按照从远到近的顺序（这类算法有画家算法、空间分割BSP树算法）绘制三角形，否则直接覆盖的方法获取不了正确的像素值。正确渲染结果应该是像素的$RGBA$值为最靠近视点的片元值，一种常用的技术是借助第三维信息——深度来对每个相同位置的不同片元做深度的比较，并且取深度较低的那一个。 2、深度测试&emsp;&emsp;为了获取正确的三维渲染结果，我们采用一种深度缓冲的技术。深度缓冲存储深度信息，它的分辨率应该与颜色缓冲一致，深度值存储在每个片段里面（作为片段的z值），当片段想要输出它的颜色时，我们将它的深度值和z缓冲进行比较，如果当前的片段在其它片段之后，它将会被丢弃，否则将会覆盖。这个过程称为深度测试。在OpenGL和DirectX这些渲染API中，深度缓冲会自动执行而无需用户操作。在我们的软渲染器中，我们自己实现一个这样的深度测试，算法原理很简单，但是效果非常不错！ &emsp;&emsp;深度缓冲通常和颜色缓冲一起，作为帧缓冲的附件，我们在帧缓冲类中增加深度缓冲相关的变量、方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class FrameBuffer&#123;private: ...... std::vector&lt;double&gt; m_depthBuffer;public: ...... void clearColorAndDepthBuffer(const Vector4D &amp;color); double getDepth(const unsigned int &amp;x, const unsigned int &amp;y)const; void drawDepth(const unsigned int &amp;x, const unsigned int &amp;y, const double &amp;value);&#125;;void FrameBuffer::clearColorAndDepthBuffer(const Vector4D &amp;color)&#123; // fill the color buffer and depth buffer. unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); for(unsigned int row = 0;row &lt; m_height;++ row) &#123; for(unsigned int col = 0;col &lt; m_width;++ col) &#123; m_depthBuffer[row*m_width+col] = 1.0f; ...... &#125; &#125;&#125;double FrameBuffer::getDepth(const unsigned int &amp;x, const unsigned int &amp;y) const&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return 0.0f; return m_depthBuffer[y*m_width+x];&#125;void FrameBuffer::drawDepth(const unsigned int &amp;x, const unsigned int &amp;y, const double &amp;value)&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return; unsigned int index = y*m_width + x; m_depthBuffer[index] = value;&#125; &emsp;&emsp;然后我们对于每一个片元，我们获取深度缓冲中相应的数值并进行比较。在这之前，我们还要简单回顾一下在透视投影矩阵中深度值的非线性映射，在前面的数学篇中我们知道透视投影矩阵有如下形式： M_{projection}= \\left( \\begin{matrix} \\frac{1}{aspect*tan(fovy/2)}&0&0&0\\\\ 0&\\frac{1}{tan(fovy/2)}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right)&emsp;&emsp;因而视图空间中的深度信息$z_e$和标准化设备空间中的深度信息$z_n$关系为： z_n=(-\\frac{f+n}{f-n}z_e-\\frac{2fn}{f-n})/{-z_e} =\\frac{2fn}{z_e(f-n)}+\\frac{f+n}{f-n} \\tag {1}&emsp;&emsp;可以看到$z_e$d到$z_n$是一种从$[-f, -n]$到$[-1,1]$的非线性映射。当$z_e$比较小的时候，公式$(1)$有很高的精度；当$z_e$比较大的时候，公式$(1)$应为取值精度降低。这个关系可以直观地从下图的函数曲线看出来： &emsp;&emsp;可以看到，深度值很大一部分是由很小的z值所决定的，这给了近处的物体很大的深度精度。$z_n$取值为$[-1,1]$，我们最后将其简单地映射到$[0,1]$，这一步我放在透视除法后。 123456789void Pipeline::perspectiveDivision(VertexOut &amp;target)&#123; target.posH.x /= target.posH.w; target.posH.y /= target.posH.w; target.posH.z /= target.posH.w; target.posH.w = 1.0f; // map from [-1,1] to [0,1] target.posH.z = (target.posH.z+1.0f) * 0.5f;&#125; &emsp;&emsp;在写入深度缓冲之前应该要清除上一帧的深度缓冲，全部置$1.0f$即可，我把这一步和清除颜色缓冲放一起了，即前面的帧缓冲类的$clearColorAndDepthBuffer$方法。在光栅化步骤，获取每个片元的屏幕位置，查找深度缓并比较，若小于当前深度缓冲中获取的值，则通过深度测试并写入深度缓冲。 12345678910111213141516171819202122232425262728void Pipeline::scanLinePerRow(const VertexOut &amp;left, const VertexOut &amp;right)&#123; // scan the line from left to right. VertexOut current; int length = right.posH.x - left.posH.x + 1; for(int i = 0;i &lt;= length;++i) &#123; // linear interpolation double weight = static_cast&lt;double&gt;(i)/length; current = lerp(left, right, weight); current.posH.x = left.posH.x + i; current.posH.y = left.posH.y; // depth testing. double depth = m_backBuffer-&gt;getDepth(current.posH.x, current.posH.y); if(current.posH.z &gt; depth) continue;// fail to pass the depth testing. m_backBuffer-&gt;drawDepth(current.posH.x,current.posH.y,current.posH.z); double w = 1.0/current.oneDivZ; current.posTrans *= w; current.color *= w; current.texcoord *= w; // fragment shader m_backBuffer-&gt;drawPixel(current.posH.x, current.posH.y, m_shader-&gt;fragmentShader(current)); &#125;&#125; &emsp;&emsp;然后就可以根据深度信息正确地渲染出三维的立体感了。 3、裁剪、剔除优化&emsp;&emsp;目前目前我们已经构建出三维的渲染管线，但是这还不够，因为图形渲染计算量很大，通常我们需要做一些优化。常见的嵌入在渲染管线中的优化算法有几何裁剪、背面剔除。 几何裁剪&emsp;&emsp;注意在坐标系统的变换过程中，位于视锥体内的顶点坐标各分量都会被映射到$[-1,1]$的范围内，超出视锥体的顶点则被映射到超出$[-1,1]$的范围。我们在这个基础上的做相关的裁剪，注意在透视除法之前各分量实际上是处于$[-w,w]$的范围内的，这里的$w$就是该顶点坐标的第四个分量$w$。针对线框模式渲染和填充模式渲染，我们有两种不同的裁剪算法。 Cohen-Sutherland线条裁剪算法&emsp;&emsp;一条线段在视口内的情况有如下所示的四种。其中端点完全在视口内和一端在视口内而另一端是在视口外的情况很好判断，但是线段完全在视口外就没那么简单了。可以看到线段$GH$的端点都在视口外部，但是线段的一部分却在视口的内部，这是如果直接根据两个端点是否在视口外做剔除的话会导致在边缘部分的线段直接消失，得到错误的结果。一种暴力的解法就是计算线段与视口窗口的交点，但是这并不高效。 &emsp;&emsp;Cohen-Sutherland提出了一种基于编码的判断算法，通过简单的移位、与或逻辑运算就可以判断一条线段处于哪种情况。对于每一个端点$(x,y)$，我们定义一个outcode——$b_0b_1b_2b_3$，视口所处的范围用$x_{min}$、$x_{max}$、$y_{min}$、$y_{max}$表示。每个端点$(x,y)$的outcode的计算方法如下： &emsp;&emsp;$b_0 = 1\\ if \\ y &gt; y_{max},\\ 0\\ otherwiose$ &emsp;&emsp;$b_1 = 1\\ if \\ y &lt; y_{min},\\ 0\\ otherwiose$ &emsp;&emsp;$b_2 = 1\\ if \\ x &gt; x_{min},\\ 0\\ otherwiose$ &emsp;&emsp;$b_3 = 1\\ if \\ x &lt; x_{max},\\ 0\\ otherwiose$ &emsp;&emsp;可以看出outcode将屏幕空间分成了$9$个部分： &emsp;&emsp;观察上面的$9$个区域，对于两个端点outcode1和outcode2，做如下的判断策略，其中的$OR$和$AND$是逻辑按位运算： &emsp;&emsp;若$(outcode1\\ OR\\ outcode2)==0$，那么线段就完全在视口内部； &emsp;&emsp;若$(outcode1\\ AND\\ outcode2)!=0$，那么线段就完全在视口外部； &emsp;&emsp;若$(outcode1\\ AND\\ outcode2)==0$，那么线段就可能部分在视口外部，部分在内部，还需要做进一步的判断（这里我进一步判断用了包围盒，因为比较常见和简单，就不过多描述了）。 &emsp;&emsp;这里我的实现就是只裁剪掉肯定完全在视口外部的线段，若还想裁剪掉部分外视口外部的线段则需要进一步的求交运算。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950bool Pipeline::lineCliping(const VertexOut &amp;from, const VertexOut &amp;to)&#123; // return whether the line is totally outside or not. float vMin = -from.posH.w, vMax = from.posH.w; float x1 = from.posH.x, y1 = from.posH.y; float x2 = to.posH.x, y2 = to.posH.y; int tmp = 0; int outcode1 = 0, outcode2 = 0; // outcode1 calculation. tmp = (y1&gt;vMax)?1:0; tmp &lt;&lt;= 3; outcode1 |= tmp; tmp = (y1&lt;vMin)?1:0; tmp &lt;&lt;= 2; outcode1 |= tmp; tmp = (x1&gt;vMax)?1:0; tmp &lt;&lt;= 1; outcode1 |= tmp; tmp = (x1&lt;vMin)?1:0; outcode1 |= tmp; // outcode2 calculation. tmp = (y2&gt;vMax)?1:0; tmp &lt;&lt;= 3; outcode2 |= tmp; tmp = (y2&lt;vMin)?1:0; tmp &lt;&lt;= 2; outcode2 |= tmp; tmp = (x2&gt;vMax)?1:0; tmp &lt;&lt;= 1; outcode2 |= tmp; tmp = (x2&lt;vMin)?1:0; outcode2 |= tmp; if((outcode1 &amp; outcode2) != 0) return true; // bounding box judge. Vector2D minPoint,maxPoint; minPoint.x = min(from.posH.x, to.posH.x); minPoint.y = min(from.posH.y, to.posH.y); maxPoint.x = max(from.posH.x, to.posH.x); maxPoint.y = max(from.posH.y, to.posH.y); if(minPoint.x &gt; vMax || maxPoint.x &lt; vMin || minPoint.y &gt; vMax || maxPoint.y &lt; vMin) return true; return false;&#125; 三角形裁剪&emsp;&emsp;判断三角形是否完全在外面也不能直接根据三个端点是否完全在视口外部来判断（我看有些软渲染的博主就用了这个错误的策略），因为还要考略以下的特殊情况。 &emsp;&emsp;为此，我直接计算三角形的轴向包围盒，然后这个包围盒判断三角形是否完全是视口外部。更进一步的裁剪是将部分在视口内部的三角形做求交，然后重新分割成完全在视口内部的三角形，这里我没有做进一步的裁剪。 1234567891011121314151617181920212223242526bool Pipeline::triangleCliping(const VertexOut &amp;v1, const VertexOut &amp;v2, const VertexOut &amp;v3)&#123; // true:not clip; // false: clip. float vMin = -1.0; float vMax = +1.0; // if the triangle is too far to see it, just return false. if(v1.posH.z &gt; vMax &amp;&amp; v2.posH.z &gt; vMax &amp;&amp; v3.posH.z &gt; vMax) return false; // if the triangle is behind the camera, just return false. if(v1.posH.z &lt; vMin &amp;&amp; v2.posH.z &lt; vMin &amp;&amp; v3.posH.z &lt; vMin) return false; // calculate the bounding box and check if clip or not. Vector2D minPoint,maxPoint; minPoint.x = min(v1.posH.x, min(v2.posH.x, v3.posH.x)); minPoint.y = min(v1.posH.y, min(v2.posH.y, v3.posH.y)); maxPoint.x = max(v1.posH.x, max(v2.posH.x, v3.posH.x)); maxPoint.y = max(v1.posH.y, max(v2.posH.y, v3.posH.y)); if(minPoint.x &gt; vMax || maxPoint.x &lt; vMin || minPoint.y &gt; vMax || maxPoint.y &lt; vMin) return false; return true;&#125; &emsp;&emsp;然后我们把几何裁剪放到渲染管线中，几何裁剪一般是在顶点着色器之后、光栅化之前。这里我把它放到了透视除法和视口变换之前。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667void Pipeline::drawIndex(RenderMode mode)&#123; // renderer pipeline. bool line1 = false, line2 = false, line3 = false; m_mode = mode; if(m_indices.empty()) return; for(unsigned int i = 0;i &lt; m_indices.size();i += 3) &#123; //! assembly to triangle primitive. Vertex p1,p2,p3; &#123; ...... &#125; //! vertex shader stage. VertexOut v1,v2,v3; &#123; ...... &#125; //! perspective division. &#123; ...... &#125; //! geometry cliping. &#123; if(m_config.m_geometryCliping) &#123; if(m_config.m_polygonMode == PolygonMode::Wire) &#123; line1 = lineCliping(v1,v2); line2 = lineCliping(v2,v3); line3 = lineCliping(v3,v1); &#125; if(m_config.m_polygonMode == PolygonMode::Fill &amp;&amp; !triangleCliping(v1,v2,v3)) continue; &#125; &#125; //! view port transformation. &#123; ...... &#125; //! rasterization and fragment shader stage. &#123; if(mode == RenderMode::wire) &#123; if(!line1) bresenhamLineRasterization(v1,v2); if(!line2) bresenhamLineRasterization(v2,v3); if(!line3) bresenhamLineRasterization(v3,v1); &#125; else if(mode == RenderMode::fill) &#123; edgeWalkingFillRasterization(v1,v2,v3); &#125; &#125; ...... &#125;&#125; 背面剔除&emsp;&emsp;背面剔除网上的这篇博客已经讲得非常详细了，原理也很简单，我就不过多描述。我们定义顶点逆时针的环绕顺序正面，然后通过三角形的三个顶点计算出法线，将顶点与视线做点乘并判断其符号即可。 123456789101112131415bool Pipeline::backFaceCulling(const Vector4D &amp;v1, const Vector4D &amp;v2, const Vector4D &amp;v3)&#123; // back face culling. if(m_mode == RenderMode::wire) return true; Vector4D tmp1 = v2 - v1; Vector4D tmp2 = v3 - v1; Vector3D edge1(tmp1.x, tmp1.y, tmp1.z); Vector3D edge2(tmp2.x, tmp2.y, tmp2.z); Vector3D viewRay(m_eyePos.x - v1.x, m_eyePos.y - v1.y, m_eyePos.z - v1.z); Vector3D normal = edge1.crossProduct(edge2); return normal.dotProduct(viewRay) &gt; 0;&#125; &emsp;&emsp;然后背面剔除应该放在渲染管线的顶点着色器输出之后，如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849void Pipeline::drawIndex(RenderMode mode)&#123; // renderer pipeline. bool line1 = false, line2 = false, line3 = false; m_mode = mode; if(m_indices.empty())return; for(unsigned int i = 0;i &lt; m_indices.size();i += 3) &#123; //! assembly to triangle primitive. Vertex p1,p2,p3; &#123; ...... &#125; //! vertex shader stage. VertexOut v1,v2,v3; &#123; ...... &#125; //! back face culling. &#123; if(!backFaceCulling(v1.posTrans, v2.posTrans, v3.posTrans)) continue; &#125; //! geometry cliping. &#123; ...... &#125; //! perspective division. &#123; ...... &#125; //! view port transformation. &#123; ...... &#125; //! rasterization and fragment shader stage. &#123; ...... &#125; &#125;&#125; 二、透视纹理映射、采样&emsp;&emsp;纹理映射是丰富三维物体细节的一个非常重要的方法，简单、廉价、快速，只需计算好的纹理坐标、纹理图片即可实现物体的多姿多彩。通常纹理图片的制作（除了过程式纹理的生成）由设计师完成，无需我们关心。而纹理坐标的计算则需要非常注意，送入渲染管线的纹理坐标只是逐顶点的纹理坐标，在光栅化阶段我们还要将纹理坐标做插值操作，最后根据插值后得到的纹理坐标对纹理图片采样获取片元的像素值。 1、透视纹理映射&emsp;&emsp;在光栅化阶段，我们是根据屏幕空间的$x$值和$y$值做线性插值操作获取片元的位置，而片元的纹理坐标如果也这么获得的话（这种方法叫做仿射纹理映射），将会导致严重的纹理扭曲。这是因为仿射纹理映射是基于这样的一个假设：物体空间的纹理坐标与屏幕空间的顶点坐标呈线性管线。 &emsp;&emsp;我们知道纹理坐标是定义在物体的顶点上面的，当我们根据屏幕空间的顶点坐标插值时，就默认了纹理坐标的变化与屏幕空间顶点坐标的变化是呈线性、均匀的关系的。但是问题在于：默认的屏幕空间上的线性关系，还原到世界空间中，就不是那么回事了，如下图所示。这张图是相机空间的一张俯视图。我们把一个多边形通过透视投影的方式变换到了投影平面上，图中红色的是世界空间中的多边形，蓝色的是变换到投影平面之后的多边形。可以看到，在投影平面上的蓝色线段被表示成若干个相等的单位步长线段。与此同时，投影面上单位步长的线段所对应的投影之前的红色线段的长度却不是相等的，从左到右所对应的长度依次递增。我们的纹理坐标是定义在红色的多边形上的，因此纹理坐标的增量应该是和红色线段的步长对应的。我们的线性插值却把纹理坐标增量根据蓝色线段的步长平均分配了。 &emsp;&emsp;这就导致了仿射纹理映射的错误的结果，如下图所示，仿射纹理映射产生了严重的扭曲。 &emsp;&emsp;而如果你不信，大可以试一试，然后你就会得到和我下面这张图一样奇怪的结果。 &emsp;&emsp;那么如何进行矫正了？网上的这篇博客已经非常详细地说明了相关的矫正方法，核心思想就是想办法让纹理坐标变得与屏幕空间的坐标线性相关，这一点可以看成纹理坐标的透视投影（与世界空间的顶点坐标投影到屏幕空间，从而通过插值获得其他的屏幕空间坐标进行光栅化有异曲同工之妙）。 &emsp;&emsp;纹理透视投影的详细过程请看这篇博客，其中借助的关系就是纹理坐标与世界空间顶点坐标是相关的（我们定义纹理坐标就是逐个顶点定义的），然后世界空间顶点坐标（为了便于讨论，这里世界空间就是视图空间）通过投影矩阵变成屏幕空间顶点坐标。在世界空间中，顶点的$x$和$y$值与$z$值呈线性关系（因为我们定义基本图元是三角形，在三角形平面上，必然是线性的，否则就是非线性的曲面了），即存在$A$和$B$有： x_e = Az_e+B\\\\ y_e = Az_e+B \\tag {2}&emsp;&emsp;$(x_e,y_e,z_e)$是视图空间的顶点坐标，即$(x’,y’)$是投影到近平面的顶点坐标。根据透视投影矩阵可知（其实就是相似三角形），$(x’,y’)$与视图空间的顶点坐标关系如下： \\begin{cases} x'=-N\\frac {x_e}{z_e}\\ \\to x_e= -\\frac{x'z_e}{N} \\\\ y'=-N\\frac {y_e}{z_e}\\ \\to y_e= -\\frac{y'z_e}{N} \\end{cases} \\tag {3}&emsp;&emsp;将公式$(3)$带入公式$(2)$，则有： \\begin{cases} x'=-N\\frac{B}{z_e}-AN\\\\ y'=-N\\frac{B}{z_e}-AN \\end{cases} \\tag {4}&emsp;&emsp;其中的$A$、$B$、$N$都是常量，把$\\frac 1{z_e}$看成一个整体，则通过透视投影矩阵的变换之后$x’$、$y’$均与$\\frac{1}{z_e}$成线性关系，这也就是透视投影的效果是近大远小的根本原因。然后注意到在三维空间中，纹理坐标$(s,t)$和$(x_e,y_e)$成线性关系。即有（这里只是定性分析，$A$和$B$具体多少我们不用关心）： \\begin{cases} x_e=As+B\\\\ x_e=At+B\\\\ y_e=As+B\\\\ y_e=At+B \\end{cases} \\tag {5}&emsp;&emsp;把公式$(5)$带入$(3)$则有（以公式$(5)$的第一个为例，其他类似）： As+B=-\\frac{x'z_e}{N}\\ \\to\\ A\\frac{s}{z_e}+B\\frac{1}{z_e}=-\\frac{x'}{N} \\tag {6}&emsp;&emsp;公式$(6)$彻底说明了纹理坐标与屏幕空间的顶点坐标的关系！$s$和$x’$并不是简单的线性关系，因为还出现了$\\frac{1}{z_e}$这个项，如果$\\frac{1}{z_e}$具体值已知，那么$\\frac{s}{z_e}$就与 $x’$成线性关系！那么我们在线性插值之前给纹理坐标$s$乘上一个$\\frac{1}{z_e}$，就可以根据屏幕空间的顶点坐标做线性插值了，然后对插值得到的纹理坐标$s’$乘上$z_e$就能还原出正确的纹理坐标！！！！ &emsp;&emsp;说了这么多都是在捋清函数关系，实现其实很简单的，上面已经说的很清楚了。我们在$VertexOut$中定义的变量$oneDivZ$就用于的透射投影映射的。除开纹理坐标，其他的世界空间坐标、顶点颜色、法线都是定义在世界空间的坐标顶点上的，为了得到正确的插值，都需要做与纹理坐标一样的处理。乘上$\\frac{1}{z_e}$这一步我放在了顶点着色器的最后一步，只要放在插值之前都行。 1234567891011VertexOut SimpleShader::vertexShader(const Vertex &amp;in)&#123; ..... // oneDivZ to correct mapping. result.oneDivZ = 1.0 / result.posH.w; result.posTrans *= result.oneDivZ; result.texcoord *= result.oneDivZ; result.color *= result.oneDivZ; return result;&#125; &emsp;&emsp;然后再光栅化插值之后各自乘上相应的倒数即可恢复出正确的插值结果。 123456789101112131415161718192021222324252627void Pipeline::scanLinePerRow(const VertexOut &amp;left, const VertexOut &amp;right)&#123; // scan the line from left to right. VertexOut current; int length = right.posH.x - left.posH.x + 1; for(int i = 0;i &lt;= length;++i) &#123; // linear interpolation double weight = static_cast&lt;double&gt;(i)/length; current = lerp(left, right, weight); current.posH.x = left.posH.x + i; current.posH.y = left.posH.y; // depth testing. ...... // restore. double w = 1.0/current.oneDivZ; current.posTrans *= w; current.color *= w; current.texcoord *= w; // fragment shader m_backBuffer-&gt;drawPixel(current.posH.x, current.posH.y, m_shader-&gt;fragmentShader(current)); &#125;&#125; 2、双线性纹理采样&emsp;&emsp;定义的纹理坐标都是$[0.0f,1.0f]$的浮点数，为了采样纹理我们需要把它乘上纹理的宽高转成整数的下标取访问纹理的像素矩阵。乘上纹理的宽高之后我们得到的依然应该是一个浮点数，为了获取像素下标，一个简单的方法就是向下取整（这种采样方法对应于OpenGL的GL_NEAREST纹理过滤方法）。如下所示： 12345678910double trueU = texcoord.x * (m_width - 1);double trueV = texcoord.y * (m_height - 1);x = static_cast&lt;unsigned int&gt;(trueU);y = static_cast&lt;unsigned int&gt;(trueV);int index[0] = (x * m_width + y) * m_channel;Vector3D texels;// INV_SCALE is 1.0/255texels.x = static_cast&lt;float&gt;(m_pixelBuffer[index + 0]) * INV_SCALE;texels.y = static_cast&lt;float&gt;(m_pixelBuffer[index + 1]) * INV_SCALE;texels.z = static_cast&lt;float&gt;(m_pixelBuffer[index + 2]) * INV_SCALE; &emsp;&emsp;问题就出在这里，这样直接抛弃小数点以后的值导致采样出的相邻纹理并不连续，那么用float采样行吗？答案是：不行！这边实现的采样函数是从数组取值，纹理坐标转为数组下标，数组下标不能用float只能用int，那么就没办法了吗？并不是，可以对周围纹理进行采样然后按照各自比例进行混合，这样能够提高显示效果。混合的方法就是双线性插值。所谓双线性插值，就是先后线性插值一次，共两次。即横向线性插值一次，然后根据前面一次的插值结果竖向插值一次，二维纹理是有两个维度，所以做双线性插值。 &emsp;&emsp;除了采样之外，还有一个纹理坐标溢出的问题。纹理坐标超过的$[0,1]$通常由两种处理方式，一种是$clamp$，超过$[0,1]$的地方的像素都获取边上的像素，这样效果就是拉伸。一种是$repeat$，故名思议，即重复平铺。这里我实现的是重复平铺，在计算真正的纹理下标之前做相应的判断和处理即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108class Texture2D&#123;private: int m_width; int m_height; int m_channel; unsigned char *m_pixelBuffer;public: Texture2D():m_width(0), m_height(0), m_channel(0), m_pixelBuffer(nullptr)&#123;&#125; ~Texture2D(); bool loadImage(const std::string &amp;path); Vector4D sample(const Vector2D &amp;texcoord) const;&#125;;bool Texture2D::loadImage(const std::string &amp;path)&#123; if(m_pixelBuffer)delete m_pixelBuffer; m_pixelBuffer = nullptr; m_pixelBuffer = stbi_load(path.c_str(), &amp;m_width, &amp;m_height, &amp;m_channel, 0); if(m_pixelBuffer == nullptr) &#123; qDebug() &lt;&lt; \"Failed to load image-&gt;\" &lt;&lt; QString::fromStdString(path); &#125; return m_pixelBuffer != nullptr;&#125;Vector4D Texture2D::sample(const Vector2D &amp;texcoord) const&#123; // just for rgb and rgba format. Vector4D result(0.0,0.0,0.0,1.0); if(m_pixelBuffer == nullptr) return result; unsigned int x = 0, y = 0; // for bilinear interpolation. double factorU = 0, factorV = 0; // calculate the corresponding coordinate. if(texcoord.x &gt;= 0.0f &amp;&amp; texcoord.x &lt;= 1.0f &amp;&amp; texcoord.y &gt;= 0.0f &amp;&amp; texcoord.y &lt;= 1.0f) &#123; double trueU = texcoord.x * (m_width - 1); double trueV = texcoord.y * (m_height - 1); x = static_cast&lt;unsigned int&gt;(trueU); y = static_cast&lt;unsigned int&gt;(trueV); factorU = trueU - x; factorV = trueV - y; &#125; else &#123; // repeating way. float u = texcoord.x,v = texcoord.y; if(texcoord.x &gt; 1.0f) u = texcoord.x - static_cast&lt;int&gt;(texcoord.x); else if(texcoord.x &lt; 0.0f) u = 1.0f - (static_cast&lt;int&gt;(texcoord.x) - texcoord.x); if(texcoord.y &gt; 1.0f) v = texcoord.y - static_cast&lt;int&gt;(texcoord.y); else if(texcoord.y &lt; 0.0f) v = 1.0f - (static_cast&lt;int&gt;(texcoord.y) - texcoord.y); double trueU = u * (m_width - 1); double trueV = v * (m_height - 1); x = static_cast&lt;unsigned int&gt;(trueU); y = static_cast&lt;unsigned int&gt;(trueV); factorU = trueU - x; factorV = trueV - y; &#125; // texel fetching. Vector3D texels[4]; int index[4]; index[0] = (x * m_width + y) * m_channel; index[1] = (x * m_width + y + 1) * m_channel; index[2] = ((x + 1) * m_width + y + 1) * m_channel; index[3] = ((x + 1) * m_width + y) * m_channel; // left bottom texels[0].x = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 0]) * INV_SCALE; texels[0].y = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 1]) * INV_SCALE; texels[0].z = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 2]) * INV_SCALE; //return texels[0]; // left top texels[1].x = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 0]) * INV_SCALE; texels[1].y = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 1]) * INV_SCALE; texels[1].z = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 2]) * INV_SCALE; // right top texels[2].x = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 0]) * INV_SCALE; texels[2].y = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 1]) * INV_SCALE; texels[2].z = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 2]) * INV_SCALE; // right bottom texels[3].x = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 0]) * INV_SCALE; texels[3].y = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 1]) * INV_SCALE; texels[3].z = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 2]) * INV_SCALE; // bilinear interpolation. // horizational texels[0] = texels[0] * (1.0 - factorU) + texels[3] * factorU; texels[1] = texels[1] * (1.0 - factorU) + texels[2] * factorU; //vertical result = texels[0] * (1.0 - factorV) + texels[1] *factorV; return result;&#125; &emsp;&emsp;加载图片我的用的stb_image，一个简单使用的头文件，因为加载图片不是我们的重点，所以就不造这方面的轮子了。 三、程序结果&emsp;&emsp;目前的帧率还不错hhh。 参考资料$[1]$ https://learnopengl.com/Advanced-OpenGL/Depth-testing $[2]$ https://www.cnblogs.com/pbblog/p/3484193.html $[3]$ https://learnopengl.com/Getting-started/Coordinate-Systems $[4]$ http://www.songho.ca/opengl/gl_projectionmatrix.html $[5]$ https://blog.csdn.net/popy007/article/details/5570803 $[6]$ https://learnopengl-cn.github.io/01%20Getting%20started/06%20Textures/","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/tags/Soft-Renderer/"},{"name":"3D pipeline","slug":"3D-pipeline","permalink":"http://yoursite.com/tags/3D-pipeline/"}]},{"title":"软渲染器Soft Renderer：光栅化篇","slug":"SoftRenderer-Rasterization","date":"2019-05-01T02:37:54.047Z","updated":"2019-05-23T12:43:31.730Z","comments":true,"path":"2019/05/01/SoftRenderer-Rasterization/","link":"","permalink":"http://yoursite.com/2019/05/01/SoftRenderer-Rasterization/","excerpt":"本章开始构建基于Qt平台软渲染器的初步框架，当然Qt相关的内容并不是软渲染器的重点，我只是借助Qt平台将渲染出来的像素矩阵用Qt的控件显示出来。光栅化是当今图形学渲染的一种方式，与之对应的是光线追踪渲染方式，本章我根据自己的理解着重讲述线框光栅化的Bresenham画线算法以及三角形填充光栅化的Edge-Walking算法。注意：初学者慎入。本篇相关的完整代码请看这里。","text":"本章开始构建基于Qt平台软渲染器的初步框架，当然Qt相关的内容并不是软渲染器的重点，我只是借助Qt平台将渲染出来的像素矩阵用Qt的控件显示出来。光栅化是当今图形学渲染的一种方式，与之对应的是光线追踪渲染方式，本章我根据自己的理解着重讲述线框光栅化的Bresenham画线算法以及三角形填充光栅化的Edge-Walking算法。注意：初学者慎入。本篇相关的完整代码请看这里。 渲染管线框架 光栅化算法 一、渲染管线框架&emsp;&emsp;渲染管线的搭建主要包含像素显示、网格数据封装、渲染循环、帧率fps计算、帧缓冲、着色器、渲染逻辑、光栅化等等，其中光栅化作为重点对象抽出来放在后面。当然我们不会一下子就完成渲染管线的基本功能，我们现在是要搭建一个框架，大部分的内容不用写入或者仅仅是做简单的处理，这样后面完善软渲染器的时候只需在相应的位置填写相应的代码逻辑即可。本章目标就是搭建一个渲染管线，用光栅化算法画三角形。当然，如果仅仅是画一个三角形，当然不用这么麻烦，但是我的目标是实现三维的软渲染器，深入理解三维渲染的整个流程，得从基础一步一步慢慢来。 1、像素显示的画布&emsp;&emsp;渲染器最终渲染出来的是一个像素矩阵，我们要把这个像素矩阵显示出来。显示的方法有很多，因人而异，这里我采用自己最熟悉的$Qt$来实现。显示的窗口继承一个普通的$QWidget$父类，然后我们通过重写它的$paintEvent$函数，将渲染出来的像素画到$QWidget$上。但是采用$QPainter$直接画上去的方式效率非常低，我通过查询资料得知，若想要快速地绘制给定的像素矩阵，可以利用$QImage$来实现。话不多说，上代码： 123456789101112131415class Window : public QWidget&#123; Q_OBJECTpublic: explicit Window(QWidget *parent = nullptr); ~Window();private: void paintEvent(QPaintEvent *) override;private: Ui::Window *ui; QImage *canvas;&#125;; &emsp;&emsp;接收到一帧的像素之后，在重绘事件里面利用$QImage$绘制给定的像素数组（记得调用$update$触发重绘事件）。由于篇幅原因，我不会讲太多细节方面的东西，代码也不会全部放出来，那样没意义。想看完整源代码的朋友直接去本人的github上看。 12345678910111213141516void Window::receiveFrame(unsigned char *image)&#123; if(canvas) delete canvas; canvas = new QImage(image, width(), height(), QImage::Format_RGBA8888); update();&#125;void Window::paintEvent(QPaintEvent *event)&#123; if(canvas) &#123; QPainter painter(this); painter.drawImage(0, 0, *canvas); &#125; QWidget::paintEvent(event);&#125; 2、帧缓冲类&emsp;&emsp;帧缓冲通常包含基本的颜色缓冲附件、深度缓冲附件等，这里我们暂且只实现颜色缓冲附件（四通道，格式为$RGBA$，各占一个字节），深度缓冲附件后面再加上。渲染管线最终的渲染结果是写入帧缓冲的，我们采用一个一维的单字节数组作为帧缓冲的颜色缓冲。帧缓冲的最基本的功能就是清楚缓冲区、写入像素： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class FrameBuffer&#123;private: int m_width, m_height, m_channel; std::vector&lt;unsigned char&gt; m_colorBuffer;public: FrameBuffer(int width, int height); ~FrameBuffer() = default; int getWidth()&#123;return m_width;&#125; int getHeight()&#123;return m_height;&#125; unsigned char *getColorBuffer() &#123;return m_colorBuffer.data();&#125; void clearColorBuffer(const Vector4D &amp;color); void drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color);&#125;;FrameBuffer::FrameBuffer(int width, int height) :m_channel(4), m_width(width), m_height(height)&#123; m_colorBuffer.resize(m_width*m_height*m_channel, 255);&#125;void FrameBuffer::clearColorBuffer(const Vector4D &amp;color)&#123; // fill the color buffer. unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); for(int row = 0;row &lt; m_height;++ row) &#123; for(int col = 0;col &lt; m_width;++ col) &#123; m_colorBuffer[row*m_width*m_channel+col*m_channel + 0] = red; m_colorBuffer[row*m_width*m_channel+col*m_channel + 1] = green; m_colorBuffer[row*m_width*m_channel+col*m_channel + 2] = blue; m_colorBuffer[row*m_width*m_channel+col*m_channel + 3] = alpha; &#125; &#125;&#125;void FrameBuffer::drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color)&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return; unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); unsigned int index = y*m_width*m_channel + x*m_channel; m_colorBuffer[index + 0] = red; m_colorBuffer[index + 1] = green; m_colorBuffer[index + 2] = blue; m_colorBuffer[index + 3] = alpha;&#125; 3、网格顶点数据&emsp;&emsp;三维的渲染程序中的顶点数据通常包含顶点位置、顶点颜色、纹理坐标、顶点法线，然后在此基础上利用一组给定顺序的顶点数据表示一个网格，渲染时网格的数据将被送入管线进行处理。为此，有必要对顶点数据做一定的封装。 1234567891011121314class Vertex&#123;public: Vector4D position; Vector4D color; Vector2D texcoord; Vector3D normal; Vertex() = default; Vertex(Vector4D _pos, Vector4D _color, Vector2D _tex, Vector3D _normal) :position(_pos),color(_color),texcoord(_tex),normal(_normal) &#123;&#125; Vertex(const Vertex &amp;rhs) :position(rhs.position),color(rhs.color),texcoord(rhs.texcoord),normal(rhs.normal)&#123;&#125;&#125;; &emsp;&emsp;顶点数据经过顶点着色器的处理之后，会被送到下一个渲染管线的阶段处理。顶点着色器的顶点数据输出与输入有些差异，为此我们也定义一个类表示为顶点着色器的输出，这对于构建渲染管线尤为重要。 12345678910111213141516171819class VertexOut&#123;public: Vector4D posTrans; //世界变换后的坐标 Vector4D posH; //投影变换后的坐标 Vector2D texcoord; //纹理坐标 Vector3D normal; //法线 Vector4D color; //颜色 double oneDivZ; //1/z用于深度测试 VertexOut() = default; VertexOut(Vector4D _posT, Vector4D _posH, Vector2D _tex, Vector3D _normal, Vector4D _color, double _oneDivZ) :posTrans(_posT),posH(_posH),texcoord(_tex), normal(_normal),color(_color),oneDivZ(_oneDivZ) &#123;&#125; VertexOut(const VertexOut&amp; rhs) :posTrans(rhs.posTrans), posH(rhs.posH), texcoord(rhs.texcoord), normal(rhs.normal), color(rhs.color), oneDivZ(rhs.oneDivZ) &#123;&#125;&#125;; &emsp;&emsp;然后就是关于网格的表示，为了节省空间（特别是对于很大的模型），我们直接采用索引来组织网格。若想详细了解OpenGL的顶点索引概念请看这里。一个网格有两个数组，分别是$Vertex$数组和$Index$数组。下面的代码中，有一个$asTriangle$方法，这是一个三角形网格，调用这个方法之后网格存储的就是一个三角形，用于后面的光栅化调试，光栅化的基本单元就是三角形。通常情况，所有的网格模型都可以用一定数量的三角形构成，因而我们实现的软渲染器的基本图元就是三角形。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Mesh&#123;public: std::vector&lt;Vertex&gt; vertices; std::vector&lt;unsigned int&gt; indices; Mesh() = default; ~Mesh() = default; Mesh(const Mesh&amp; mesh) :vertices(mesh.vertices), indices(mesh.indices)&#123;&#125; Mesh&amp; operator=(const Mesh&amp; mesh) &#123; if (&amp;mesh == this) return *this; vertices = mesh.vertices; indices = mesh.indices; return *this; &#125; void setVertices(Vertex* _vs, int count) &#123; vertices.resize(count); new(&amp;vertices[0])std::vector&lt;Vertex&gt;(_vs, _vs + count); &#125; void setIndices(int* _es, int count) &#123; indices.resize(count); new(&amp;indices)std::vector&lt;unsigned int&gt;(_es, _es + count); &#125; void asBox(double width, double height, double depth); void asTriangle(const Vector3D p1, const Vector3D p2, const Vector3D p3);&#125;;void Mesh::asTriangle(Vector3D p1, Vector3D p2, Vector3D p3)&#123; vertices.resize(3); indices.resize(3); vertices[0].position = p1; vertices[0].normal = Vector3D(0.f, 0.f, 1.f); vertices[0].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[0].texcoord = Vector2D(0.f, 0.f); vertices[1].position = p2; vertices[1].normal = Vector3D(0.f, 0.f, 1.f); vertices[1].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[1].texcoord = Vector2D(1.f, 0.f); vertices[2].position = p3; vertices[2].normal = Vector3D(0.f, 0.f, 1.f); vertices[2].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[2].texcoord = Vector2D(0.5f, 1.f); indices[0] = 0; indices[1] = 1; indices[2] = 2;&#125; 4、简单的着色器&emsp;&emsp;着色器方面时软渲染中较为高级的内容，目前我们只是搭建一个框架，因而着色器不需要什么复杂的操作，只需简单地传递数据就行了。博主实现的软渲染器只包含必不可少的顶点着色器和片元着色器，目前的顶点着色器将顶点原封不动地输出，片元着色器也是如此，这样我们后面要实现光照效果的时候直接在着色器里写上就行了。为了更加有条理，我们设计一个着色器的虚类，这样实现不同效果的着色器时我们直接继承这个虚类即可。 123456789101112class BaseShader&#123;public: BaseShader() = default; virtual ~BaseShader() = default; virtual VertexOut vertexShader(const Vertex &amp;in) = 0; virtual Vector4D fragmentShader(const VertexOut &amp;in) = 0; virtual void setModelMatrix(const Matrix4x4 &amp;world) = 0; virtual void setViewMatrix(const Matrix4x4 &amp;view) = 0; virtual void setProjectMatrix(const Matrix4x4 &amp;project) = 0;&#125;; 12345678910111213141516171819202122232425262728293031323334353637383940414243class SimpleShader : public BaseShader&#123;public: SimpleShader() = default; virtual ~SimpleShader() = default; virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); virtual void setModelMatrix(const Matrix4x4 &amp;world); virtual void setViewMatrix(const Matrix4x4 &amp;view); virtual void setProjectMatrix(const Matrix4x4 &amp;project);&#125;;VertexOut SimpleShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = in.position; result.posH = in.position; result.color = in.color; result.normal = in.normal; result.oneDivZ = 1.0; result.texcoord = in.texcoord; return result;&#125;Vector4D SimpleShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor; litColor = in.color; return litColor;&#125;void SimpleShader::setModelMatrix(const Matrix4x4 &amp;world)&#123;&#125;void SimpleShader::setViewMatrix(const Matrix4x4 &amp;view)&#123;&#125;void SimpleShader::setProjectMatrix(const Matrix4x4 &amp;project)&#123;&#125; &emsp;&emsp;可以看到$SimpleShader$仅仅是将顶点数据直接输出，不进行任何处理。 5、搭建基本的渲染管线&emsp;&emsp;目前我们已经有了一些渲染管线的基本组件，现在就需要把这些组件串起来。首先是渲染循环的问题，$Qt$有它自己的事件循环，而且主线程的事件循环要尽量避免大量的运算（否则UI控件会陷入未响应），因此将渲染循环放到子线程里是一个不错的渲染，这样也可以避免我们的软渲染逻辑与$Qt$的接口耦合得太高。 渲染线程&emsp;&emsp;$Qt$提供了$QThread$类构建线程，我采用的方式为：渲染循环类继承$QObject$，然后调用$moveToThread$番方法挂到子线程上运行，最后将线程的启动信号与$loop$渲染循环关联即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class RenderLoop : public QObject&#123; Q_OBJECTpublic: explicit RenderLoop(int w, int h, QObject *parent = nullptr); ~RenderLoop(); void stopIt() &#123;stoped = true;&#125; void setFpsZero()&#123;fps = 0;&#125; int getFps()&#123;return fps;&#125;signals: void frameOut(unsigned char *image);public slots: void loop();private: bool stoped; int fps; int width, height, channel;&#125;;RenderLoop::RenderLoop(int w, int h, QObject *parent) : QObject(parent), width(w), height(h), channel(4)&#123; fps = 0; stoped = false;&#125;RenderLoop::~RenderLoop()&#123;&#125;void RenderLoop::loop()&#123; // pipeline initialization ...... // fps counting. fps = 0; while(!stoped) &#123; // render logic ...... ++ fps; &#125;&#125; &emsp;&emsp;然后在主窗口中创建$RenderLoop$对象，挂到$QThread$上启动。此外还有一点要注意的是在子线程中最好不用使用$QTimer$类，因此我在主窗口中创建$QTimer$类，设定为每秒触发，触发时主线程读取子线程的$fps$，这样就达到了显示帧率的目的了。 12345678910111213141516171819202122232425262728293031在Window类声明处：private: QTimer *timer; QThread *loopThread; RenderLoop *loop;在Window类构造函数处： loop = new RenderLoop(width(), height(), nullptr); loopThread = new QThread(this); // fps counting. timer = new QTimer(); connect(timer,&amp;QTimer::timeout,this,&amp;Window::fpsTimeOut); // render thread. loop-&gt;moveToThread(loopThread); connect(loopThread,&amp;QThread::finished,loop, &amp;RenderLoop::deleteLater); connect(loopThread,&amp;QThread::started,loop,&amp;RenderLoop::loop); connect(loop,&amp;RenderLoop::frameOut,this,&amp;Window::receiveFrame); // begin the thread. loopThread-&gt;start(); timer-&gt;start(1000);Window的其他函数：void Window::fpsTimeOut()&#123; int fps = loop-&gt;getFps(); loop-&gt;setFpsZero(); this-&gt;setWindowTitle(QString(\" fps: %1\").arg(fps));&#125; 渲染流程&emsp;&emsp;回顾一下$OpenGL$的渲染流程（这里只考虑一般的情况，即不包含几何着色器、细分着色器等），首先外部处理网格，将网格顶点数据和网格顶点索引送入渲染管线，设置基本图元（如三角形）、渲染方式（如线框模式）。渲染管线的第一阶段为顶点着色器阶段（在这之前还有个缓冲清理阶段），顶点着色器对网格数据逐顶点处理（包含坐标空间变换、投影变换等等），随之输出。然后渲染管线对输出的顶点数据进行裁剪，送入光栅化部件，计算几何图元覆盖的像素点，其中进行了大量的线性插值操作。接着片元着色器获取光栅化后的像素，对每个像素做颜色计算等，然后输出颜色数据、深度数据，最后根据这些缓冲数据做深度测试。 &emsp;&emsp;所以一个最基本的渲染管线应该有如下几个步骤： &emsp;&emsp;初始化（如缓冲区创建）$\\to$输入顶点缓冲、索引缓冲$\\to$清除缓冲区$\\to$设置着色器、渲染方式$\\to$绘制$\\to$交换双缓冲$\\to$输出。根据这些步骤，创建$Pipeline$类如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192class Pipeline&#123;private: int m_width, m_height; // width and height of viewport. BaseShader *m_shader; // shaders including vertex shader and fragment shader. FrameBuffer *m_frontBuffer; FrameBuffer *m_backBuffer; Matrix4x4 viewPortMatrix; // viewport transformation matrix. std::vector&lt;Vertex&gt; m_vertices; // vertex buffer. std::vector&lt;unsigned int&gt; m_indices;// index buffer.public: Pipeline(int width, int height); ~Pipeline(); void initialize(); void clearBuffer(const Vector4D &amp;color, bool depth = false); void setVertexBuffer(const std::vector&lt;Vertex&gt; &amp;vertices)&#123;m_vertices = vertices;&#125; void setIndexBuffer(const std::vector&lt;unsigned int&gt; &amp;indices)&#123;m_indices = indices;&#125; void setShaderMode(ShadingMode mode); void drawIndex(RenderMode mode); void swapBuffer(); unsigned char *output()&#123;return m_frontBuffer-&gt;getColorBuffer();&#125;&#125;;Pipeline::Pipeline(int width, int height) :m_width(width),m_height(height) ,m_shader(nullptr),m_frontBuffer(nullptr) ,m_backBuffer(nullptr)&#123;&#125;Pipeline::~Pipeline()&#123; if(m_shader)delete m_shader; if(m_frontBuffer)delete m_frontBuffer; if(m_backBuffer)delete m_backBuffer; m_shader = nullptr; m_frontBuffer = nullptr; m_backBuffer = nullptr;&#125;void Pipeline::initialize()&#123; if(m_frontBuffer) delete m_frontBuffer; if(m_backBuffer) delete m_backBuffer; if(m_shader) delete m_shader; viewPortMatrix.setViewPort(0,0,m_width,m_height); m_frontBuffer = new FrameBuffer(m_width, m_height); m_backBuffer = new FrameBuffer(m_width, m_height); m_shader = new SimpleShader();&#125;void Pipeline::drawIndex(RenderMode mode)&#123; 输入顶点着色器; 光栅化; 输入片元着色器; 写入缓冲区;&#125;void Pipeline::clearBuffer(const Vector4D &amp;color, bool depth)&#123; (void)depth; m_backBuffer-&gt;clearColorBuffer(color);&#125;void Pipeline::setShaderMode(ShadingMode mode)&#123; if(m_shader)delete m_shader; if(mode == ShadingMode::simple) m_shader = new SimpleShader(); else if(mode == ShadingMode::phong) ;&#125;void Pipeline::swapBuffer()&#123; FrameBuffer *tmp = m_frontBuffer; m_frontBuffer = m_backBuffer; m_backBuffer = tmp;&#125; &emsp;&emsp;注意到我创建了帧缓冲，分别是$m_frontBuffer$和$m_backBuffer$，前者存储着当前显示的像素，后者缓冲区用于写入像素。这就是著名的双缓冲原理，可以避免画面的闪烁、撕裂等现象。除此之外，还有一个值得特别说明的就是视口变换矩阵$viewPortMatrix$，这个一般很少见到，因为被内嵌在了渲染管线里面了。经过投影变换、透视除法操作之后，顶点数据都在标准化设备空间中，即$x$轴、$y$轴、$z$轴取值范围为$[-1,1]$。但是屏幕的像素坐标范围并非如此，通常屏幕的$x$轴坐标范围为$[0,width]$，$y$轴坐标范围为$[0,height]$，屏幕像素坐标原点在左上角，$x$轴正向朝右，$y$轴正向朝下，所以我们还要把标准化设备坐标顶点数据变换到屏幕的坐标范围中，这就是视口变换（$z$轴一般保持不变）。视口变换矩阵的构造并没有难度，因为这仅仅是简单的线性映射，因此不再赘述。视口变换矩阵如下所示： viewPortMatrix= \\left[ \\begin{matrix} \\frac{w}{2}&0&0&s_x+\\frac{w}{2}\\\\ 0&-\\frac{h}{2}&0&s_y+\\frac{h}{2}\\\\ 0&0&1&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {1}&emsp;&emsp;其中$(s_x,s_y)$是视口左上角的坐标，$(w,h)$为屏幕的宽度和高度。 12345678void Matrix4x4::setViewPort(int left, int top, int width, int height)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(width)/2.0f; entries[5] = -static_cast&lt;float&gt;(height)/2.0f; entries[12] = static_cast&lt;float&gt;(left)+static_cast&lt;float&gt;(width)/2.0f; entries[13] = static_cast&lt;float&gt;(top)+static_cast&lt;float&gt;(height)/2.0f;&#125; &emsp;&emsp;$Pipeline$还有个非常重要的函数$drawIndex$，它是渲染管线的核心部分，涉及到了图元装配、顶点着色器调度、光栅化、片元着色器调度、写入帧缓冲这几个重要的步骤。我们实现的软渲染器几何图元默认为三角形，所以图元装配就是每三个顶点装成一个图元。 123456789101112131415161718192021222324252627282930313233343536373839void Pipeline::drawIndex(RenderMode mode)&#123; if(m_indices.empty())return; for(unsigned int i = 0;i &lt; m_indices.size()/3;++ i) &#123; //! vertices assembly to triangle primitive Vertex p1,p2,p3; &#123; p1 = m_vertices[3*i+0]; p2 = m_vertices[3*i+1]; p3 = m_vertices[3*i+2]; &#125; //! vertex shader stage. VertexOut v1,v2,v3; &#123; v1 = m_shader-&gt;vertexShader(p1); v2 = m_shader-&gt;vertexShader(p2); v3 = m_shader-&gt;vertexShader(p3); &#125; //! rasterization and fragment shader stage. &#123; v1.posH = viewPortMatrix * v1.posH; v2.posH = viewPortMatrix * v2.posH; v3.posH = viewPortMatrix * v3.posH; if(mode == RenderMode::wire) &#123; // bresenham rasterization &#125; else if(mode == RenderMode::fill) &#123; // edge walking rasterization &#125; &#125; &#125;&#125; &emsp;&emsp;有了以上的$Pipeline$函数，我们的渲染循环逻辑的一般形式如下： 1234567891011while(!stoped)&#123; pipeline-&gt;clearBuffer(Vector4D(0.502f,0.698f,0.800f,1.0f)); pipeline-&gt;drawIndex(RenderMode::fill); pipeline-&gt;swapBuffer(); emit frameOut(pipeline-&gt;output()); ++ fps;&#125; 二、光栅化算法&emsp;&emsp;顶点着色器处理的还是一个个离散的几何顶点，在顶点着色器之后我们还需要进行光栅化操作，将几何覆盖的屏幕像素计算出来，送入片元着色器计算每个点的像素数据。光栅化一般有两种模式：一种是线框模式，即只描绘几何的边；二是填充模式，即将几何的面片全部填充完。Bresenham算法是经典的描线算法，它采用迭代的形式将所需的算术操作降低到最少。除此之外还有DDA描线算法，效率上不如Bresenham算法，所以我没有实现。 1、Bresenham描线算法&emsp;&emsp;我们要描绘的是从$(x_0,y_0)$到$(x_1,y_1)$的一条直线线段。一些数学符号标记如下： \\Delta x= x_1-x_0>0,\\ \\Delta y=y_1-y_0>0,\\ m=\\frac{\\Delta y}{\\Delta x}&emsp;&emsp;其中$m$即直线线段的斜率，为了便于讨论，我们假设$|m|\\leq 1$，其他情况很容易推广。 &emsp;&emsp;在如上的情况下，Bresenham算法从$x=x_0$开始，每次将$x$坐标值加一，然后推算相应的$y$坐标值。记第$i$次迭代获得的点为$(x_i,y_i)$。那么第$i+1$次迭代时获取的点就在$(\\overline x_i+1,\\overline y_i)$和$(\\overline x_i+1,\\overline y_i+1)$这两个中选取。那如何判断应该选哪个呢？即选择这两个点之一的判断标准是什么？直观上，我们应该选取距离的直线线段在该$y$轴上的交点最近的点，如下图1所示。 图1 判别标准 &emsp;&emsp;直线的一般表达式为$y=mx+B$，$m$为直线的斜率，那么$(x_{i+1},y_{i+1})$表示为如下（注意$y_{i+1}$表示的是直线在$x_{i+1}$上真正的$y$值）： x_{i+1}=x_i+1\\\\ y_{i+1}=mx_{i+1}+B=m(x_i+1)+B \\tag {2} 图2 交点到右边的点、右上的点的距离 &emsp;&emsp;故$d_{upper}$和$d_{lower}$的取值如下： d_{upper}=\\overline y_i+1-\\overline y_{i+1}=\\overline y_i+1-m\\overline x_{i+1}-B\\\\ d_{lower}=y_{i+1}-\\overline y_i=mx_{i+1}+B-\\overline y_i \\tag {3}&emsp;&emsp;显然，如果$d_{lower}-d_{upper}&gt;0$，则应该取右上方的点；如果$d_{lower}-d_{upper}0$的符号。 d_{lower}-d_{upper}=m(x_i+1)+B-\\overline y_i-(\\overline y_i+1-m(x_i+1)-B)\\\\ =2m(x_i+1)-2\\overline y_i+2B-1 \\tag {4}&emsp;&emsp;式$(4)$中的$m$是直线的斜率，因此将式$(4)$作为判断标准需要做非常昂贵的浮点数除法运算。为了消去除法，注意到$m=\\frac{\\Delta y}{\\Delta x}$，两边同时乘上$\\Delta x&gt;0$，正负符号不变。 p_i=\\Delta x\\cdot (d_{lower}-d_{upper}) =2\\Delta y\\cdot(x_i+1)-2\\Delta x\\cdot \\overline y_i+(2B-1)\\Delta x\\\\ =2\\Delta y\\cdot x_i-2\\Delta x\\cdot\\overline y_i+c\\\\ where \\ \\ c=(2B-1)\\Delta x+2\\Delta y \\tag {5}&emsp;&emsp;所以可以用$p_i$的符号作为选取的标准。但是，式$(5)$的计算能够进一步简化，考虑$p_i$和$p_{i+1}$（注意我们根据$p_i$的符号来选取$\\overline y_{i+1}$）： p_{i+1}-p_{i} = (2\\Delta y\\cdot x_{i+1}-2\\Delta x\\cdot\\overline y_{i+1}+c) - (2\\Delta y\\cdot x_i-2\\Delta x\\cdot\\overline y_i+c) \\\\= 2\\Delta y-2\\Delta x(\\overline y_{i+1}-\\overline y_i) \\tag {6}&emsp;&emsp;若$p_i\\leq 0$，那么选择右边的点，此时$\\overline y_{i+1}=\\overline y_i$，那么有： p_{i+1}=p_i+2\\Delta y \\tag {7}&emsp;&emsp;若$p_i&gt;0$，那么选择右上角的点，此时$\\overline y_{i+1}=\\overline y_i+1$，那么有： p_{i+1}=p_i+2\\Delta y-2\\Delta x \\tag {8}&emsp;&emsp;所以我们可以根据$p_i$的符号快速计算出$p_{i+1}$的符号，如此迭代下去： Bresenham Algorithm: $draw (x_0, y_0);$ Calculate $\\Delta x$,$\\Delta y$,$2\\Delta y$,$2\\Delta y-2\\Delta x$,$p_0=2\\Delta y-\\Delta x$; for $x$ from $x_0$ to $x_1$: &emsp;&emsp;if $p_i\\leq 0$ &emsp;&emsp;&emsp;&emsp;draw $(x_{i+1},\\overline y_{i+1})=(x_i+1,\\overline y_i)$ ; &emsp;&emsp;&emsp;&emsp;compute $p_{i+1}=p_i+2\\Delta y$; &emsp;&emsp;if $p_i &gt; 0$ &emsp;&emsp;&emsp;&emsp;draw $(x_{i+1},\\overline y_{i+1})=(x_i+1,\\overline y_i+1)$ ; &emsp;&emsp;&emsp;&emsp;compute $p_{i+1}=p_i+2\\Delta y-2\\Delta x$; &emsp;&emsp;$x += 1;$ &emsp;&emsp;上面我们讨论的都是$|m|1$的情况呢？其实这是对称的，这时把$x$看成$y$，把$y$看成$x$即可。另外，当$\\Delta x &lt;0$时，我们的$x$不是递增$1$，而是递减$1$，具体实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465void Pipeline::bresenhamLineRasterization(const VertexOut &amp;from, const VertexOut &amp;to)&#123; int dx = to.posH.x - from.posH.x; int dy = to.posH.y - from.posH.y; int stepX = 1, stepY = 1; // judge the sign if(dx &lt; 0) &#123; stepX = -1; dx = -dx; &#125; if(dy &lt; 0) &#123; stepY = -1; dy = -dy; &#125; int d2x = 2*dx, d2y = 2*dy; int d2y_minus_d2x = d2y - d2x; int sx = from.posH.x; int sy = from.posH.y; VertexOut tmp; // slope &lt; 1. if(dy &lt;= dx) &#123; int flag = d2y - dx; for(int i = 0;i &lt;= dx;++ i) &#123; // linear interpolation tmp = lerp(from, to, static_cast&lt;double&gt;(i)/dx); // fragment shader m_backBuffer-&gt;drawPixel(sx,sy,m_shader-&gt;fragmentShader(tmp)); sx += stepX; if(flag &lt;= 0) flag += d2y; else &#123; sy += stepY; flag += d2y_minus_d2x; &#125; &#125; &#125; // slope &gt; 1. else &#123; int flag = d2x - dy; for(int i = 0;i &lt;= dy;++ i) &#123; // linear interpolation tmp = lerp(from, to, static_cast&lt;double&gt;(i)/dy); // fragment shader m_backBuffer-&gt;drawPixel(sx,sy,m_shader-&gt;fragmentShader(tmp)); sy += stepY; if(flag &lt;= 0) flag += d2x; else &#123; sx += stepX; flag -= d2y_minus_d2x; &#125; &#125; &#125;&#125; 2、Edge-Walking三角形填充算法&emsp;&emsp;三角形光栅化填充对输入给定的三个三角形顶点，计算这个三角区域覆盖的所有像素。三角形填充的光栅化算法有很多种，这里仅实现了Edge-Walking算法，此外还有Edge-Equation算法。关于Edge-Walking算法的前世今生我不再赘述了，这个算法的思路比较简单，但是实现起来比较麻烦一点。 &emsp;&emsp;话不多少，直接上伪代码（懒得自己写了伪代码了）： &emsp;&emsp;大致的思想就是从上往下（或从下往上）扫描，获取每对$X_L$、$X_R$，然后在$[X_L,X_R]$范围内从左到右扫描。显然就是双重循环。一般，我们的三角形光栅化对象有如下四种情况： 图3 四类三角形 &emsp;&emsp;先来看平底三角形的情况，如下图4所示。显然，平底三角形很容易地实现从下往上扫面，竖直方向上仅需考虑左右两条边。当然这里有个问题，就是如何确定$X_L$和$X_R$？如果直接采用算法伪代码中的利用$dx/dy$迭代获取$X$值，因为$X$值是整数，而$dx/dy$是浮点数，当$dx/dy&lt;1$时，把$dx/dy$加到$X$上面计算机对整数类型坐标自动向下取整，结果相当于没加。（即便是浮点数类型，最终也要取整，因为屏幕空间的像素坐标必须是整数） 图4 平底三角形 &emsp;&emsp;一种解决方案就是线性插值，算法从下往上扫描时，$y-=1$，我们根据当前的$y$值来获取$x$值： X_L = (1.0f-\\frac{y1-y}{y1-y0})*x1+\\frac{y1-y}{y1-y0}*x0 \\\\ X_y = (1.0f-\\frac{y2-y}{y2-y0})*x2+\\frac{y2-y}{y2-y0}*x0&emsp;&emsp;平顶的三角形光栅化亦类似，不再赘述。那么除了平底和平顶的情况之外，我们该如何处理其余的情况？一个技巧就是将其他情况的三角形分割乘一个平底三角形、一个平顶三角形，如下图所示： 图5 三角形分割 &emsp;&emsp;这样我们通过调用平底三角形光栅化方法、平顶三角形光栅化方法即可实现一般情况的三角形光栅化： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114void Pipeline::scanLinePerRow(const VertexOut &amp;left, const VertexOut &amp;right)&#123; VertexOut current; int length = right.posH.x - left.posH.x + 1; for(int i = 0;i &lt;= length;++i) &#123; // linear interpolation double weight = static_cast&lt;double&gt;(i)/length; current = lerp(left, right, weight); current.posH.x = left.posH.x + i; current.posH.y = left.posH.y; // fragment shader m_backBuffer-&gt;drawPixel(current.posH.x, current.posH.y, m_shader-&gt;fragmentShader(current)); &#125;&#125;void Pipeline::rasterTopTriangle(VertexOut &amp;v1, VertexOut &amp;v2, VertexOut &amp;v3)&#123; VertexOut left = v2; VertexOut right = v3; VertexOut dest = v1; VertexOut tmp, newleft, newright; if(left.posH.x &gt; right.posH.x) &#123; tmp = left; left = right; right = tmp; &#125; int dy = left.posH.y - dest.posH.y + 1; for(int i = 0;i &lt; dy;++i) &#123; double weight = 0; if(dy != 0) weight = static_cast&lt;double&gt;(i)/dy; newleft = lerp(left, dest, weight); newright = lerp(right, dest, weight); newleft.posH.y = newright.posH.y = left.posH.y - i; scanLinePerRow(newleft, newright); &#125;&#125;void Pipeline::rasterBottomTriangle(VertexOut &amp;v1, VertexOut &amp;v2, VertexOut &amp;v3)&#123; VertexOut left = v1; VertexOut right = v2; VertexOut dest = v3; VertexOut tmp, newleft, newright; if(left.posH.x &gt; right.posH.x) &#123; tmp = left; left = right; right = tmp; &#125; int dy = dest.posH.y - left.posH.y + 1; for(int i = 0;i &lt; dy;++i) &#123; double weight = 0; if(dy != 0) weight = static_cast&lt;double&gt;(i)/dy; newleft = lerp(left, dest, weight); newright = lerp(right, dest, weight); newleft.posH.y = newright.posH.y = left.posH.y + i; scanLinePerRow(newleft, newright); &#125;&#125;void Pipeline::edgeWalkingFillRasterization(const VertexOut &amp;v1, const VertexOut &amp;v2, const VertexOut &amp;v3)&#123; // split the triangle into two part VertexOut tmp; VertexOut target[3] = &#123;v1, v2,v3&#125;; if(target[0].posH.y &gt; target[1].posH.y) &#123; tmp = target[0]; target[0] = target[1]; target[1] = tmp; &#125; if(target[0].posH.y &gt; target[2].posH.y) &#123; tmp = target[0]; target[0] = target[2]; target[2] = tmp; &#125; if(target[1].posH.y &gt; target[2].posH.y) &#123; tmp = target[1]; target[1] = target[2]; target[2] = tmp; &#125; // bottom triangle if(equal(target[0].posH.y,target[1].posH.y)) &#123; rasterBottomTriangle(target[0],target[1],target[2]); &#125; // top triangle else if(equal(target[1].posH.y,target[2].posH.y)) &#123; rasterTopTriangle(target[0], target[1], target[2]); &#125; // split it. else &#123; double weight = static_cast&lt;double&gt;(target[1].posH.y-target[0].posH.y)/(target[2].posH.y-target[0].posH.y); VertexOut newPoint = lerp(target[0],target[2],weight); newPoint.posH.y = target[1].posH.y; rasterTopTriangle(target[0], newPoint, target[1]); rasterBottomTriangle(newPoint,target[1],target[2]); &#125;&#125; 三、程序结果&emsp;&emsp;最终，不借用任何图形接口通过自己实现的光栅化算法画出了三角形： 参考资料$[1]$ https://blog.csdn.net/cppyin/article/details/6232453 $[2]$ https://blog.csdn.net/y1196645376/article/details/78937614 $[3]$ https://blog.csdn.net/y1196645376/article/details/78907914","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/tags/Soft-Renderer/"},{"name":"Rasterization","slug":"Rasterization","permalink":"http://yoursite.com/tags/Rasterization/"}]},{"title":"软渲染器Soft Renderer：3D数学篇","slug":"SoftRenderer-Math","date":"2019-05-01T02:37:49.425Z","updated":"2019-05-05T13:13:44.571Z","comments":true,"path":"2019/05/01/SoftRenderer-Math/","link":"","permalink":"http://yoursite.com/2019/05/01/SoftRenderer-Math/","excerpt":"本章开始博主将手动搭建一个渲染管线，深入理解3D渲染的整个流程。线性代数中的向量和矩阵是计算机图形学的常客，深入理解和掌握对于图形渲染有着非常重要的意义，本节主要是关于3D数学库的内容。","text":"本章开始博主将手动搭建一个渲染管线，深入理解3D渲染的整个流程。线性代数中的向量和矩阵是计算机图形学的常客，深入理解和掌握对于图形渲染有着非常重要的意义，本节主要是关于3D数学库的内容。 向量 矩阵 一、向量&emsp;&emsp;$n$维向量本质就是一个$n$元组，从几何意义上来说，向量是有大小和方向的有向线段。向量的大小就是向量的长度（模）向量有非负的长度，而向量的方向描述了空间中向量的指向。向量的相关内容高中就已涉及，因此不再赘述。若想要重新深入了解相关内容，可以查看这个地址。 &emsp;&emsp;图形渲染中通常使用的向量为$2$到$4$维，如下分别是$2$维、$3$维、$4$维向量类的常用方法，主要是运算操作符重载以及点乘、叉乘、模、标准化、线性插值等基本操作。向量的内容简单，没什么要特别说明的。 1、2D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Vector2D&#123;public: float x,y; // constructors Vector2D():x(0.0f), y(0.0f) &#123;&#125; Vector2D(float newX, float newY):x(newX), y(newY)&#123;&#125; Vector2D(const float * rhs):x(*rhs), y((*rhs)+1) &#123;&#125; Vector2D(const Vector2D &amp; rhs):x(rhs.x), y(rhs.y)&#123;&#125; ~Vector2D() = default; // setter,getter void set(float newX, float newY)&#123;x=newX;y=newY; &#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; // normalization void normalize(); Vector2D getNormalize()const; // length float getLength() const &#123; return static_cast&lt;float&gt;(sqrt(x*x + y*y));&#125; float getSquaredLength()const&#123;return static_cast&lt;float&gt;(x*x + y*y);&#125; // overloaded operators Vector2D operator+(const Vector2D &amp;rhs) const &#123;return Vector2D(x + rhs.x, y + rhs.y);&#125; Vector2D operator-(const Vector2D &amp;rhs) const &#123;return Vector2D(x - rhs.x, y - rhs.y);&#125; Vector2D operator*(const float rhs) const &#123;return Vector2D(x*rhs, y*rhs);&#125; Vector2D operator/(const float rhs) const &#123;return (rhs==0) ? Vector2D(0.0f, 0.0f) : Vector2D(x / rhs, y / rhs);&#125; bool operator==(const Vector2D &amp;rhs) const &#123;return (equal(x,rhs.x) &amp;&amp; equal(y,rhs.y));&#125; bool operator!=(const Vector2D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector2D &amp;rhs)&#123;x+=rhs.x; y+=rhs.y;&#125; void operator-=(const Vector2D &amp;rhs)&#123;x-=rhs.x; y-=rhs.y;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs, 0.0))&#123;x/=rhs;y/=rhs;&#125;&#125; Vector2D operator-() const &#123;return Vector2D(-x, -y);&#125; Vector2D operator+() const &#123;return *this;&#125; // interpolation Vector2D lerp(const Vector2D &amp;v2,const float factor)const &#123;return (*this)*(1.0f - factor) + v2*factor;&#125; Vector2D quadraticInterpolate(const Vector2D &amp; v2, const Vector2D &amp; v3, const float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor) + v2*2.0f*factor*(1.0f-factor) + v3*factor*factor;&#125;&#125;; 2、3D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Vector3D&#123;public: float x,y,z; // constructors Vector3D():x(0.0f), y(0.0f), z(0.0f)&#123;&#125; Vector3D(float newX, float newY, float newZ):x(newX), y(newY), z(newZ)&#123;&#125; Vector3D(const float * rhs):x(*rhs), y(*(rhs+1)), z(*(rhs+2))&#123;&#125; Vector3D(const Vector3D &amp;rhs):x(rhs.x), y(rhs.y), z(rhs.z)&#123;&#125; ~Vector3D() = default; // setter,getter void set(float newX, float newY, float newZ)&#123;x=newX;y=newY;z=newZ;&#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; void setZ(float newZ) &#123;z = newZ;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; float getZ() const &#123;return z;&#125; // normalization void normalize(); Vector3D getNormalized() const; // length caculation float getLength() const &#123;return static_cast&lt;float&gt;(sqrt(x*x+y*y+z*z));&#125; float getSquaredLength() const &#123;return x*x+y*y+z*z;&#125; // product float dotProduct(const Vector3D &amp;rhs) const &#123;return x*rhs.x + y*rhs.y + z*rhs.z;&#125; Vector3D crossProduct(const Vector3D &amp;rhs) const &#123;return Vector3D(y*rhs.z - z*rhs.y, z*rhs.x - x*rhs.z, x*rhs.y - y*rhs.x);&#125; // linear interpolation Vector3D lerp(const Vector3D &amp;v2, float factor) const &#123;return (*this)*(1.0f-factor) + v2*factor;&#125; Vector3D QuadraticInterpolate(const Vector3D &amp;v2, const Vector3D &amp;v3, float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor) + v2*2.0f*factor*(1.0f-factor) + v3*factor*factor;&#125; // overloaded operators Vector3D operator+(const Vector3D &amp;rhs) const &#123;return Vector3D(x + rhs.x, y + rhs.y, z + rhs.z);&#125; Vector3D operator-(const Vector3D &amp;rhs) const &#123;return Vector3D(x - rhs.x, y - rhs.y, z - rhs.z);&#125; Vector3D operator*(const float rhs) const &#123;return Vector3D(x*rhs, y*rhs, z*rhs);&#125; Vector3D operator/(const float rhs) const &#123;return (equal(rhs,0.0f))?Vector3D(0.0f, 0.0f, 0.0f):Vector3D(x/rhs, y/rhs, z/rhs);&#125; bool operator==(const Vector3D &amp;rhs) const &#123;return (equal(x,rhs.x) &amp;&amp; equal(y,rhs.y) &amp;&amp; equal(z,rhs.z));&#125; bool operator!=(const Vector3D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector3D &amp;rhs) &#123;x+=rhs.x;y+=rhs.y;z+=rhs.z;&#125; void operator-=(const Vector3D &amp; rhs) &#123;x-=rhs.x;y-=rhs.y;z-=rhs.z;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;z*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs,0.0f))&#123;x/=rhs; y/=rhs; z/=rhs;&#125;&#125; Vector3D operator-() const &#123;return Vector3D(-x, -y, -z);&#125; Vector3D operator+() const &#123;return *this;&#125;&#125;; 3、4D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Vector4D&#123;public: float x,y,z,w; // constructors Vector4D():x(0.0f), y(0.0f), z(0.0f), w(0.0f)&#123;&#125; Vector4D(float newX, float newY, float newZ, float newW):x(newX), y(newY), z(newZ), w(newW)&#123;&#125; Vector4D(const float * rhs):x(*rhs), y(*(rhs+1)), z(*(rhs+2)), w(*(rhs+3))&#123;&#125; Vector4D(const Vector4D &amp;rhs):x(rhs.x), y(rhs.y), z(rhs.z), w(rhs.w)&#123;&#125; Vector4D(const Vector3D &amp; rhs): x(rhs.x), y(rhs.y), z(rhs.z), w(1.0f)&#123;&#125; ~Vector4D() = default; // setter,getter void set(float newX, float newY, float newZ, float newW)&#123;x=newX;y=newY;z=newZ;w=newW;&#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; void setZ(float newZ) &#123;z = newZ;&#125; void setW(float newW) &#123;w = newW;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; float getZ() const &#123;return z;&#125; float getW() const &#123;return w;&#125; // product float dotProduct(const Vector4D &amp;rhs) const &#123;return x*rhs.x + y*rhs.y + z*rhs.z + w*rhs.w;&#125; // linear interpolation Vector4D lerp(const Vector4D &amp;v2, float factor) const &#123;return (*this)*(1.0f-factor) + v2*factor;&#125; Vector4D QuadraticInterpolate(const Vector4D &amp;v2, const Vector4D &amp;v3, float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor)+v2*2.0f*factor*(1.0f-factor)+v3*factor*factor;&#125; // overloaded operators Vector4D operator+(const Vector4D &amp;rhs) const &#123;return Vector4D(x+rhs.x, y+rhs.y, z+rhs.z, w+rhs.w);&#125; Vector4D operator-(const Vector4D &amp;rhs) const &#123;return Vector4D(x-rhs.x, y-rhs.y, z-rhs.z, w-rhs.w);&#125; Vector4D operator*(const float rhs) const &#123;return Vector4D(x*rhs, y*rhs, z*rhs, w*rhs);&#125; Vector4D operator/(const float rhs) const &#123;return (equal(rhs,0.0f))?Vector4D(0.0f, 0.0f, 0.0f, 0.0f):Vector4D(x/rhs, y/rhs, z/rhs, w/rhs);&#125; bool operator==(const Vector4D &amp;rhs) const &#123;return (equal(x,rhs.x)&amp;&amp;equal(y,rhs.y)&amp;&amp;equal(z,rhs.z)&amp;&amp;equal(w,rhs.w));&#125; bool operator!=(const Vector4D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector4D &amp;rhs) &#123;x+=rhs.x;y+=rhs.y;z+=rhs.z;w+=rhs.w;&#125; void operator-=(const Vector4D &amp; rhs) &#123;x-=rhs.x;y-=rhs.y;z-=rhs.z;w-=rhs.w;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;z*=rhs;w*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs,0.0f))&#123;x/=rhs; y/=rhs; z/=rhs; w/=rhs;&#125;&#125; Vector4D operator-() const &#123;return Vector4D(-x, -y, -z, -w);&#125; Vector4D operator+() const &#123;return *this;&#125;&#125;; 二、矩阵&emsp;&emsp;矩阵本质就是向量的进一步扩展的，一个$n\\times m$的矩阵可看成$n$个$m$维行向量组成或者$m$个$n$维列向量组成，关于矩阵的基本概念、操作请看这里。通常我们采用方阵来描述线性变换。所谓线性变换，即变换之后保留了直线而不被弯曲，平行线依然平行，原点没有变化，但其他的几何性质如长度、角度、面积和体积可能被变换改变了。直观来说，线性变换可能“拉伸”坐标系，但不会“弯曲”或“卷折”坐标系。 &emsp;&emsp;矩阵在计算机中有行主序存储、列主序存储两种方式，行主序存储即按照顺序逐行存储，列主序存储则按照顺序逐列存储。图形学渲染中我们通常采用的是列主序的方式，以下的讨论都是列主序的矩阵存储方式。那么矩阵是如何变换向量的？ &emsp;&emsp;向量在几何上能被解释成一系列与轴平行的位移，一般来说，任意向量$\\vec v$都能写成如下的形式： \\vec v=\\left[\\begin{matrix}x\\\\y\\\\z\\end{matrix}\\right]=\\left[\\begin{matrix}x\\\\0\\\\0\\end{matrix}\\right]+\\left[\\begin{matrix}0\\\\y\\\\0\\end{matrix}\\right]+\\left[\\begin{matrix}0\\\\0\\\\z\\end{matrix}\\right]=x\\left[\\begin{matrix}1\\\\0\\\\0\\end{matrix}\\right]+y\\left[\\begin{matrix}0\\\\1\\\\0\\end{matrix}\\right]+z\\left[\\begin{matrix}0\\\\0\\\\1\\end{matrix}\\right] \\tag {1}&emsp;&emsp;公式$(1)$右边的单位向量就是$x$、$y$、$z$轴方向的向量，向量的每个坐标都表明了平行于相应坐标轴的有向位移。我们记$\\vec p$、$\\vec q$、$\\vec r$分别为公式$(1)$中右边的$x$、$y$、$z$轴的单位列向量，则有： \\vec v=x\\vec p+y\\vec q+z\\vec r=\\left[\\begin{matrix}\\vec p &\\vec q&\\vec r\\end{matrix}\\right]\\left[\\begin{matrix}x \\\\y\\\\z\\end{matrix}\\right] \\tag {2}&emsp;&emsp;向量$\\vec v$就变成了向量$\\vec p$、$\\vec q$、$\\vec r$的线性表示，向量$\\vec p$、$\\vec q$、$\\vec r$称作基向量。以上仅仅讨论的是笛卡尔坐标系，但更通用的情况是，一个$3$维坐标系能用任意$3$个线性无关的基向量表示，以列向量$\\vec p$、$\\vec q$、$\\vec r$构建$3\\times 3$的矩阵$M$： M=\\left[\\begin{matrix}\\vec p &\\vec q&\\vec r\\end{matrix}\\right]=\\left[\\begin{matrix}p_x &q_x&r_x\\\\p_y &q_y&r_y\\\\p_z &q_z&r_z\\end{matrix}\\right] \\tag {3}&emsp;&emsp;结合公式$(2)$和公式$(3)$，即有： \\vec v=M\\left[\\begin{matrix}x \\\\y\\\\z\\end{matrix}\\right] \\tag{4}&emsp;&emsp;坐标系变换矩阵的每一列（如果是行主序，就是每一行）都是该坐标系的基向量，一个点$v$右乘该矩阵就相当于执行了一次坐标系转换。求解线性变换矩阵的关键就是根据当前的坐标系求解变换之后的坐标系的基向量，然后将基向量填入向量位置！ &emsp;&emsp;一个矩阵类通常有如下方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Matrix4x4&#123;public: float entries[16]; // constructors Matrix4x4()&#123;loadIdentity();&#125; Matrix4x4(float e0, float e1, float e2, float e3, float e4, float e5, float e6, float e7, float e8, float e9, float e10,float e11, float e12,float e13,float e14,float e15); Matrix4x4(const float *rhs); Matrix4x4(const Matrix4x4 &amp;rhs); ~Matrix4x4() = default; // setter,getter void setEntry(int position, float value); float getEntry(int position) const; Vector4D getRow(int position) const; Vector4D getColumn(int position) const; void loadIdentity(); void loadZero(); // overloaded operators Matrix4x4 operator+(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator-(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator*(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator*(const float rhs) const; Matrix4x4 operator/(const float rhs) const; bool operator==(const Matrix4x4 &amp; rhs) const; bool operator!=(const Matrix4x4 &amp; rhs) const; void operator+=(const Matrix4x4 &amp; rhs); void operator-=(const Matrix4x4 &amp; rhs); void operator*=(const Matrix4x4 &amp; rhs); void operator*=(const float rhs); void operator/=(const float rhs); Matrix4x4 operator-() const; Matrix4x4 operator+() const &#123;return (*this);&#125; Vector4D operator*(const Vector4D rhs) const; // inverse, transpose void inverted(); Matrix4x4 getInverse() const; void transpose(); Matrix4x4 getTranspose() const; void invertTranspose(); Matrix4x4 getInverseTranspose() const; // operation on space void setTranslation(const Vector3D &amp; translation); void setScale(const Vector3D &amp; scaleFactor); void setRotationAxis(const double angle, const Vector3D &amp; axis); void setRotationX(const double angle); void setRotationY(const double angle); void setRotationZ(const double angle); void setRotationEuler(const double angleX, const double angleY, const double angleZ); void setPerspective(float fovy, float aspect, float near, float far); void setPerspective(float left, float right, float bottom, float top, float near, float far); void setOrtho(float left, float right, float bottom, float top, float near, float far);&#125;; 1、线性变换、仿射变换&emsp;&emsp;满足$F(a+b)=F(a)+F(b)$和$F(ka)=kF(a)$的映射$F(a)$就是线性的。对于映射$F(a)=Ma$，当$M$为任意方阵时，也可以说明$F$映射是一个线性变换。在计算机图形学中，缩放、旋转的变换操作都是线性的，但是平移不是线性变换。 &emsp;&emsp;具有$v’=Mv’+b$形式的变换都是仿射变换。平移作为最常用的变换之一，然而却不是线性变换；所以为了包括平移变换提出了仿射变换。仿射变换是指线性变换后接着平移。因此，仿射变换的集合是线性变换的超集，任何线性变换都是仿射变换，但不是所有的仿射变换都是线性变换。为了统一用矩阵表示低维度的仿射变换，我们可以通过高维度的线性变换来完成，为此引入了$4$维齐次坐标。（当然引入第$4$维$w$还有其他的用途，如当$w=0$时，可解释为无穷远的“点”，其意义是描述方向），关于齐次坐标的更多内容请查看这里。 &emsp;&emsp;从而，对于高维度来说只是经历了一次切变+投影变换就可以实现低维度的平移（更多内容查看这里），在$3D$渲染中，我们采用$4\\times 4$的矩阵做相应的变换。关于平移和缩放不再赘述： 123456789101112131415void Matrix4x4::setTranslation(const Vector3D &amp;translation)&#123; loadIdentity(); entries[12] = translation.x; entries[13] = translation.y; entries[14] = translation.z;&#125;void Matrix4x4::setScale(const Vector3D &amp;scaleFactor)&#123; loadIdentity(); entries[0] = scaleFactor.x; entries[5] = scaleFactor.y; entries[10] = scaleFactor.z;&#125; 2、绕任意轴旋转&emsp;&emsp;在3D中，绕坐标轴旋转，而不是绕点旋转，此时首先需要定义的是何为旋转正方向： 左手坐标系中定义此方向的规则为左手法则。首先，要明确旋转轴指向哪个方向。当然，旋转轴在理论上是无限延伸的，但我们还是要认为它有正端点和负端点。与笛卡尔坐标轴定义坐标系相同，左手法则是这样的:伸出左手，大拇指向上，其余手指弯曲。大拇指指向旋转轴的正方向，此时，四指弯曲的方向就是旋转的正方向。右手坐标系则根据右手法则利用右手判断旋转正方向，本文讨论的是常见的右手坐标系。 &emsp;&emsp;在旋转变换中，一个常见的特殊情况就是绕$x$轴、绕$y$轴、绕$z$轴旋转，这类的旋转矩阵求解比较简单，只需牢牢记住列主序矩阵的列向量就是变换后的坐标系的基向量即可快速推导出相应的旋转矩阵： R_x(\\theta)=\\left[ \\begin{matrix} 1&0&0\\\\ 0&cos\\theta&-sin\\theta\\\\ 0&sin\\theta&cos\\theta \\end{matrix}\\right] \\\\ R_y(\\theta)=\\left[\\begin{matrix}cos\\theta&0&sin\\theta\\\\0&1&0\\\\-sin\\theta&0&cos\\theta \\end{matrix}\\right]\\\\ R_z(\\theta)=\\left[\\begin{matrix}cos\\theta&-sin\\theta&0\\\\ sin\\theta&cos\\theta&0\\\\0&0&1\\end{matrix}\\right] \\tag {5}1234567891011121314151617181920212223242526void Matrix4x4::setRotationX(const double angle)&#123; loadIdentity(); entries[5] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[6] = static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[9] = -entries[6]; entries[10] = entries[5];&#125;void Matrix4x4::setRotationY(const double angle)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[2] = -static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[8] = -entries[2]; entries[10] = entries[0];&#125;void Matrix4x4::setRotationZ(const double angle)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[1] = static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[4] = -entries[1]; entries[5] = entries[0];&#125; &emsp;&emsp;但是更一般的情况是绕任意轴进行旋转，构建这样的矩阵稍微有点麻烦，我们接下来就做一些绕任意轴旋转的矩阵构建推到。在这里我们不考虑平移，因而围绕旋转的轴一定是通过原点的。如下图1所示，将$\\vec v$旋转到$\\vec v ‘$，任意轴用单位向量$\\vec n$表示，绕$\\vec n$旋转$\\theta$角度的矩阵记为$R(\\vec n, \\theta)$，$\\vec v’$是向量绕轴$\\vec n$旋转后的向量，即$\\vec v’=R(\\vec n,\\theta)\\vec v$。 图1 绕任意轴旋转 &emsp;&emsp;我们的目标就是用$\\vec v$、$\\vec n$和$\\theta$来表示$\\vec v’$，从而构造出$R(\\vec n, \\theta)$。首先将$\\vec v$分解成平行于$\\vec n$的向量$\\vec v_{||}$和垂直于$\\vec n$的分量$\\vec v_{⊥}$，而$\\vec v’_{⊥}$是垂直于$\\vec n$的分向量。注意，$\\vec n$是单位向量，但$\\vec v$不是单位向量，可得$\\vec v$在$\\vec n$方向的投影向量$\\vec v_{||}$为： \\vec v_{||}=(\\vec v\\cdot\\vec n)\\vec n \\tag {6}&emsp;&emsp;从而根据$\\vec v_{||}$和$\\vec v$可知$\\vec v_{⊥}$和$w$，$w$是垂直于$\\vec n$和$\\vec v_{⊥}$的向量： \\vec v_{⊥}=\\vec v-\\vec v_{||} \\tag {7} w=\\vec n \\times \\vec v_{⊥} = \\vec n\\times (\\vec v-\\vec v_{||})\\\\ =\\vec n\\times\\vec v-\\vec n\\times\\vec v_{||}=\\vec n\\times\\vec v-0=\\vec n\\times \\vec v \\tag{8}&emsp;&emsp;$\\vec w$和$\\vec v_{⊥}$相互垂直，$\\vec w$、$\\vec v_{⊥}$和$\\vec v’_{⊥}$在同一个平面上，$\\vec v’_{⊥}$和$\\vec v_{⊥}$的夹角为$\\theta$，从而$\\vec v’_{⊥}$可由$\\vec w$和$\\vec v_{⊥}$线性表示为： \\vec v'_{⊥}=cos\\theta\\vec v_{⊥}+sin\\theta\\vec w\\\\ =cos\\theta(\\vec v-(\\vec v\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec v)\\tag {9}&emsp;&emsp;最后，根据公式$(6)$和公式$(9)$我们已知$\\vec v_{||}$和$\\vec v’_{⊥}$，从而可以得出$\\vec v’$： \\vec v'=\\vec v_{||}+\\vec v'_{⊥}\\\\ =cos\\theta(\\vec v-(\\vec v\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec v)+(\\vec v\\cdot\\vec n)\\vec n \\tag {10}&emsp;&emsp;由公式$(10)$可知，我们已经用$\\vec v$、$\\vec n$和$\\theta$表示$\\vec v’$，那如何根据上述的公式$(10)$构建旋转矩阵$R(\\vec n, \\theta)$？还是那个思路：列主序变换矩阵的列向量就是变换后的坐标系的基向量。我们只需求出笛卡尔坐标系的$\\vec x$、$\\vec y$、$\\vec z$三个轴方向上的基向量按照公式$(10)$旋转之后的基向量$\\vec x’$、$\\vec y’$、$\\vec z’$，然后填入矩阵$R(\\vec n, \\theta)$即可，以$\\vec x=[1\\ \\ 0 \\ \\ 0]^T$为例： \\vec x'=cos\\theta(\\vec x-(\\vec x\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec x)+(\\vec x\\cdot\\vec n)\\vec n =\\left[\\begin{matrix} n^2_x(1-cos\\theta)+cos\\theta \\\\n_xn_y(1-cos\\theta)+n_zsin\\theta \\\\n_xn_z(1-cos\\theta)-n_ysin\\theta) \\end{matrix}\\right] \\tag {11}&emsp;&emsp;$\\vec y=[0\\ \\ 1\\ \\ 0]^T$和$\\vec z=[0\\ \\ 0\\ \\ 1]^T$同理： \\vec y' =\\left[\\begin{matrix} n_xn_y(1-cos\\theta)-n_zsin\\theta \\\\n^2_y(1-cos\\theta)+cos\\theta \\\\n_yn_z(1-cos\\theta)+n_xsin\\theta \\end{matrix}\\right] \\tag {12} \\vec z' =\\left[\\begin{matrix} n_xn_z(1-cos\\theta)+n_ysin\\theta \\\\n_yn_z(1-cos\\theta)-n_xsin\\theta \\\\n^2_z(1-cos\\theta)+cos\\theta \\end{matrix}\\right] \\tag {13}&emsp;&emsp;将$\\vec x’$、$\\vec y’$、$\\vec z’$合并到$R(\\vec n, \\theta)$中： R(\\vec n, \\theta) =\\left[\\begin{matrix} \\vec x'&\\vec y'&\\vec z' \\end{matrix}\\right] \\\\=\\begin{bmatrix} {n_x}^2(1-cos\\theta)+cos\\theta&n_xn_y(1-cos\\theta)-n_zsin\\theta&n_xn_z(1-cos\\theta)+n_ysin\\theta \\\\n_xn_y(1-cos\\theta)+n_zsin\\theta&n^2_y(1-cos\\theta)+cos\\theta&n_yn_z(1-cos\\theta)-n_xsin\\theta \\\\n_xn_z(1-cos\\theta)-n_ysin\\theta)&n_yn_z(1-cos\\theta)+n_xsin\\theta&n^2_z(1-cos\\theta)+cos\\theta \\end{bmatrix} \\tag {14}12345678910111213141516171819202122void Matrix4x4::setRotationAxis(const double angle, const Vector3D &amp;axis)&#123; Vector3D u = axis.getNormalized(); float sinAngle = static_cast&lt;float&gt;(sin(M_PI*angle/180)); float cosAngle = static_cast&lt;float&gt;(cos(M_PI*angle/180)); float oneMinusCosAngle = 1.0f - cosAngle; loadIdentity(); entries[0] = (u.x)*(u.x) + cosAngle*(1-(u.x)*(u.x)); entries[4] = (u.x)*(u.y)*(oneMinusCosAngle) - sinAngle*u.z; entries[8] = (u.x)*(u.z)*(oneMinusCosAngle) + sinAngle*u.y; entries[1] = (u.x)*(u.y)*(oneMinusCosAngle) + sinAngle*u.z; entries[5] = (u.y)*(u.y) + cosAngle*(1-(u.y)*(u.y)); entries[9] = (u.y)*(u.z)*(oneMinusCosAngle) - sinAngle*u.x; entries[2] = (u.x)*(u.z)*(oneMinusCosAngle) - sinAngle*u.y; entries[6] = (u.y)*(u.z)*(oneMinusCosAngle) + sinAngle*u.x; entries[10] = (u.z)*(u.z) + cosAngle*(1-(u.z)*(u.z));&#125; 3、透视投影、正交投影&emsp;&emsp;$3D$空间中的物体最终都要通过投影显示到$2D$的屏幕上，这一过程就是投影变换。投影变换矩阵将视图空间中的顶点数据变换到裁剪空间，裁剪空间中的顶点最后通过透视除法被变换到标准化设备坐标（$NDC$）。通常由两类投影：透视投影、正交投影。 透视投影矩阵&emsp;&emsp;关于透视投影矩阵的前世今生我不过多说，直接上透视投影矩阵的推导过程。一个视锥体我们目前用六个参数表示：$left$，$right$，$bottom$，$top$，$near$，$far$，简写为$l$、$r$、$b$、$t$、$n$和$f$，即视锥体的六个面。我们的目标就是将视图空间中在视锥体内的点变换到标准化设备坐标中的立方体内。即$x$轴方向从$[l,r]$映射到$[-1,1]$，$y$轴方向从$[b,t]$映射到$[-1,1]$，$z$轴方向从$[-n,-f]$映射到$[-1,1]$。 &emsp;&emsp;可能你会觉得奇怪，$z$轴方向为什么是从$[-n,-f]$映射到$[-1,1]$？这是因为摄像机空间的坐标系是右手坐标系，在视图空间中摄像机是朝向视图坐标系的$z$轴的负方向，如下图左边所示，$+Y$、$+Z$、$+X$标准摄像机坐标系的三个轴，而摄像机的观察视锥体是朝向$-Z$方向的。而$NDC$又是左手坐标系，朝向$+Z$方向，所以我们要取负。 图2 透视投影视锥和标准化设备坐标 图3 从-Y方向看去的视锥横截面 图4 从-X方向看去的视锥横截面 &emsp;&emsp;在视锥体中的顶点$(x_e,y_e,z_e)$被投影到视锥体的近平面，近平面上的点我们记为$(x_p,y_p,-n)$。如图3和图4所示，根据三角形相似的原理，我们有： \\frac{x_p}{x_e}=\\frac{-n}{z_e}\\ \\rightarrow\\ x_p=\\frac{-n\\cdot x_e}{z_e}=\\frac{n\\cdot x_e}{-z_e} \\tag {15} \\frac{y_p}{y_e}=\\frac{-n}{y_e}\\ \\rightarrow\\ y_p=\\frac{-n\\cdot y_e}{z_e}=\\frac{n\\cdot y_e}{-z_e} \\tag {16}&emsp;&emsp;注意到公式$(15)$和$(16)$中分母都是一个$-z_e$，这与我们将裁剪空间中的顶点做透视除法相对应，透视投影然后做透视除法如下公式$(17)$、$(18)$所示： \\left( \\begin{matrix} x_{clip}\\\\ y_{clip}\\\\ z_{clip}\\\\ w_{clip} \\end{matrix} \\right) =M_{projection}\\cdot \\left( \\begin{matrix} x_{eye}\\\\ y_{eye}\\\\ z_{eye}\\\\ w_{eye} \\end{matrix} \\right) \\tag {17} \\left( \\begin{matrix} x_{ndc}\\\\ y_{ndc}\\\\ z_{ndc} \\end{matrix} \\right) = \\left( \\begin{matrix} x_{clip}/w_{clip}\\\\ y_{clip}/w_{clip}\\\\ z_{clip}/w_{clip} \\end{matrix} \\right) \\tag {18}&emsp;&emsp;为了便于构建矩阵（$x_e$和$y_e$均与$-z_e$相除，不好构建矩阵），我们令裁剪空间中的$w_{clip}$为$-z_e$，将除以$-z_e$的这一步挪到了透视除法去做。故目前的透视矩阵就变为： \\left( \\begin{matrix} x_{c}\\\\ y_{c}\\\\ z_{c}\\\\ w_{c} \\end{matrix} \\right) = \\left( \\begin{matrix} .&.&.&.\\\\ .&.&.&.\\\\ .&.&.&.\\\\ 0&0&-1&0 \\end{matrix} \\right) \\left( \\begin{matrix} x_{e}\\\\ y_{e}\\\\ z_{e}\\\\ w_{e} \\end{matrix} \\right) \\tag {19}&emsp;&emsp;其中”$.$”均表示未知。得到在近平面的$x_p$和$y_p$之后，我们还要将$x_p$映射到$[-1,1]$范围，同理$y_p$也是。以$x_p$为例，我们知道其值域为$[l,r]$。为了将$x_p$其映射到$[-1,1]$，我们首先将其映射到$[0,1]$，不难得到如下式子： \\frac{x_p-l}{r-l}\\in[0,1] \\tag {20}&emsp;&emsp;式$(20)$乘上一个$2$再减去$1$就映射到了$[-1,1]$，映射之后记为$x_n$： x_n=2\\frac{x_p-l}{r-l}-1=\\frac{2x_p}{r-l}-\\frac{r+l}{r-l}\\in[-1,1] \\tag {21}&emsp;&emsp;同理$y_p$到$y_n$的映射： y_n=\\frac{2y_p}{r-l}-\\frac{t+b}{t-b}\\in[-1,1] \\tag {22}&emsp;&emsp;然后将公式$(15)$中的$x_p$带入公式$(21)$，将公式$(16)$中的$y_p$带入公式$(22)$，以$x_p$为例： x_n=\\frac{2x_p}{r-l}-\\frac{r+l}{r-l} =\\frac{2\\frac{n\\cdot x_e}{-z_e}}{r-l}-\\frac{r+l}{r-l}\\\\ =\\frac{2n\\cdot x_e}{(r-l)(-z_e)}-\\frac{r+l}{r-l} =\\frac{\\frac{2n}{r-l}\\cdot x_e}{-z_e}-\\frac{r+l}{r-l}\\\\ =\\frac{\\frac{2n}{r-l}\\cdot x_e}{-z_e}+\\frac{\\frac{r+l}{r-l}\\cdot z_e}{-z_e} =\\underbrace{(\\frac{2n}{r-l}\\cdot x_e+\\frac{r+l}{r-l}\\cdot z_e)}_{x_c}/-z_e \\tag {23}&emsp;&emsp;其中$x_c$即公式$(19)$中的裁剪空间中的$x$轴坐标值。$y_p$同理可得$y_c$: y_n =\\underbrace{(\\frac{2n}{t-b}\\cdot y_e+\\frac{t+b}{t-b}\\cdot z_e)}_{y_c}/-z_e \\tag {24}&emsp;&emsp;现在我们已经知道了$x_c$和$y_c$分辨关于$x_e$、$y_e$以及$z_e$的表达形式，我们可以填充式$(19)$中的投影矩阵第一行与第二行： \\left( \\begin{matrix} x_{c}\\\\ y_{c}\\\\ z_{c}\\\\ w_{c} \\end{matrix} \\right) = \\left( \\begin{matrix} \\frac{2n}{r-l}&0&\\frac{r+l}{r-l}&0\\\\ 0&\\frac{2n}{t-b}&\\frac{t+b}{t-b}&0\\\\ 0&0&A&B\\\\ 0&0&-1&0 \\end{matrix} \\right) \\left( \\begin{matrix} x_{e}\\\\ y_{e}\\\\ z_{e}\\\\ w_{e} \\end{matrix} \\right) \\tag {25}&emsp;&emsp;现在我们还剩下投影矩阵的第三行还不知道。因为我们知道$z$的投影与$x_e$和$y_e$无关，只与$z_e$、$w_e$有关，故可以假设投影矩阵的第三行如上式$(25)$所示，$A$和$B$就是我们假设的要求解的未知表达式。此外，在视图空间中的$w_e$是等于$1$的，$w_c$即前面提到的$-z_e$，从而有： z_n=z_c/w_c=\\frac{Az_e+Bw_e}{-z_e}=\\frac{Az_e+B}{-z_e} \\tag {26}&emsp;&emsp;为了求出公式$(26)$中的$A$和$B$，我们取两个极端的例子：在$-n$处的$z$值被映射到$-1$，在$-f$处的$z$值被映射到$1$，将$(z_n,z_e)=(-1,-n)$和$(z_n,z_e)=(1,-f)$带入式$(26)$中，可得方程组： \\begin{cases} \\frac{-An+B}{n}=-1\\\\ \\frac{-Af+B}{f}=1\\\\ \\end{cases}\\ \\rightarrow\\ \\begin{cases} {-An+B}=-n\\\\ {-Af+B}=f\\\\ \\end{cases} \\tag {27}&emsp;&emsp;求解方程$(27)$，可得$A$与$B$如下所示： A=-\\frac{f+n}{f-n}\\\\ B=-\\frac{2fn}{f-n} \\tag {28}&emsp;&emsp;将公式$(28)$带入公式$(26)$中： z_n=\\underbrace{(-\\frac{f+n}{f-n}z_e-\\frac{2fn}{f-n})}_{z_c}/{-z_e} \\tag {29}&emsp;&emsp;我们最终得到了$z_c$关于$z_e$的表达式，将$A$与$B$填入式$(25)$的投影矩阵即可，$M_{projection}$就是我们一直在寻求的透视投影矩阵： M_{projection}= \\left( \\begin{matrix} \\frac{2n}{r-l}&0&\\frac{r+l}{r-l}&0\\\\ 0&\\frac{2n}{t-b}&\\frac{t+b}{t-b}&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {30}&emsp;&emsp;公式$(30)$中的透视投影矩阵只是一个通用的形式，在视图空间中的视锥体通常都是关于$x$轴和$y$轴对称的，从而有$r=-l$、$t=-b$，将式$(30)$简化成如下形式： M_{projection}= \\left( \\begin{matrix} \\frac{2n}{r-l}&0&0&0\\\\ 0&\\frac{2n}{t-b}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {31}&emsp;&emsp;&emsp;但是通常我们传入构建透视矩阵函数的参数是$fovy$（$y$轴方向的视域角）、$aspect$（屏幕的宽高比）、$near$（近平面）以及$far$（远平面），如何根据这些参数构造式$(31)$的透视投影矩阵呢？注意到$r-l=width$即近平面宽度，$t-b=height$即近平面的高度，我们可以根据$fovy$和$aspect$得出$width$和$height$，具体细节不再赘述： r-l=width=2*near*aspect*tan(fovy/2)\\\\ t-b=height=2*near*tan(fovy/2) M_{projection}= \\left( \\begin{matrix} \\frac{1}{aspect*tan(fovy/2)}&0&0&0\\\\ 0&\\frac{1}{tan(fovy/2)}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {32}123456789101112void Matrix4x4::setPerspective(float fovy, float aspect, float near, float far)&#123; loadZero(); // convert fov from degrees to radians float rFovy = fovy*M_PI/180; const float tanHalfFovy = tanf(static_cast&lt;float&gt;(rFovy*0.5f)); entries[0] = 1.0f/(aspect*tanHalfFovy); entries[5] = 1.0f/(tanHalfFovy); entries[10] = -(far+near)/(far-near); entries[11] = -1.0f; entries[14] = (-2.0f*near*far)/(far-near);&#125; 正交投影矩阵&emsp;&emsp;理解了透视投影矩阵的构造之后，正交投影就简单太多了，正交投影只需做简单的线性映射就行了。只需将$x$轴方向从$[l,r]$映射到$[-1,1]$，$y$轴方向从$[b,t]$映射到$[-1,1]$，$z$轴方向从$[-n,-f]$映射到$[-1,1]$，而这个映射的过程很简单，正如前面公式$(20)$和$(21)$那样，先映射到$[0,1]$，再映射到$[0,2]$，最后映射到$[-1,1]$，这个过程我也不细说了，直接上结果： M_{projection}= \\left( \\begin{matrix} \\frac{2}{r-l}&0&0&-\\frac{r+l}{r-l}\\\\ 0&\\frac{2}{t-b}&0&-\\frac{t+b}{t-b}\\\\ 0&0&\\frac{-2}{f-n}&-\\frac{f+n}{f-n}\\\\ 0&0&0&1 \\end{matrix} \\right) \\tag {33}&emsp;&emsp;然后又因为视锥体关于$x$轴、$y$轴对称，简化的正交投影矩阵就为： M_{projection}= \\left( \\begin{matrix} \\frac{2}{r-l}&0&0&0\\\\ 0&\\frac{2}{t-b}&0&0\\\\ 0&0&\\frac{-2}{f-n}&-\\frac{f+n}{f-n}\\\\ 0&0&0&1 \\end{matrix} \\right) \\tag {33}12345678910void Matrix4x4::setOrtho(float left, float right, float bottom, float top, float near, float far)&#123; loadIdentity(); entries[0] = 2.0f/(right-left); entries[5] = 2.0f/(top-bottom); entries[10] = -2.0f/(far-near); entries[12] = -(right+left)/(right-left); entries[13] = -(top+bottom)/(top-bottom); entries[14] = -(far+near)/(far-near);&#125; 4、lookAt函数构造视图矩阵&emsp;&emsp;视图矩阵的工作目标是将世界坐标系中的所有物体的顶点的坐标从世界坐标系转换到摄像机坐标系。这是因为摄像机坐标系的原点不一定与世界坐标系重合，同时由于自身的旋转，坐标轴也一定不与世界坐标系的坐标轴平行。为完成工作任务，需要分为两步走：首先整体平移，将摄像机平移至世界坐标系原点，然后将顶点从世界坐标系变换至摄像机坐标系。 &emsp;&emsp;lookAt函数的输入参数分别为：$eye$摄像机的位置，$target$摄像机目标点，$up$世界空间的上向量,。首先我们要根据这些参数确定摄像机坐标系的三个轴向量，其中需要非常注意的就是变换到视图空间中时摄像机是朝向视图空间的$-Z$方向的，所以求视图空间中的$Z$轴时是摄像机的位置减去目标点的位置： Z = normalize(eye - target)\\\\ X = normalize(cross(up, Z))\\\\ Y = normalize(cross(Z,X))&emsp;&emsp;通过以上的方式我们就求出了视图空间的三条轴向量，再加上摄像机的位置我们就可以求出将世界坐标变换到与视图坐标重合的矩阵了，记为$M=T\\cdot R$，其中$T$是平移到摄像机位置$eye$的变换矩阵，$R$是旋转到摄像机坐标轴方向的旋转矩阵： M=T\\cdot R= \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\cdot \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {34}&emsp;&emsp;然而公式$(34)$并不是我们要求的视图矩阵，上式中的矩阵$M$仅仅是将世界坐标轴变换到摄像机坐标轴。摄像机只是一个虚拟的物品，我们不能将上述的矩阵$M$作用于摄像机，因为摄像机根本不存在！我们视图矩阵最终作用的世界空间中的物体，这就涉及到了一个相对运动的概念！ &emsp;&emsp;当我们向前移动摄像机的时候，可以看成是摄像机不动，而物体朝着与摄像机朝向相反的方向移动。当我们向右旋转摄像机时，相当于摄像机不动而物体朝着摄像机的左边移动。摄像机的构造得益于相对于运动的理论，计算机图形学中的虚拟$3D$摄像机实际上是通过物体的移动来实现的，所以我们要构造的视图矩阵是公式$(34)$中的逆矩阵。 viewMatrix = M^{-1}=(T\\cdot R)^{-1}=R^{-1}\\cdot T^{-1} = \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} \\cdot \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} \\tag {35}&emsp;&emsp;由上式可知，构造视图矩阵涉及到$R$和$T$的求逆，其中的平移矩阵$T$的求逆则是直接取平移量的相反数即可： T^{-1}= \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} = \\left[ \\begin{matrix} 1&0&0&-eye_x\\\\ 0&1&0&-eye_x\\\\ 0&0&1&-eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {36}&emsp;&emsp;至于旋转矩阵$R$，我们知道旋转矩阵都是正交矩阵，正交矩阵的一个特点就是它的逆等于它的转置： R^{-1}= \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} = \\left[ \\begin{matrix} X_x&X_y&X_z&0\\\\ Y_x&Y_y&Y_z&0\\\\ Z_x&Z_y&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {37}&emsp;&emsp;最后，我们得到视图矩阵： viewMatrix=R^{-1}\\cdot T^{-1}= \\left[ \\begin{matrix} X_x&X_y&X_z&0\\\\ Y_x&Y_y&Y_z&0\\\\ Z_x&Z_y&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\cdot \\left[ \\begin{matrix} 1&0&0&-eye_x\\\\ 0&1&0&-eye_x\\\\ 0&0&1&-eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\\\= \\left[ \\begin{matrix} X_x&X_y&X_z&-(\\vec X\\cdot \\vec {eye})\\\\ Y_x&Y_y&Y_z&-(\\vec Y\\cdot \\vec {eye})\\\\ Z_x&Z_y&Z_z&-(\\vec Z\\cdot \\vec {eye})\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {38}1234567891011121314151617181920212223242526void Matrix4x4::setLookAt(Vector3D cameraPos, Vector3D target, Vector3D worldUp)&#123; Vector3D zAxis = cameraPos - target; zAxis.normalize(); Vector3D xAxis = worldUp.crossProduct(zAxis); xAxis.normalize(); Vector3D yAxis = zAxis.crossProduct(xAxis); yAxis.normalize(); loadIdentity(); entries[0] = xAxis.x; entries[4] = xAxis.y; entries[8] = xAxis.z; entries[1] = yAxis.x; entries[5] = yAxis.y; entries[9] = yAxis.z; entries[2] = zAxis.x; entries[6] = zAxis.y; entries[10] = zAxis.z; entries[12] = -(xAxis.dotProduct(cameraPos)); entries[13] = -(yAxis.dotProduct(cameraPos)); entries[14] = -(zAxis.dotProduct(cameraPos));&#125; 参考资料$[1]$ http://www.songho.ca/opengl/gl_projectionmatrix.html $[2]$ https://blog.csdn.net/zsq306650083/article/details/8773996 $[3]$ https://blog.csdn.net/y1196645376/article/details/78463248 $[4]$ https://www.cnblogs.com/J1ac/p/9340622.html $[5]$ https://learnopengl-cn.github.io/01%20Getting%20started/08%20Coordinate%20Systems/","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/tags/Soft-Renderer/"},{"name":"3D Math","slug":"3D-Math","permalink":"http://yoursite.com/tags/3D-Math/"}]},{"title":"流体模拟Fluid Simulation：流体模拟基础","slug":"fluidSimulation","date":"2019-05-01T02:37:38.127Z","updated":"2019-06-04T08:32:47.592Z","comments":true,"path":"2019/05/01/fluidSimulation/","link":"","permalink":"http://yoursite.com/2019/05/01/fluidSimulation/","excerpt":"本文主要参考文献《FLUID SIMULATION SIGGRAPH 2007 Course Notes》，结合我的理解单纯地讲述一下流体渲染的一些基础知识，本人水平有限，如有错误，欢迎指出。本文只是单纯针对流体模拟领域，可能一些地方不太严谨，但是对于虚拟模拟来说是可行的。即便如此，本文涉及到大量的数学方法。","text":"本文主要参考文献《FLUID SIMULATION SIGGRAPH 2007 Course Notes》，结合我的理解单纯地讲述一下流体渲染的一些基础知识，本人水平有限，如有错误，欢迎指出。本文只是单纯针对流体模拟领域，可能一些地方不太严谨，但是对于虚拟模拟来说是可行的。即便如此，本文涉及到大量的数学方法。 矢量微积分 Naiver-Stokes偏微分方程组 N-S方程的分步求解 对流算法 一、矢量微积分&emsp;&emsp;高等数学中太多数讨论的是一维的微积分，而矢量微积分则是一维微积分的高维扩展。矢量微积分的三个基础算子：梯度（符号为$∇$），散度（符号为$∇\\cdot$)，旋度（符号为$∇\\times$），在此基础上流体力学中经常用到的还有拉普拉斯算子。 1、梯度（Gradient）&emsp;&emsp;梯度实际上就是矢量的空间偏导数，且结果依然是一个矢量，$2$维的梯度如下： ∇f(x,y)=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}) \\tag {1.1}&emsp;&emsp;依此类推，$3$维的梯度有如下形式： ∇f(x,y,z)=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y},\\frac{\\partial f}{\\partial z}) \\tag {1.2}&emsp;&emsp;有时也会采用如下形式来表示梯度： ∇f=\\frac{\\partial f}{\\partial \\vec x} \\tag {1.3}&emsp;&emsp;梯度通常用来近似计算函数值（实际上就是一维形式的推广)： f(\\vec x+\\Delta \\vec x)\\approx f(\\vec x)+∇f(\\vec x)\\cdot \\Delta \\vec x \\tag {1.4}&emsp;&emsp;同样的，多个函数的梯度就构成了一个矩阵： ∇\\vec F=∇(f,g,h)=\\left( \\begin{matrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} & \\frac{\\partial f}{\\partial z} \\\\ \\frac{\\partial g}{\\partial x} & \\frac{\\partial g}{\\partial y} & \\frac{\\partial g}{\\partial z} \\\\ \\frac{\\partial h}{\\partial x} & \\frac{\\partial h}{\\partial y} & \\frac{\\partial h}{\\partial z} \\\\ \\end{matrix} \\right) =\\left( \\begin{matrix}∇f\\\\ ∇g\\\\ ∇h\\\\ \\end{matrix} \\right) \\tag {1.5}2、散度（Divergence）&emsp;&emsp;散度算子仅仅应用于向量场，它衡量在某一点出相应的矢量聚集或者发散程度，测量方向为径向，结果为标量。$2$维、$3$维形式的散度算子如下所示： ∇\\cdot \\vec u=∇\\cdot (u,v)=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y} ∇\\cdot \\vec u=∇\\cdot (u,v,w)=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}+\\frac{\\partial w}{\\partial z} \\tag {1.6}&emsp;&emsp;输入是矢量，而输出为标量。类比梯度，散度符号$∇\\cdot \\vec u$可以理解为梯度$∇$与矢量$\\vec u$的点乘： ∇\\cdot \\vec u=(\\frac{\\partial}{\\partial x},\\frac{\\partial}{\\partial y},\\frac{\\partial}{\\partial z})\\cdot (u,v,w)=\\frac{\\partial}{\\partial x}u+\\frac{\\partial}{\\partial y}v+\\frac{\\partial}{\\partial z}w \\tag {1.7}&emsp;&emsp;若矢量场散度为$0$，则称该矢量场无散度。 3、旋度（Curl）&emsp;&emsp;旋度衡量围绕某一点的旋转速度，测量方向为切向，三维形式的旋度是一个向量： ∇\\times \\vec u=∇\\times (u,v,w) =(\\frac{\\partial w}{\\partial y}-\\frac{\\partial v}{\\partial z}, \\frac{\\partial u}{\\partial z}-\\frac{\\partial w}{\\partial x}, \\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y}) \\tag {1.8}&emsp;&emsp;倒推到$2$维，我们取上式中的$w=0$，即矢量场为$(u,v,0)$，$2$维向量场的旋度是一个标量： ∇\\times \\vec u=∇\\times (u,v)=\\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y} \\tag {1.9}&emsp;&emsp;同样地，旋度符号$∇\\times \\vec u$我们可以理解为梯度$∇$与矢量场$\\vec u$的叉乘： ∇\\times \\vec u= (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})\\times(u,v,w) \\tag {1.10}&emsp;&emsp;若矢量场旋度为$0$，则称该矢量场无旋度。 4、拉普拉斯算子（Laplacian）&emsp;&emsp;拉普拉斯算子定义为梯度的散度，符号表示为$∇\\cdot∇$，显然$∇\\cdot$是散度，而后面的$∇$则为梯度，故拉普拉斯算子即梯度的散度，是一个二阶微分算子。$2$维、$3$维形式分别如下： ∇\\cdot∇f=\\frac{\\partial^2f}{\\partial x^2}+\\frac{\\partial^2f}{\\partial y^2} ∇\\cdot∇f=\\frac{\\partial^2f}{\\partial x^2}+\\frac{\\partial^2f}{\\partial y^2}+\\frac{\\partial^2f}{\\partial z^2} \\tag {1.11}&emsp;&emsp;简言之，拉普拉斯算子定义如下： ∇\\cdot∇f=\\Sigma_{i=1}^n\\frac{\\partial^2f}{\\partial x_i^2} \\tag {1.12}&emsp;&emsp;偏微分方程$∇\\cdot ∇f=0$被称为拉普拉斯方程；而如果右边为某个非$0$常数，即$∇\\cdot ∇f=q$，我们称之为泊松方程。更一般地，如果梯度再乘上一个标量$a$（如$1/\\rho$)，即$∇\\cdot (a∇f)=q$，我们依旧称之为泊松问题。 二、$Naiver-Stokes$偏微分方程组&emsp;&emsp;流体模拟器的构建主要围绕著名的不可压缩$Navier-Stokes$方程展开，它是一个流体力学领域的偏微分方程，方程形式如下： \\frac{\\partial \\vec u}{\\partial t}+\\vec u\\cdot ∇\\vec u+\\frac1\\rho∇p=\\vec g+\\nu∇\\cdot∇\\vec u \\tag {2.1} ∇\\cdot\\vec u=0 \\tag {2.2}&emsp;&emsp;这个方程组看起非常地复杂，接下来我们就把它剖析成一个个比较简单的部分来理解。 1、符号标记&emsp;&emsp;我们有必要定义一些物理量的符号用以标记： &emsp;&emsp;符号$\\vec u$在流体力学中通常表示为流体的速度矢量，记$3$维的速度矢量$\\vec u=(u,v,w)$； &emsp;&emsp;希腊字符$\\rho$是流体的密度，对于水，该值大约为$1000kg/m^3$，而空气则大约为$1.3kg/m^3$； &emsp;&emsp;字符$p$代表压力，流体对任何物体施加的单位面积力； &emsp;&emsp;字符$\\vec g$则是我们熟悉的重力加速度，通常取$(0,-9.81,0)m/s^2$。我们约定$y$轴向上，而$x$轴和$z$轴在水平面上。另外补充一点，我们把其他的一些类似的力都累加到$\\vec g$上，也就是我们统一用$\\vec g$表示所有类似力之和，这类力我们称之为体积力（称之为体积力是因为它们的力是作用到整个流体而不只是流体的表面）； &emsp;&emsp;希腊字符$\\nu$是流体的运动粘度，它衡量流体的黏滞性。糖蜜之类的流体有非常高的粘度，而像酒精之类的流体有很低的粘度； &emsp;&emsp;其它一些矢量微积分的符号算子前面已经提到过，不再赘述。 2、动量方程&emsp;&emsp;偏微分方程$(2.1)$我们称之为动量方程，它本质上就是我们熟悉的牛顿定律$\\vec F=m\\vec a$的形式，描述了施加在流体上的力是如何影响流体的运动。 &emsp;&emsp;假设我们用粒子系统来模拟流体，每个粒子代表流体的一小滴，每个粒子有各自的质量$m$、体积$V$和速度$\\vec u$。为了让整个粒子系统运作起来，我们必须弄清楚每个粒子所承受的力的作用。牛顿第二定律告诉我们：$\\vec F=m\\vec a$，而根据加速度定义，我们有： \\vec a=\\frac{D\\vec u}{Dt} \\tag {2.3}&emsp;&emsp;符号$D$是指物质导数，所谓物质导数，就是对流体质点求导数，而且是针对流体质点（在这里就是流体粒子）而不是空间的固定点。因而牛顿第二定律就变成： m\\frac{D\\vec u}{Dt}=\\vec F \\tag {2.4}&emsp;&emsp;那么流体粒子承受的力有哪些呢？一个最简单的力就是重力：$m\\vec g$。而其他的流体质点（或其他流体粒子）也会对当前的流体粒子产生作用力。流体内部的相互作用力之一便是压力，较大压力的区域会向较低压力区域产生作用力。值得注意的是，我们只关注施加在粒子上的压力的净合力。例如，若施加在粒子上压力在每个方向上都相等，那么它的压力的合力便为0。我们用压力的负梯度（取负是因为方向是由压力大的区域指向压力小的区域）来衡量在当前流体粒子处压力的不平衡性，即取$-∇p$。那么流体粒子所承受的压力就是对$-∇p$在整个流体粒子的体积上进行积分，为了简化，我们简单地将$V$与$-∇p$相乘，故粒子压力部分为$-V∇p$。 &emsp;&emsp;其他的流体相互作用力则是由流体的黏性产生的，我们直观地把这种力理解为尽可能使得粒子以周围区域的平均速度移动的力，也就是使得粒子的速度与周围区域粒子速度的差距最小化。拉普拉斯算子是衡量一个量与之周围区域该量平均值之差的算符，因而$∇\\cdot∇\\vec u$是当前粒子速度矢量与周围区域平均速度矢量之差。为了计算粘滞力，我们同样对$∇\\cdot∇\\vec u$在整个粒子体积$V$上进行积分，与前面类似，我们简单取$V∇\\cdot∇\\vec u$。除此之外，我们还引进一个称为动力粘度系数的物理量，符号为$\\mu$。因而粘滞力为$V\\mu∇\\cdot∇\\vec u$。 &emsp;&emsp;把重力、压力和粘滞力综合一起，我们可得： m\\frac{D\\vec u}{Dt}=\\vec F=m\\vec g-V∇p+V\\mu∇\\cdot∇\\vec u \\tag {2.5}&emsp;&emsp;当粒子系统中的粒子数量趋于无穷大，而每个粒子大小趋于$0$时，会产生一个问题：此时每个粒子的质量$m$和体积$V$变为$0$，此时上式变得没有意义。为此，我们把$(2.5)$式调整一下，两边同除以体积$V$，又因$\\rho=m/V$，故有： \\rho\\frac{D\\vec u}{Dt}=\\rho\\vec g-∇p+\\mu∇\\cdot∇\\vec u \\tag {2.6}&emsp;&emsp;两边同除以$\\rho$，移项调整： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g+\\frac\\mu\\rho∇\\cdot∇\\vec u \\tag {2.7}&emsp;&emsp;为了进一步简化，定义运动粘度为$\\nu=\\mu/\\rho$，式$(2.7)$变为： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g+\\nu∇\\cdot∇\\vec u \\tag {2.8}&emsp;&emsp;我们已经快把动量方程推导出来，现在我们要把物质导数$\\frac{D\\vec u}{Dt}$弄清楚，为此，我们需要了解两种描述方法：拉格朗日描述和欧拉描述。 3、拉格朗日描述与欧拉描述&emsp;&emsp;当我们尝试研究流体或可变形固体的运动的时候，通常有两种方法来描述：拉格朗日描述（ Lagrangian viewpoint）、欧拉描述（Eulerian viewpoint）。 &emsp;&emsp;拉格朗日描述方法是我们比较熟悉的方法，这种描述方法把物体看成是由类似于粒子系统的形式组成，固体或流体的每个点看作一个独立的粒子，粒子有各自相应的位置$\\vec x$和速度$\\vec u$。我们可以把粒子理解为组成物体的分子。对于我们通常采用拉格朗日描述法进行建模模拟，即用一系列离散的粒子集来构建，粒子之间通过网格相联系。 &emsp;&emsp;欧拉描述方法则采用了完全不同的角度，它常被用于流体力学中。与拉格朗日描述追踪每个物体粒子的方法不同，欧拉描述关注点是空间中的一个固定点，并考察在这个固定点上流体性质（如密度、速度、温度等）是如何随着时间变化的。流体流动经过这个固定点可能会导致这个固定点的物理性质发生一些变化（如一个温度较高的流体粒子流经这个固定点，后面紧跟着一个温度较低的流体粒子流过固定点，那么这个固定点的温度会降低，但是并没有任何一个流体粒子的温度发生了变化）。 &emsp;&emsp;用天气测量举个简单的例子：拉格朗日描述方法就是你乘坐在一个随风而飘的热气球上，测量周围空气的压力、密度和浑浊度等天气指标；而欧拉描述方法就是你固定在地面上，测量流过的空气的天气指标。 &emsp;&emsp;欧拉描述法似乎看起来带来了一些不必要的复杂度，但是目前大多数的流体模拟器都是基于欧拉描述法，这是因为欧拉描述法相比于拉格朗日描述法有一些不可比拟的优点：欧拉描述法能够更加方便地计算一些物理量的空间导数（例如压力梯度和粘度）；而如果用粒子方法的话（即拉格朗日描述法），那么计算物理量相对于空间位置的变化是比较难的。 &emsp;&emsp;把拉格朗日描述法和欧拉描述法联系起来的关键点就是物质导数。首先从拉格朗日描述法出发，假设有一群粒子，每个粒子都有各自的位置$\\vec x$和速度$\\vec u$。记$q$为通用的物理量（如密度、速度和温度等），每个粒子有其对应的$q$值。方程$q(t,\\vec x)$描述在时间点$t$而位置为$\\vec x$的粒子对应的物理量值$q$。则一个粒子的物理量$q$随时间$t$的变化率是多少？这是一个拉格朗日描述角度下的问题，我们取对时间$t$的导数（注意用到了求导链式法则，以及$\\frac{\\partial q}{\\partial \\vec x}=∇q$和$\\vec u=\\frac{d\\vec x}{dt}）$： \\frac d{dt}q(t,\\vec x)=\\frac{\\partial q}{\\partial t}+∇q\\cdot\\frac{d\\vec x}{dt}=\\frac{\\partial q}{\\partial t}+∇q\\cdot\\vec u\\equiv\\frac{Dq}{Dt} \\tag {2.9}&emsp;&emsp;这就是物质导数。把式$(2.9)$代入式$(2.8)$我们就得到了流体动量方程$(2.1)$。物质导数针对的是流体质点（在这里就是流体粒子）而不是空间的固定点。式$(2.9)$写完整一点就是： \\frac{Dq}{Dt}=\\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}+v\\frac{\\partial q}{\\partial y}+w\\frac{\\partial q}{\\partial z} \\tag {2.10}&emsp;&emsp;对于给定的速度场$\\vec u$， 流体的物理性质如何在这个速度场$\\vec u$下变化的计算我们称之为对流（advection）。一个最简单的对流方程，就是其物理量的物质导数为$0$，如下所示： \\frac{Dq}{Dt}=0\\implies\\frac{\\partial q}{\\partial t}+\\vec u\\cdot ∇q = 0 \\tag {2.11}&emsp;&emsp;公式$(2.11)$的意义即在拉格朗日视角观察下，每个流体粒子的物理量保持不变。 4、不可压缩性&emsp;&emsp;关于流体的压缩性在此不做过多的物理细节描述，只需知道一点：通常情况下流体的体积变化非常小（除开一些极端的情况，而且这些极端情况我们日常生活中较少出现）。可压缩流体的模拟涉及到非常复杂的情况，往往需要昂贵的计算资源开销，为此在计算机流体模拟中我们通常把所有的流体当作是不可压缩的，即它们的体积不会发生变化。 &emsp;&emsp;任取流体的一部分，设其体积为$\\Omega$而其边界闭合曲面为$\\partial\\Omega$，我们可以通过围绕边界曲面$\\partial\\Omega$对流体速度$\\vec u$在曲面法线方向上的分量进行积分来衡量这块部分流体的体积变化速率： \\frac d{dt}Volume(\\Omega)=\\int\\int_{\\partial\\Omega}\\vec u\\cdot n \\tag{2.12}&emsp;&emsp;对于不可压缩的流体，其体积保持为某个常量，故其体积变化速率为$0$： \\int\\int_{\\partial\\Omega}\\vec u\\cdot n=0 \\tag {2.13}&emsp;&emsp;由高斯散度定理，我们可以把式$(2.13)$转换为体积分： \\int\\int_{\\partial\\Omega}\\vec u\\cdot n=\\int\\int\\int_\\Omega∇\\cdot \\vec u=0 \\tag{2.14}&emsp;&emsp;式$(13)$应该对任意的$\\Omega$成立，意即无论$\\Omega$取何值，积分值均为$0$。这种情况下只有令积分函数值取$0$方可成立，即对$0$积分无论$\\Omega$取何值结果均为$0$。所以有： ∇\\cdot \\vec u=0 \\tag{2.15}&emsp;&emsp;这就是$Navier-Stokes$方程中的不可压缩条件$(2.2)$。满足不可压缩条件的速度场被称为是无散度的，即在该速度场下流体体积既不膨胀也不坍缩，而是保持在一个常量。模拟不可压缩流体的关键部分就是使得流体的速度场保持无散度的状态，这也是流体内部压力的来源。 &emsp;&emsp;为了把压力与速度场的散度联系起来，我们在动量方程$(2.1)$两边同时取散度： ∇\\cdot\\frac{\\partial \\vec u}{\\partial t}+∇\\cdot(\\vec u\\cdot ∇\\vec u)+∇\\cdot\\frac1\\rho∇p=∇\\cdot(\\vec g+\\nu∇\\cdot∇\\vec u) \\tag {2.16}&emsp;&emsp;对于上式$(2.16)$第一项，我们转变一下求导次序： \\frac {\\partial}{\\partial t}∇\\cdot\\vec u \\tag {2.17}&emsp;&emsp;如果满足流体不可压缩条件，那么式$(2.17)$取值$0$（因为无散度），然后我们调整一下式$(2.16)$可得关于压力的方程： ∇\\cdot\\frac1\\rho∇p=∇\\cdot(-\\vec u\\cdot ∇\\vec u+\\vec g+\\nu∇\\cdot∇\\vec u) \\tag{2.18}5、丢弃粘度项&emsp;&emsp;在某些流体如蜂蜜、小水珠等的模拟中，粘滞力起着非常重要的作用。但是在大多数流体动画模拟中，粘滞力的影响微乎其微，为此秉持着方程组越简单越好的原则，我们常常丢弃粘度项。当然这也不可避免地带来一些误差，事实上，在计算流体力学中尽可能地减少丢弃粘度项带来的误差是一个非常大的挑战。下面的叙述都是基于丢弃粘度项的前提。 &emsp;&emsp;丢弃了粘度项的$Navier-Stokes$方程被称为欧拉方程，而这种理想的流体则是无粘度的。丢弃了粘度项的欧拉方程如下： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g \\tag {2.19} ∇\\cdot\\vec u=0 \\tag{2.20}&emsp;&emsp;大多数的流体模拟的计算方程都是欧拉方程。 6、边界条件&emsp;&emsp;目前为止我们讨论的都是流体内部的情况，然而边界部分也是流体模拟非常关键的部分。在流体模拟中我们仅仅关注两种边界条件：固体墙（solid walls）、自由面（free surfaces）。 &emsp;&emsp;固体墙顾名思义就是流体与固体接触的边界，用速度来描述很简单：流体既不会流进固体内部也不会从固体内部流出，因此流体在固体墙法线方向上的分量为$0$： \\vec u\\cdot n=0 \\tag {2.21}&emsp;&emsp;当然，上述是固体自身不移动的情况下。通常来说，流体速度在法线方向上的分量与固体的移动速度在法线方向上的分量应该保持一致： \\vec u\\cdot n=\\vec u_{solid}\\cdot n \\tag{2.22}&emsp;&emsp;上述的两个公式都是仅对流体速度在法线方向上的分量做了限制，对于无粘度的流体，切线方向上的流体速度与固体的移动速度无必然的联系。 &emsp;&emsp;自由面是另外一个非常重要的边界条件，它通常就是与另外一种流体相接壤的边界部分。例如在模拟水花四溅时，水流表面不与固体接触的都是自由面（如与空气这种流体接触）。因空气密度远小于水导致空气对水体的仿真影响非常小，为了简化模拟，我们将空气所占的空间设为某个固定大气压的区域，设为$0$是最方便的方案，此时自由面就是压强$p=0$的水体表面。 &emsp;&emsp;在小规模的流体仿真中，自由面的表面张力占据着非常重要的地位。在微观分子层面下，表面张力的存在是因为不同的分子相互吸引产生的力。从几何的角度来解释就是，表面张力就是促使流体的表面积尽可能小的一种力。物理学上，两种不同的流体之间实际上存在着与表面平均曲率成正比的压力骤变： [p]=\\lambda k. \\tag {2.23}&emsp;&emsp;公式$(2.23)$中的$[p]$记为压力之差。$\\lambda$是表面张力系数，可以根据模拟的流体类型查找对应的张力系数（例如空气与水在室温下张力系数为$\\lambda \\approx 0.073N/m$）。而$k$就是平均曲率，单位为$m^{-1}$。又因为我们常常设空气的压力为$0$，因此水与空气交界的自由面的压力为： p=\\lambda k \\tag {2.24}​ 三、N-S方程的分步求解&emsp;&emsp;有了对以上对$Navier-Stokes$方程的理论支撑，接下来我们就要如何用计算机来对该组偏微分方程进行离散化求解。为了程序的松耦合性以及使计算尽可能地高效、简单，在流体模型领域，我们将流体方程分成几个独立的步骤，然后按顺序先后推进。对于不可压缩的无粘度流体方程（即前面的欧拉方程$(2.19)$和$(2.20)$，我们将其离散化成对流项（advection）如公式$(3.1)$、体积力项（body force）如公式$(3.2)$、压力/不可压缩项如公式$(3.3)$： \\frac{Dq}{Dt}=0 \\tag {3.1} \\frac{\\partial \\vec u}{\\partial t}=\\vec g \\tag {3.2} \\begin{cases} \\frac{\\partial \\vec u}{\\partial t}+\\frac{1}{\\rho}∇p=0\\\\ ∇\\cdot\\vec u=0 \\end{cases} \\tag {3.3}&emsp;&emsp;需要注意的是，在对流项公式$(3.1)$中我们用了一个通用量的符号$q$是因为我们不仅仅要对流体的速度进行对流，还需要对其他物理量进行对流。我们记对流项公式$(3.1)$的对流计算算法为$advect(\\vec u, \\Delta t, q)$，即对于给定的时间步长$\\Delta t$和速度场$\\vec u$，对物理量q进行对流。 &emsp;&emsp;对于体积力项$(3.2)$，我们采用简单的前向欧拉法即可：$\\vec u \\leftarrow \\vec u + g\\Delta t$。 &emsp;&emsp;对于压力/不可压缩项$(3.3)$，我们用一个称为$project(\\Delta t, \\vec u)$的算法，通过$project(\\Delta t, \\vec u)$计算出正确的压力以确保速度场$\\vec u$的无散度性质。欧拉方案不会着重研究具体粒子间的作用力，因而不会正向去求解$\\frac{1}{\\rho}∇p$，它是利用流体不可压缩的特性，将速度场$\\vec u$投影到散度为$0$的空间上，间接地解算了压力项。这种思想相当于，已知一个中间量$\\vec u_{temp}$，对这个中间量的唯一一个操作（如正向求解压力$\\frac{1}{\\rho}∇p$）不可行，但是直到最终量$\\vec u_{fianl}$符号的一个性质（散度为$0$），于是只要将$\\vec u_{temp}$投影到符合散度为$0$的特性平面上，即可间接地还原正向求解压力的操作，得到最终的速度场$\\vec u_{temp}$。 &emsp;&emsp;对流项$advect(\\vec u, \\Delta t, q)$的输入速度场$\\vec u$要确保为无散度的状态，投影项$project(\\Delta t, \\vec u)$确保了流体体积保持不变，因而投影项输出的速度场必然是无散度的。所以我们只要确保投影项$project(\\Delta t, \\vec u)$输出的速度场$\\vec u$作为对流项$advect(\\vec u, \\Delta t, q)$的输入即可，这时我们的分步求解流体方程的优势就体现出来了，其伪代码如下所示。 算法1 Fluid Simulation($\\vec u_n$, $\\Delta t$): 1: 初始化速度场$\\vec u_n$,使得$\\vec u_n$无散度 2: 对于每个时间步$n = 0,1,2,…$ 3: &emsp;&emsp;决定一个合理的时间步长$\\Delta t = t_{n+1}-t_n$ 4: &emsp;&emsp;对流项计算$\\vec u_A=advect(\\vec u_n,\\Delta t,\\vec q)$ 5: &emsp;&emsp;体积力项计算$\\vec u_B=\\vec u_A+\\Delta t\\vec g$ 6: &emsp;&emsp;无散度投影$\\vec u_{n+1}=project(\\Delta t,\\vec u_B)$ 1、时间步长&emsp;&emsp;在流体模拟算法中，确定适当的时间步长是算法的第一步。因为计算流体模拟的最后结果是呈现在屏幕上的，所以$\\Delta t$的选取与屏幕的刷新率有重要的关系。若选取的$\\Delta t$有$t_n+\\Delta t &gt; t_{frame}$，那么必须做一个截断使$\\Delta t=t_{frame}-t_n$。此外，流体模拟的三个步骤即对流项、体积力项、无散度投影项对时间步长$\\Delta t$的要求不尽相同，要选择一个满足所有要求的最小时间步长能确保计算的收敛性。此外，一方面为了流体模拟的真实性，我们可能需要选取一个足够小的时间步长来复现流体的高质量细节。另一方面，有时高性能的需求又使得我们不能选取太小的时间步长去渲染一帧。假设一帧至少要进行三个时间步的模拟，那么$\\Delta t$应该至少设成帧间隔时间的三分之一。 2、网格结构&emsp;&emsp;欧拉法的整个流程都是基于网格的，所以合理的网格结构是算法高效的关键点。$Harlow$和$Welch$提出了一种经典的$MAC$（marker and cell）网格结构，许多不可压缩流体模拟的算法都在这个网格结构上呈现出了良好的效率。$MAC$网格是一种交叉排列的网格，不同类型的物理量被存储于网格的不同位置。以二维的网格为例，如图3-1左图所示，流体粒子的压力数据存储于网格的中心点$P_{i,j}$，而速度则沿着笛卡尔坐标被分成了两部分。水平方向的$u$成分被存储在了网格单元竖直边的中心处，例如网格单元$(i,j)$和$(i+1,j)$之间的水平速度记为$u_{i+1/2,j}$。垂直方向的$v$成分则被存储在了网格单元水平面的中心上。这样的存储方案十分有利于估算流体流进/流出某个网格单元的量。 图3-1 MAC网格,左图二维,右图三维 &emsp;&emsp;扩展到三维的情况，$MAC$网格同样是交错排列的结构网格，如图3-1右图所示。压力数值存储在立方体网格单元的中心，三个速度分量分别被记录在立方体网格单元的三个表面的中心点上。在数值计算时，这样的分配方式使得我们可以准确地采用中心差分法计算压力梯度和速度的散度，同时克服了中心差分法的一个普遍的缺点。一维的情况为例，在网格顶点位置$…,q_{i-1},q_i,q_{i+1}…$上估算量场$q$的导数，为了无偏（所谓无偏，就是不偏向左边或者右边）估计网格顶点$i$处的$\\frac{\\partial q}{\\partial x}$，一种比较自然的方式就是采用一阶中心差分法： (\\frac{\\partial q}{\\partial x})_i\\approx \\frac{q_{i+1}-q_{i-1}}{2\\Delta x} \\tag {3.4}&emsp;&emsp;公式$(3.4)$是无偏的，且精确度为$O(\\Delta x^2)$。而前向欧拉差分法偏向右边且精确度只有$O(\\Delta x)$： (\\frac{\\partial q}{\\partial x})_i\\approx \\frac{q_{i+1}-q_i}{\\Delta x} \\tag {3.5}&emsp;&emsp;然而，公式$(3.4)$存在着一个非常严重的问题：网格点$i$的估算导数完全忽略了$q_i$的值。数学上，只有常数函数的一阶导数为零。但是公式$(3.4)$遇到了锯齿函数如$q_i=(-1)^i$时，它错误地将该类函数的导数估算为$0$，这种问题被称为零空间问题（null-space problem）。 &emsp;&emsp;交叉错排的$MAC$网格完美地克服了中心差分法的零空间问题，同时也保持了它的无偏二阶精度。在$MAC$网格上运用中心差分法，网格点$i$处的估算导数公式如下所示： (\\frac{\\partial q}{\\partial x})_i\\approx\\frac{q_{i+1/2}-q_{i-1/2}}{\\Delta x} \\tag {3.6}&emsp;&emsp;$MAC$网格确实给流体的压力计算和不可压缩性的处理带来了很大的便利，但与此同时也带来了一些其他方面的麻烦。如果我们要估算某个地方的速度向量，即便采样点恰好在网格点上我们也要做一些插值才能获取相应的速度向量。在网格点处，我们通常采用平均法，以二维为例： \\vec u_{i,j}=(\\frac{u_{i-1/2,j}+u_{i+1/2,j}}{2},\\frac{v_{i,j-1/2}+v_{i,j+1/2}}{2}),\\\\ \\vec u_{i+1/2,j}=(u_{i+1/2,j},\\frac{v_{i,j-1/2}+v_{i,j+1/2}+v_{i+1,j-1/2}+v_{i+1,j+1/2}}{4}),\\\\ \\vec u_{i,j+1/2}=(\\frac{u_{i-1/2,j}+u_{i+1/2,j}+u_{i-1/2,j+1}+u_{i+1/2,j+1}}{4},v_{i,j+1/2}).\\tag {3.7}&emsp;&emsp;最后，在实现中下标索引一般没有浮点数之说，前面直接采用$i+1/2$的记法是为了便于叙述。一般约定如下： p(i,j,k)=p_{i,j,k},\\\\ u(i,j,k)=u_{i-1/2,j,k},\\\\ v(i,j,k)=v_{i,j-1/2,k},\\\\ w(i,j,k)=w_{i,j,k-1/2}. \\tag{3.8}&emsp;&emsp;因而对于$nx\\times ny\\times nz$分辨率的网格，压力数值存储在$nx\\times ny\\times nz$的数组中，速度的$u$成分存储在$(nx+1)\\times ny\\times nz$数组中，速度的$v$成分存储在$nx\\times (ny+1)\\times nz$数组中，速度的$w$成分存储在$nx\\times ny\\times (nz+1)$数组中。 四、对流算法&emsp;&emsp;求解如下所示的对流方程是流体模拟的关键一步： \\frac{Dq}{Dt}=0 \\tag {4.1}&emsp;&emsp;我们把这个对流数值计算的算法记为： q^{n+1}=advect(\\vec u,\\Delta t,q^n) \\tag {4.2}&emsp;&emsp;公式$(4.2)$中的各个符号含义： &emsp;&emsp;$\\vec u$：在$MAC$网格上的离散化的速度场； &emsp;&emsp;$\\Delta t$：时间步长； &emsp;&emsp;$q^n$：当前的物理量场$q$（如流体密度、速度、燃烧物浓度等）； &emsp;&emsp;$q^{n+1}$：经过对流后得到的新的量场。 &emsp;&emsp;在这里要特别注意，输入对流算法的速度场$\\vec u$必须是无散度的，否则模拟结果会出现一些奇怪的失真现象。 1、半拉格朗日对流算法（Semi-Lagrangian Advection）&emsp;&emsp;一维情况下，对流方程$(4.1)$写成偏微分的形式如下： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=0 \\tag {4.3}&emsp;&emsp;分别采用前向欧拉差分法计算对时间的偏导和中心差分法计算对空间的偏导，我们有： \\frac{q^{n+1}_{i}-q^n_i}{\\Delta t}+u^n_i\\frac{q^n_{i+1}-q^n_{i-1}}{2\\Delta x}=0 \\tag {4.4}&emsp;&emsp;转成以$q^{n+1}_i$为计算目标的显式公式，得： q^{n+1}_i=q^n_i-\\Delta t u^n_i\\frac{q^n_{i+1}-q^n_{i-1}}{2\\Delta x} \\tag {4.5}&emsp;&emsp;公式$(4.5)$看起来没什么问题，但是却存在非常严重的漏洞。首先，前向欧拉法被证明是无条件不稳定的空间离散方法：无论取多么小Δ𝑡，随着时间步的推进，累积误差终将发散。即使使用更稳定的时间积分方法来取代前向欧拉方法，解决了时间上的PDE（Partial Differential Equation，偏微分方程）计算，空间上的PDE计算还是会带来重大的麻烦。标准中心差分方法不可避免地会出现的零空间问题，具有高频震荡性质的速度场对空间的导数被错误地计算为$0$或几乎为$0$，低离速度分量被分离出来，从而导致模拟效果中出现许多奇怪的高频摆动和震荡。 &emsp;&emsp;针对这些问题，研究者们提出了一个解然不同的、更加简单和更具物理直观意义的半拉格朗日法。之所以叫半拉格朗日法，是因为这种方法是以拉格朗日视角去解决欧拉视角的对流方程（“半”字的由来）。假设我们的目标是求解网格点$\\vec x_G$的在第$n+1$个时间步时关于物理量$q$的新值，记为$q^{n+1}_G$。在拉格朗日的视角下，我们可以寻找在第$n+1$时间步之前，是空间中的哪一个点上的流体粒子在速度场$\\vec u$的作用下“流向”了$\\vec x_G$，我们记这个粒子在第$n$个时间步时的网格位置为$\\vec x_P$，则第$n+1$个时间步时$\\vec x_G$的$q^{n+1}_G$即为第$n$个时间步时$\\vec x_P$的$q^{n}_P$。如下图4-1为半拉格朗日对流法的示意图。 图4-1 半拉格朗日对流法 &emsp;&emsp;半拉格朗日对流法的第一步就是要找出$\\vec x_P$，为此我们根据$\\vec x_G$做反向的追踪。粒子位置对时间的导数就是速度场： \\frac{d\\vec x}{dt}=\\vec u(\\vec x) \\tag {4.6}&emsp;&emsp;经过一个时间步长$\\Delta t$之后，粒子由$\\vec x_P$移动到$\\vec x_G$。为了得到$\\vec x_P$，最简单的方法就是采用前向欧拉法进行倒推： \\vec x_P=\\vec x_G-\\Delta t\\vec u(\\vec x_G) \\tag {4.7}&emsp;&emsp;然而前向欧拉法只有一阶的精度，若在不改变$\\Delta t$的情况下提高精度，我们可以采用高阶的龙格库塔法（Runge-Kutta method）。采用二阶的龙格库塔法如下所示： \\vec x_{mid}=\\vec x_G-\\frac12\\Delta t\\vec u(\\vec x_G),\\\\ \\vec x_P=\\vec x_G-\\Delta t\\vec u(\\vec x_{mid}). \\tag {4.7}&emsp;&emsp;倒推得到$\\Delta t$之前的网格位置$\\vec x_P$一般不会恰好在网格顶点上，为此我们需要做些插值。三维模拟通常采用三线性插值，而二维的则采用双线性插值。 q^{n+1}_G=interpolate(q_n,\\vec x_P) \\tag {4.8}2、边界情况&emsp;&emsp;若我们倒推得到的$\\vec x_P$仍然在流体的内部，那么做插值是完全没问题的。但若$\\vec x_P$在流体的边界之外呢？这种情况的出现的原因通常有两个：一个是$\\vec x_P$确确实实在流体的外部且即将流入流体内部，另一个是由前向欧拉法或龙格库塔法的数值计算方法带来的误差导致。 &emsp;&emsp;在一种情况下，我们应该知道当流体流入时其携带的物理量，此时我们将这个外部流入的物理量作为返回值即可。例如，第$n$个时间步时的外部流体以速度$\\vec U$和温度$T$在第$n+1$个时间步时注入流体内部$\\vec x_G$的位置，那么$\\vec T^{n+1}_G$的值就为$T$。 &emsp;&emsp;在第二种由误差导致的情况下，一个适当的策略就是根据边界上的最近点外推出所求得物理量。在模拟某些流体时，外推变得很简单。例如，在模拟烟雾时我们简单地假设烟雾流体外部即空气的速度风场为某个常数$\\vec U$（可能为$0$），这样边界上的速度场都取$\\vec U$。但还有一些必须根据流体内部的已知量外推出未知量，这时情况就变得比较复杂了。具体如何外推将在后面介绍，目前我们只需要知道大概的步骤：首先寻找边界上的最近点，然后在最近点的领域内插值获取相应的物理量场。 3、时间步长大小&emsp;&emsp;对任何一种数值计算方法的主要的考虑点就是它是否稳定。幸运的是，半拉格朗日对流法已经被证明是一种无条件稳定的算法：无论$\\Delta t$取多大，它永远不会出现数值爆炸的现象。因为每一个新值$q$的确定，都是通过对旧值得插值，无论是线性插值、双线性插值还是三线性插值，$q$的大小都是处于插值点之间，不会得到比原来插值点更大或者更小的值，因而$q$是有上下界的。这使得我们可以尽情地根据所需的模拟质量和模拟效率去调整时间步长。 &emsp;&emsp;但是在实践中，时间步长的大小也不能选得太过极端，否则会产生一些奇观的现象。Foster和Fekiw提出了一个对$\\Delta t$的限制：流体粒子在$\\Delta t$内的倒推轨迹最多经过某个常数个网格单元为宜，例如5个： \\Delta t \\leq \\frac{5\\Delta x}{u_{max}} \\tag {4.9}&emsp;&emsp;公式$(4.9)$中，$u_{max}$是速度场的最大值，我们可以简单地取 存储在网格中的最大速度值。一个更鲁棒的方法考虑了体积力（如重力、浮力等）对最大速度的影响： u_{max}=max(|u^n|)+\\Delta t|g| \\tag {4.10}&emsp;&emsp;将不等式$(4.9)$的最大值带入公式$(4.10)$，我们有： u_{max}=max(|u^n|)+\\frac{5\\Delta x}{u_{max}}|g| \\tag {4.11}&emsp;&emsp;取一个简单的速度上界（简化了公式$(4.11)$），$u_{max}$： u_{max}=max(|u^n|)+\\sqrt{5\\Delta xg} \\tag {4.12}&emsp;&emsp;这样确保了$u_{max}$始终为正，且避免公式$(4.9)$的除$0$错误。 &emsp;&emsp;关于时间步长的讨论离不开$CFL$（以Courant、Friedrichs、Lewy三人的名字命名）条件。$CFL$条件是一个简单而直观的判断计算是否收敛的必要条件。它的直观物理解释就是时间推进求解的速度必须大于物理扰动传播的速度，只有这样才能将物理上所有的扰动俘获到。满足$CFL$条件意味着当$\\Delta x$和$\\Delta t$趋于取极限$0$时，数值计算所求的解就会收敛到原微分方程的解。 &emsp;&emsp;对于半拉格朗日对流法，其满足$CFL$条件当且仅当在极限情况下，追踪得到的粒子轨迹足够逼近真实的轨迹。足够逼近的意思是经过正确的网格插值能够得到正确的依赖域（即差分格式的依赖域包含了原微分方程的依赖域），追踪的轨迹就会收敛到正确真实的轨迹。 &emsp;&emsp;因而，对于采用标准的显式有限差分法的对流方程求解，为了保证收敛，我们要求$q^{n+1}$的新值是由以当前网格点为中心、以$C\\Delta x$（$C$是一个小的整数常量）为半径的邻域范围内插值得到： \\Delta t \\leq C\\frac{\\Delta x}{|\\vec u|} \\tag {4.13}&emsp;&emsp;公式$(4.13)$中的$C$被称为$CFL$数，因而不等式$(4.9)$可以看成是公式$(4.13)$取$CFL$数为$5$得到。 4、数值耗散&emsp;&emsp;对流算法在对流获取新的物理量场$q^{n+1}_i$时会进行一些插值操作，插值不可避免地会平滑物理量场，这带来了一些数值耗散。一次两次的数值耗散不会由太大的影响，但是在流体模拟中我们会在每个时间步都进行对流运算，反反复复的平滑操作将数值耗散不断扩大，损失大量的流体细节。 &emsp;&emsp;以一维的对流项计算为例，流体速度为常量$u&gt;0$： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=0 \\tag {4.14}&emsp;&emsp;假设$\\Delta t &lt; \\frac{\\Delta x}{u}$，即单个时间步长内粒子追踪轨迹长度小于单个网格单元的大小。我们的目标点是$x_i$，则倒推得到的粒子位置就落在了$[x_{i-1},x_i]$上的$x_i-\\Delta tu$，然后进行线性插值得到$q^{n+1}_i$： q^{n+1}=\\frac{\\Delta tu}{\\Delta x}q^n_{i-1}+(1-\\frac{\\Delta tu}{\\Delta x})q^n_i \\tag {4.15}&emsp;&emsp;将公式$(4.15)$整理一下，有： q^{n+1}_i=q^n_i-\\Delta tu\\frac{q^n_i-q^n_{i-1}}{\\Delta x} \\tag {4.16}&emsp;&emsp;公式$(4.16)$实际上正好就是采用时间上的前向欧拉差分法和空间上的单向有限差分法的欧拉方案，把$q^n_i$看成是$q^n$关于$x_i$的函数，对$q^n_{i-1}$进行泰勒级数展开： q^n_{i-1}=q^n_i-(\\frac{\\partial q}{\\partial x})^n_i\\Delta x+(\\frac{\\partial^2q}{\\partial x^2})^n_i\\frac{\\Delta x^2}{2}+O(\\Delta x^3) \\tag {4.17}&emsp;&emsp;将公式$(4.17)$代入公式$(4.16)$，并做一些变量消去，可得： q^{n+1}_i=q^n_i-\\Delta tu(\\frac{\\partial q}{\\partial x})^n_i+\\Delta tu\\Delta x(\\frac{\\partial^2q}{\\partial x^2})^n_i+O(\\Delta x^2) \\tag {4.18}&emsp;&emsp;在二阶截断误差的情况下，结合公式$(4.18)$和公式$(4.14)$，有： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=u\\Delta x(\\frac{\\partial^2q}{\\partial x^2}) \\tag {4.19}&emsp;&emsp;右边就是对流方程计算时引入的额外类似粘度乘上系数$u\\Delta x$的项。这也就是说，当我们采用简单的半拉格朗日法去求解无粘度的对流方程时，模拟的结果却看起来我们像时在模拟有粘度的流体。这就是数值耗散！当然，当$\\Delta x\\to 0$时，这个数值耗散系数也会趋于$0$，所以取时间步无穷小时能够得到正确的模拟结果，但这需要耗费巨额的计算资源开销。我们通常模拟的流体大多数都是无粘度的，所以如何减少这个数值耗散是个至关重要的难题。 &emsp;&emsp;一个简单有效的修复数值耗散的方法就是采用更加锐利的插值方法，从而尽可能地减少由插值带来的数值耗散。在一维的情况时，我们采用三次插值（cubic interpolant）如下公式$(4.21)$，而不是简单的一次线性插值$(4.20)$： q\\approx(1-s)x_i+sx_{i+1} \\tag {4.20} q\\approx[-\\frac13s+\\frac12s^2-\\frac16s^3]q_{i-1}+[1-s^2+\\frac12(s^3-s)]q_i\\\\ +[s+\\frac12(s^2-s^3)]q_{i+1}+[\\frac16(s^3-s)]q_{i+2} \\tag {4.21}&emsp;&emsp;扩展到二维或者三维就是双三次插值（bicubic interpolation）或三三次插值（tricubic interpolation）。以二维情况为例，我们可以先沿着$x$轴做第一遍的三次插值如公式$(4.22)$，然后再沿着$y$轴做第二遍插值如公式$(4.23)$： q_{j-1}=w_{-1}(s)q_{i-1,j-1}+w_0(s)+q_{i,j-1}+w_1(s)q_{i+1,j-1}+w_2(s)q_{i+2,j-1},\\\\ q_{j}=w_{-1}(s)q_{i-1,j}+w_0(s)+q_{i,j}+w_1(s)q_{i+1,j}+w_2(s)q_{i+2,j},\\\\ q_{j+1}=w_{-1}(s)q_{i-1,j+1}+w_0(s)+q_{i,j+1}+w_1(s)q_{i+1,j+1}+w_2(s)q_{i+2,j+1},\\\\ q_{j+2}=w_{-1}(s)q_{i-1,j+2}+w_0(s)+q_{i,j+2}+w_1(s)q_{i+1,j+2}+w_2(s)q_{i+2,j+2}. \\tag {4.22} q=w_{-1}(t)q_{j-1}+w_0(t)q_j+w_1(t)q_{j+1}+w_2(t)q_{j+2} \\tag {4.23}&emsp;&emsp;当然也可以先沿着$y$轴，然后再沿着$x$轴做插值操作。","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"http://yoursite.com/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Naiver-Stokes Equations","slug":"Naiver-Stokes-Equations","permalink":"http://yoursite.com/tags/Naiver-Stokes-Equations/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"http://yoursite.com/tags/Fluid-Simulation/"},{"name":"Advection","slug":"Advection","permalink":"http://yoursite.com/tags/Advection/"}]}]}